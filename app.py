# -*- coding: utf-8 -*-
"""üìö –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ DOI —Å —É–º–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —ç–∫—Å–ø–æ—Ä—Ç–æ–º –≤ Excel
–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è Streamlit
"""

# ============================================================================
# üì¶ –ò–ú–ü–û–†–¢–´ –ò –ù–ê–°–¢–†–û–ô–ö–ê
# ============================================================================

import streamlit as st
import requests
import json
import re
import time
import pickle
import hashlib
import os
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Set, Union
from collections import defaultdict, Counter, OrderedDict
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
import threading
from queue import Queue
import math
from collections import deque
import networkx as nx
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import numpy as np
import tempfile
import base64
from io import BytesIO
import joblib
from fuzzywuzzy import fuzz

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã Streamlit
st.set_page_config(
    page_title="üìö –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ DOI",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================================
# ‚öôÔ∏è –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================

class Config:
    CROSSREF_URL = "https://api.crossref.org/works/"
    OPENALEX_URL = "https://api.openalex.org/works/https://doi.org/"
    OPENALEX_WORKS_URL = "https://api.openalex.org/works"
    ORCID_API_URL = "https://pub.orcid.org/v3.0/search/"
    ROR_API_URL = "https://api.ror.org/organizations"

    REQUEST_TIMEOUT = 10
    MAX_RETRIES = 2
    MAX_DELAY = 1.0
    MIN_DELAY = 0.1
    INITIAL_DELAY = 0.2

    # –î–ª—è Streamlit –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ —Å–µ—Å—Å–∏–∏
    CACHE_DIR = tempfile.mkdtemp(prefix="article_analyzer_cache_")
    TTL_HOURS = 24
    MAX_CACHE_SIZE_MB = 50

    MIN_WORKERS = 1
    MAX_WORKERS = 10
    DEFAULT_WORKERS = 4

    BATCH_SIZE = 50

    TOP_PERCENTILE_FOR_DEEP_ANALYSIS = 10
    MIN_CITATIONS_FOR_DEEP_ANALYSIS = 10

    # –ü–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—ç—Ç–∏—á–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫
    QUICK_CHECK_THRESHOLDS = {
        'journal_concentration': 0.7,  # >70% –∏–∑ –æ–¥–Ω–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞
        'author_self_citation': 0.3,   # >30% —Å –æ–±—â–∏–º–∏ –∞–≤—Ç–æ—Ä–∞–º–∏
        'affiliation_self_citation': 0.6,  # >60% –∏–∑ —Ç–æ–π –∂–µ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏
        'single_country': 0.8,         # >80% –∏–∑ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω—ã
        'citation_velocity': 20,       # >20 —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ –≥–æ–¥
        'first_year_share': 0.5        # >50% –≤ –ø–µ—Ä–≤—ã–π –≥–æ–¥
    }

    MEDIUM_INSIGHT_THRESHOLDS = {
        'first_two_years': 0.7,        # >70% –∑–∞ –ø–µ—Ä–≤—ã–µ 2 –≥–æ–¥–∞
        'top_journal_share': 0.6,      # >60% –∏–∑ —Ç–æ–ø-1 –∂—É—Ä–Ω–∞–ª–∞
        'cluster_coefficient': 0.8,    # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ >0.8
        'geographic_bias': 0.9         # –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π bias >0.9
    }

    COUNTRY_CODES = {
        'USA': 'US', 'United States': 'US', 'US': 'US',
        'United Kingdom': 'GB', 'UK': 'GB', 'Great Britain': 'GB',
        'Germany': 'DE', 'Deutschland': 'DE',
        'France': 'FR', 'France': 'FR',
        'China': 'CN', 'People\'s Republic of China': 'CN', 'PR China': 'CN',
        'Japan': 'JP', 'Japan': 'JP',
        'Canada': 'CA', 'Canada': 'CA',
        'Australia': 'AU', 'Australia': 'AU',
        'Italy': 'IT', 'Italia': 'IT',
        'Spain': 'ES', 'Espa√±a': 'ES',
        'Russia': 'RU', 'Russian Federation': 'RU', '–†–æ—Å—Å–∏—è': 'RU', 'Russian': 'RU',
        'India': 'IN', 'India': 'IN',
        'Brazil': 'BR', 'Brasil': 'BR',
        'South Korea': 'KR', 'Korea, Republic of': 'KR', 'Korea': 'KR',
        'Netherlands': 'NL', 'The Netherlands': 'NL',
        'Switzerland': 'CH', 'Switzerland': 'CH',
        'Sweden': 'SE', 'Sweden': 'SE',
        'Norway': 'NO', 'Norway': 'NO',
        'Denmark': 'DK', 'Denmark': 'DK',
        'Finland': 'FI', 'Finland': 'FI',
        'Austria': 'AT', 'Austria': 'AT',
        'Belgium': 'BE', 'Belgium': 'BE',
        'Poland': 'PL', 'Poland': 'PL',
        'Portugal': 'PT', 'Portugal': 'PT',
        'Greece': 'GR', 'Greece': 'GR',
        'Turkey': 'TR', 'T√ºrkiye': 'TR',
        'Israel': 'IL', 'Israel': 'IL',
        'Singapore': 'SG', 'Singapore': 'SG',
        'Taiwan': 'TW', 'Taiwan, Province of China': 'TW',
        'Hong Kong': 'HK', 'Hong Kong SAR': 'HK',
        'Mexico': 'MX', 'Mexico': 'MX',
        'Argentina': 'AR', 'Argentina': 'AR',
        'Chile': 'CL', 'Chile': 'CL',
        'Colombia': 'CO', 'Colombia': 'CO',
        'Ukraine': 'UA', 'Ukraine': 'UA',
        'Czech Republic': 'CZ', 'Czechia': 'CZ',
        'Hungary': 'HU', 'Hungary': 'HU',
        'Romania': 'RO', 'Romania': 'RO',
        'Bulgaria': 'BG', 'Bulgaria': 'BG',
        'Serbia': 'RS', 'Serbia': 'RS',
        'Croatia': 'HR', 'Croatia': 'HR',
        'Slovakia': 'SK', 'Slovakia': 'SK',
        'Slovenia': 'SI', 'Slovenia': 'SI',
        'Lithuania': 'LT', 'Lithuania': 'LT',
        'Latvia': 'LV', 'Latvia': 'LV',
        'Estonia': 'EE', 'Estonia': 'EE',
        'Ireland': 'IE', 'Ireland': 'IE',
        'New Zealand': 'NZ', 'New Zealand': 'NZ',
        'South Africa': 'ZA', 'South Africa': 'ZA',
        'Egypt': 'EG', 'Egypt': 'EG',
        'Saudi Arabia': 'SA', 'Saudi Arabia': 'SA',
        'United Arab Emirates': 'AE', 'UAE': 'AE',
        'Qatar': 'QA', 'Qatar': 'QA',
        'Iran': 'IR', 'Iran, Islamic Republic of': 'IR',
        'Pakistan': 'PK', 'Pakistan': 'PK',
        'Bangladesh': 'BD', 'Bangladesh': 'BD',
        'Vietnam': 'VN', 'Viet Nam': 'VN',
        'Thailand': 'TH', 'Thailand': 'TH',
        'Malaysia': 'MY', 'Malaysia': 'MY',
        'Indonesia': 'ID', 'Indonesia': 'ID',
        'Philippines': 'PH', 'Philippines': 'PH',
        'Kazakhstan': 'KZ', 'Kazakhstan': 'KZ',
        'Belarus': 'BY', 'Belarus': 'BY',
        'Uzbekistan': 'UZ', 'Uzbekistan': 'UZ',
        'Azerbaijan': 'AZ', 'Azerbaijan': 'AZ',
        'Georgia': 'GE', 'Georgia': 'GE',
        'Armenia': 'AM', 'Armenia': 'AM',
        'Moldova': 'MD', 'Moldova': 'MD',
        'Kyrgyzstan': 'KG', 'Kyrgyzstan': 'KG',
        'Tajikistan': 'TJ', 'Tajikistan': 'TJ',
        'Turkmenistan': 'TM', 'Turkmenistan': 'TM',
        'Mongolia': 'MN', 'Mongolia': 'MN',
    }

# ============================================================================
# üóÇÔ∏è –ö–õ–ê–°–° –£–ú–ù–û–ì–û –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø (–£–õ–£–ß–®–ï–ù–ù–´–ô)
# ============================================================================

class SmartCacheManager:
    def __init__(self, cache_dir: str = Config.CACHE_DIR, ttl_hours: int = Config.TTL_HOURS):
        self.cache_dir = cache_dir
        self.ttl_seconds = ttl_hours * 3600

        self.stats = {
            'hits': 0,
            'misses': 0,
            'expired': 0,
            'evictions': 0,
            'total_size_mb': 0,
            'memory_hits': 0,
            'file_hits': 0,
            'api_calls_saved': 0
        }

        self.memory_cache = OrderedDict()
        self.max_memory_items = 5000

        self.failed_cache = {}
        self.failed_cache_ttl = 3600

        self.popular_cache = {}

        self.ror_cache = {
            'analyzed': {},
            'ref': {},
            'citing': {},
            'summary': {}
        }

        self.insights_cache = {
            'geo_bubbles': {},
            'temporal_patterns': {},
            'hyper_citation': {},
            'citation_cascades': {},
            'mutual_citations': {}
        }

        # –ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—ç—Ç–∏—á–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫
        self.ethical_analysis_cache = {
            'quick_checks': {},
            'medium_insights': {},
            'deep_analysis': {},
            'citing_relationships': {}
        }

        # –ö—ç—à –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        self.terminology_cache = {
            'term_networks': {},
            'emerging_terms': {},
            'convergence_zones': {},
            'frontier_predictions': {}
        }

        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir, exist_ok=True)

        self._clean_expired_cache()

        self._load_popular_dois()

    def _get_cache_key(self, source: str, identifier: str) -> str:
        key_str = f"v3:{source}:{identifier}"
        return hashlib.sha256(key_str.encode()).hexdigest()[:32]

    def _get_cache_path(self, key: str) -> str:
        return os.path.join(self.cache_dir, f"{key}.pkl")

    def _get_cache_metadata_path(self, key: str) -> str:
        return os.path.join(self.cache_dir, f"{key}_meta.json")

    def _calculate_cache_size(self) -> float:
        total_size = 0
        try:
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.pkl'):
                    filepath = os.path.join(self.cache_dir, filename)
                    total_size += os.path.getsize(filepath)
        except:
            pass
        return total_size / (1024 * 1024)

    def _clean_expired_cache(self):
        try:
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.pkl'):
                    filepath = os.path.join(self.cache_dir, filename)
                    try:
                        with open(filepath, 'rb') as f:
                            cached_data = pickle.load(f)

                        if time.time() - cached_data.get('timestamp', 0) > self.ttl_seconds:
                            os.remove(filepath)
                            self.stats['expired'] += 1

                            meta_file = filepath.replace('.pkl', '_meta.json')
                            if os.path.exists(meta_file):
                                os.remove(meta_file)

                    except:
                        try:
                            os.remove(filepath)
                        except:
                            pass

            cache_size = self._calculate_cache_size()
            if cache_size > Config.MAX_CACHE_SIZE_MB:
                self._evict_old_cache_items()

        except Exception as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")

    def _evict_old_cache_items(self):
        try:
            cache_files = []
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.pkl'):
                    filepath = os.path.join(self.cache_dir, filename)
                    mtime = os.path.getmtime(filepath)
                    cache_files.append((mtime, filepath))

            cache_files.sort()

            cache_size = self._calculate_cache_size()
            while cache_files and cache_size > Config.MAX_CACHE_SIZE_MB * 0.8:
                _, old_file = cache_files.pop(0)

                try:
                    os.remove(old_file)
                    self.stats['evictions'] += 1

                    meta_file = old_file.replace('.pkl', '_meta.json')
                    if os.path.exists(meta_file):
                        os.remove(meta_file)

                except:
                    pass

                cache_size = self._calculate_cache_size()

        except Exception as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–∞—Ä—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫—ç—à–∞: {e}")

    def get(self, source: str, identifier: str, category: str = "default") -> Optional[Any]:
        failed_key = f"failed:{source}:{identifier}"
        if failed_key in self.failed_cache:
            failed_data = self.failed_cache[failed_key]
            if time.time() - failed_data['timestamp'] < self.failed_cache_ttl:
                return None

        key = self._get_cache_key(source, identifier)

        memory_key = f"{category}:{key}"
        if memory_key in self.memory_cache:
            data = self.memory_cache[memory_key]
            del self.memory_cache[memory_key]
            self.memory_cache[memory_key] = data
            self.stats['hits'] += 1
            self.stats['memory_hits'] += 1
            return data

        cache_path = self._get_cache_path(key)
        meta_path = self._get_cache_metadata_path(key)

        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    cached_data = pickle.load(f)

                if os.path.exists(meta_path):
                    try:
                        with open(meta_path, 'r') as mf:
                            metadata = json.load(mf)
                        category_match = metadata.get('category') == category
                    except:
                        category_match = True
                else:
                    category_match = True

                if (time.time() - cached_data.get('timestamp', 0) < self.ttl_seconds and
                    category_match):

                    if len(self.memory_cache) >= self.max_memory_items:
                        self.memory_cache.popitem(last=False)

                    self.memory_cache[memory_key] = cached_data['data']
                    self.stats['hits'] += 1
                    self.stats['file_hits'] += 1
                    return cached_data['data']
                else:
                    os.remove(cache_path)
                    if os.path.exists(meta_path):
                        os.remove(meta_path)
                    self.stats['expired'] += 1

            except:
                try:
                    os.remove(cache_path)
                    if os.path.exists(meta_path):
                        os.remove(meta_path)
                except:
                    pass

        self.stats['misses'] += 1
        return None

    def set(self, source: str, identifier: str, data: Any, category: str = "default"):
        key = self._get_cache_key(source, identifier)
        cache_path = self._get_cache_path(key)
        meta_path = self._get_cache_metadata_path(key)

        cache_entry = {
            'timestamp': time.time(),
            'source': source,
            'identifier': identifier,
            'data': data,
            'category': category
        }

        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(cache_entry, f, protocol=pickle.HIGHEST_PROTOCOL)

            metadata = {
                'category': category,
                'created': datetime.now().isoformat(),
                'source': source,
                'identifier_hash': hashlib.md5(str(identifier).encode()).hexdigest()
            }

            with open(meta_path, 'w') as mf:
                json.dump(metadata, mf, indent=2)

            memory_key = f"{category}:{key}"
            if len(self.memory_cache) >= self.max_memory_items:
                self.memory_cache.popitem(last=False)

            self.memory_cache[memory_key] = data

            self.stats['api_calls_saved'] += 1

        except Exception as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫—ç—à: {e}")

    def mark_as_failed(self, source: str, identifier: str, error: str = ""):
        failed_key = f"failed:{source}:{identifier}"
        self.failed_cache[failed_key] = {
            'timestamp': time.time(),
            'error': error,
            'source': source,
            'identifier': identifier
        }

    def _load_popular_dois(self):
        popular_file = os.path.join(self.cache_dir, "popular_dois.json")
        if os.path.exists(popular_file):
            try:
                with open(popular_file, 'r') as f:
                    self.popular_cache = json.load(f)
            except:
                self.popular_cache = {}

    def _save_popular_dois(self):
        popular_file = os.path.join(self.cache_dir, "popular_dois.json")
        try:
            with open(popular_file, 'w') as f:
                json.dump(self.popular_cache, f, indent=2)
        except:
            pass

    def update_popularity(self, doi: str):
        if doi in self.popular_cache:
            self.popular_cache[doi] += 1
        else:
            self.popular_cache[doi] = 1

        if len(self.popular_cache) % 100 == 0:
            self._save_popular_dois()

    def get_stats(self) -> Dict[str, Any]:
        cache_size = self._calculate_cache_size()
        total_requests = self.stats['hits'] + self.stats['misses']
        hit_ratio = (self.stats['hits'] / total_requests * 100) if total_requests > 0 else 0

        return {
            'hits': self.stats['hits'],
            'misses': self.stats['misses'],
            'expired': self.stats['expired'],
            'evictions': self.stats['evictions'],
            'memory_hits': self.stats['memory_hits'],
            'file_hits': self.stats['file_hits'],
            'api_calls_saved': self.stats['api_calls_saved'],
            'memory_items': len(self.memory_cache),
            'cache_size_mb': round(cache_size, 2),
            'hit_ratio': round(hit_ratio, 1),
            'failed_cache_size': len(self.failed_cache),
            'popular_dois': len(self.popular_cache)
        }

    def clear_all(self):
        try:
            for filename in os.listdir(self.cache_dir):
                filepath = os.path.join(self.cache_dir, filename)
                try:
                    os.remove(filepath)
                except:
                    pass

            self.memory_cache.clear()
            self.failed_cache.clear()
            self.popular_cache.clear()
            self.ror_cache = {'analyzed': {}, 'ref': {}, 'citing': {}, 'summary': {}}
            self.insights_cache = {
                'geo_bubbles': {}, 'temporal_patterns': {}, 'hyper_citation': {},
                'citation_cascades': {}, 'mutual_citations': {}
            }
            self.ethical_analysis_cache = {
                'quick_checks': {}, 'medium_insights': {}, 'deep_analysis': {}, 'citing_relationships': {}
            }
            self.terminology_cache = {
                'term_networks': {}, 'emerging_terms': {}, 'convergence_zones': {}, 'frontier_predictions': {}
            }
            self.stats = {k: 0 for k in self.stats.keys()}

            st.success("‚úÖ –ö—ç—à –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω")

        except Exception as e:
            st.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")

    def get_ror_cache(self, category: str, query: str) -> Optional[Dict]:
        if category in self.ror_cache and query in self.ror_cache[category]:
            return self.ror_cache[category][query]
        return None

    def set_ror_cache(self, category: str, query: str, data: Dict):
        if category not in self.ror_cache:
            self.ror_cache[category] = {}
        self.ror_cache[category][query] = data

    def clear_ror_cache(self, category: str = None):
        if category:
            if category in self.ror_cache:
                self.ror_cache[category].clear()
        else:
            for cat in self.ror_cache:
                self.ror_cache[cat].clear()

    def get_insight_cache(self, insight_type: str, key: str) -> Optional[Dict]:
        if insight_type in self.insights_cache and key in self.insights_cache[insight_type]:
            return self.insights_cache[insight_type][key]
        return None

    def set_insight_cache(self, insight_type: str, key: str, data: Dict):
        if insight_type not in self.insights_cache:
            self.insights_cache[insight_type] = {}
        self.insights_cache[insight_type][key] = {
            'data': data,
            'timestamp': time.time()
        }

    def clear_insight_cache(self, insight_type: str = None):
        if insight_type:
            if insight_type in self.insights_cache:
                self.insights_cache[insight_type].clear()
        else:
            for insight in self.insights_cache:
                self.insights_cache[insight].clear()

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—ç—Ç–∏—á–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫
    def get_ethical_analysis(self, analysis_type: str, doi: str) -> Optional[Dict]:
        if analysis_type in self.ethical_analysis_cache and doi in self.ethical_analysis_cache[analysis_type]:
            return self.ethical_analysis_cache[analysis_type][doi]
        return None

    def set_ethical_analysis(self, analysis_type: str, doi: str, data: Dict):
        if analysis_type not in self.ethical_analysis_cache:
            self.ethical_analysis_cache[analysis_type] = {}
        self.ethical_analysis_cache[analysis_type][doi] = {
            'data': data,
            'timestamp': time.time()
        }

    def clear_ethical_analysis(self, analysis_type: str = None):
        if analysis_type:
            if analysis_type in self.ethical_analysis_cache:
                self.ethical_analysis_cache[analysis_type].clear()
        else:
            for analysis in self.ethical_analysis_cache:
                self.ethical_analysis_cache[analysis].clear()

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    def get_terminology_cache(self, cache_type: str, key: str) -> Optional[Dict]:
        if cache_type in self.terminology_cache and key in self.terminology_cache[cache_type]:
            return self.terminology_cache[cache_type][key]
        return None

    def set_terminology_cache(self, cache_type: str, key: str, data: Dict):
        if cache_type not in self.terminology_cache:
            self.terminology_cache[cache_type] = {}
        self.terminology_cache[cache_type][key] = {
            'data': data,
            'timestamp': time.time()
        }

    def clear_terminology_cache(self, cache_type: str = None):
        if cache_type:
            if cache_type in self.terminology_cache:
                self.terminology_cache[cache_type].clear()
        else:
            for cache in self.terminology_cache:
                self.terminology_cache[cache].clear()

# ============================================================================
# üöÄ –ö–õ–ê–°–° –ê–î–ê–ü–¢–ò–í–ù–´–• –ó–ê–î–ï–†–ñ–ï–ö
# ============================================================================

class AdaptiveDelayManager:
    def __init__(self, initial_delay: float = Config.INITIAL_DELAY):
        self.initial_delay = initial_delay
        self.current_delay = initial_delay
        self.max_delay = Config.MAX_DELAY
        self.min_delay = Config.MIN_DELAY
        self.success_count = 0
        self.failure_count = 0
        self.last_request_time = 0
        self.response_times = []

        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'avg_response_time': 0,
            'total_wait_time': 0
        }

    def wait_if_needed(self):
        current_time = time.time()
        elapsed = current_time - self.last_request_time

        if elapsed < self.current_delay:
            wait_time = self.current_delay - elapsed
            time.sleep(wait_time)
            self.stats['total_wait_time'] += wait_time

        self.last_request_time = time.time()
        return self.current_delay

    def update_delay(self, success: bool, response_time: float = None):
        self.stats['total_requests'] += 1

        if response_time:
            self.response_times.append(response_time)
            if len(self.response_times) > 10:
                self.response_times.pop(0)
            self.stats['avg_response_time'] = sum(self.response_times) / len(self.response_times)

        if success:
            self.success_count += 1
            self.failure_count = max(0, self.failure_count - 1)
            self.stats['successful_requests'] += 1

            if self.success_count >= 2:
                self.current_delay = max(self.min_delay, self.current_delay * 0.7)
                self.success_count = 0

        else:
            self.failure_count += 1
            self.success_count = 0
            self.stats['failed_requests'] += 1

            self.current_delay = min(self.max_delay, self.current_delay * 1.3)

        self.current_delay = min(self.max_delay, self.current_delay)

    def get_delay(self) -> float:
        return self.current_delay

    def get_stats(self) -> Dict[str, Any]:
        total_requests = self.stats['total_requests']
        success_rate = (self.stats['successful_requests'] / total_requests * 100) if total_requests > 0 else 0

        return {
            'current_delay': round(self.current_delay, 3),
            'total_requests': total_requests,
            'success_rate': round(success_rate, 1),
            'avg_response_time': round(self.stats['avg_response_time'], 3) if self.stats['avg_response_time'] > 0 else 0,
            'total_wait_time': round(self.stats['total_wait_time'], 2)
        }

# ============================================================================
# üìä –ö–õ–ê–°–° –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê –ü–†–û–ì–†–ï–°–°–ê (–ê–î–ê–ü–¢–ò–†–û–í–ê–ù –î–õ–Ø STREAMLIT)
# ============================================================================

class ProgressMonitor:
    def __init__(self, total_items: int, stage_name: str = "–û–±—Ä–∞–±–æ—Ç–∫–∞", progress_bar=None, status_text=None):
        self.total_items = total_items
        self.processed_items = 0
        self.start_time = time.time()
        self.stage_name = stage_name
        self.last_progress_time = self.start_time
        self.processing_speeds = []
        
        # Streamlit —ç–ª–µ–º–µ–Ω—Ç—ã
        self.progress_bar = progress_bar
        self.status_text = status_text
        self.progress_container = None

        self.checkpoint_times = []
        self.checkpoint_items = []

        self.stats = {
            'success': 0,
            'failed': 0,
            'cached': 0,
            'skipped': 0
        }

    def update(self, count: int = 1, item_type: str = None):
        self.processed_items += count

        if item_type:
            if item_type in self.stats:
                self.stats[item_type] += count
            else:
                self.stats[item_type] = count

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä Streamlit
        if self.progress_bar is not None and self.total_items > 0:
            progress_percent = (self.processed_items / self.total_items) * 100
            self.progress_bar.progress(progress_percent / 100.0)
            
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Ç–µ–∫—Å—Ç
        if self.status_text is not None:
            self._update_status_text()

        current_time = time.time()
        if current_time - self.last_progress_time > 10:
            self.last_progress_time = current_time

    def _update_status_text(self):
        if self.total_items == 0:
            return

        elapsed = time.time() - self.start_time
        progress_percent = (self.processed_items / self.total_items) * 100

        if elapsed > 0:
            speed = self.processed_items / elapsed
            self.processing_speeds.append(speed)
            if len(self.processing_speeds) > 5:
                self.processing_speeds.pop(0)

            avg_speed = sum(self.processing_speeds) / len(self.processing_speeds) if self.processing_speeds else speed
            items_per_min = avg_speed * 60

            remaining_items = self.total_items - self.processed_items
            if avg_speed > 0:
                eta_seconds = remaining_items / avg_speed
                eta_str = self._format_time(eta_seconds)
            else:
                eta_str = "—Ä–∞—Å—á–µ—Ç..."

            stats_str = ""
            for stat_type, count in self.stats.items():
                if count > 0:
                    stats_str += f", {stat_type}: {count}"

            status_message = f"{self.stage_name}: {self.processed_items}/{self.total_items} " \
                           f"({progress_percent:.1f}%), " \
                           f"—Å–∫–æ—Ä–æ—Å—Ç—å: {items_per_min:.1f} DOI/–º–∏–Ω, " \
                           f"–æ—Å—Ç–∞–ª–æ—Å—å: {eta_str}{stats_str}"
            
            if self.status_text is not None:
                self.status_text.text(status_message)

    def _format_time(self, seconds: float) -> str:
        if seconds < 60:
            return f"{seconds:.0f} —Å–µ–∫"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.0f} –º–∏–Ω"
        else:
            hours = seconds / 3600
            return f"{hours:.1f} —á"

    def get_summary(self) -> Dict[str, Any]:
        elapsed = time.time() - self.start_time

        if elapsed > 0:
            total_speed = self.processed_items / elapsed
            items_per_min = total_speed * 60
        else:
            items_per_min = 0

        return {
            'total_items': self.total_items,
            'processed_items': self.processed_items,
            'elapsed_time': round(elapsed, 1),
            'speed_per_min': round(items_per_min, 1),
            'success_count': self.stats.get('success', 0),
            'failed_count': self.stats.get('failed', 0),
            'cached_count': self.stats.get('cached', 0),
            'completion_percent': round((self.processed_items / self.total_items * 100), 1) if self.total_items > 0 else 0
        }

    def complete(self):
        elapsed = time.time() - self.start_time

        if self.total_items > 0:
            progress_percent = (self.processed_items / self.total_items) * 100
        else:
            progress_percent = 100

        summary = self.get_summary()

        if self.progress_bar is not None:
            self.progress_bar.progress(1.0)
            
        if self.status_text is not None:
            self.status_text.text(f"‚úÖ {self.stage_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞! "
                                  f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.processed_items} ({progress_percent:.1f}%), "
                                  f"–≤—Ä–µ–º—è: {self._format_time(elapsed)}")

        return summary

# ============================================================================
# üìù –ö–õ–ê–°–° –¢–†–ï–ö–ò–ù–ì–ê –ù–ï–£–î–ê–ß–ù–´–• DOI (–ù–û–í–´–ô)
# ============================================================================

class FailedDOITracker:
    def __init__(self):
        self.failed_dois = {}
        self.relationships = defaultdict(list)
        self.sources = {}

        self.stats = {
            'total_failed': 0,
            'analyzed_failed': 0,
            'ref_failed': 0,
            'citing_failed': 0,
            'retry_failed': 0,
            'by_error_type': defaultdict(int)
        }

    def add_failed_doi(self, doi: str, error: str, source_type: str,
                       related_dois: List[str] = None, original_doi: str = None):

        self.failed_dois[doi] = {
            'doi': doi,
            'error': error,
            'source_type': source_type,
            'timestamp': datetime.now().isoformat(),
            'related_dois': related_dois or [],
            'original_doi': original_doi
        }

        self.sources[doi] = source_type

        if related_dois:
            self.relationships[doi].extend(related_dois)

        self.stats['total_failed'] += 1

        if source_type in self.stats:
            self.stats[f'{source_type}_failed'] += 1
        else:
            self.stats['by_error_type'][source_type] = self.stats['by_error_type'].get(source_type, 0) + 1

        self.stats['by_error_type'][error] += 1

    def get_failed_for_excel(self) -> List[Dict]:
        data = []

        for doi, info in self.failed_dois.items():
            relationship_info = ""
            if info['original_doi']:
                relationship_info = f"–ò—Å—Ç–æ—á–Ω–∏–∫: {info['original_doi']}"
            elif info['related_dois']:
                relationship_info = f"–°–≤—è–∑–∞–Ω —Å: {', '.join(info['related_dois'][:3])}"
                if len(info['related_dois']) > 3:
                    relationship_info += f"... (–µ—â–µ {len(info['related_dois']) - 3})"

            row = {
                'DOI': doi,
                'Source Type': info['source_type'],
                'Error': info['error'],
                'Relationships': relationship_info,
                'Relationship Count': len(info['related_dois']),
                'Error Date': info['timestamp']
            }
            data.append(row)

        return data

    def get_stats(self) -> Dict[str, Any]:
        return {
            'total_failed': self.stats['total_failed'],
            'analyzed_failed': self.stats['analyzed_failed'],
            'ref_failed': self.stats['ref_failed'],
            'citing_failed': self.stats['citing_failed'],
            'retry_failed': self.stats['retry_failed'],
            'error_types': dict(self.stats['by_error_type']),
            'unique_failed_dois': len(self.failed_dois)
        }

    def clear(self):
        self.failed_dois.clear()
        self.relationships.clear()
        self.sources.clear()
        self.stats = {
            'total_failed': 0,
            'analyzed_failed': 0,
            'ref_failed': 0,
            'citing_failed': 0,
            'retry_failed': 0,
            'by_error_type': defaultdict(int)
        }

# ============================================================================
# üåê –ö–õ–ê–°–° –ö–õ–ò–ï–ù–¢–û–í API
# ============================================================================

class APIClient:
    def __init__(self, cache_manager: SmartCacheManager, delay_manager: AdaptiveDelayManager):
        self.cache = cache_manager
        self.delay = delay_manager
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ArticleAnalyzer/3.0 (colab-user@example.com)',
            'Accept': 'application/json',
            'Accept-Encoding': 'gzip'
        })

    def make_request(self, url: str, cache_key: str, params: Dict = None,
                    timeout: int = Config.REQUEST_TIMEOUT, category: str = "api") -> Dict:

        full_cache_key = f"{url}:{hash(str(params) if params else '')}"

        cached_data = self.cache.get(category, full_cache_key)
        if cached_data is not None:
            return cached_data

        wait_time = self.delay.wait_if_needed()

        try:
            start_time = time.time()
            response = self.session.get(url, params=params, timeout=timeout)
            response_time = time.time() - start_time

            if response.status_code == 200:
                data = response.json()

                self.cache.set(category, full_cache_key, data)
                self.delay.update_delay(True, response_time)
                return data

            elif response.status_code == 429:
                self.delay.current_delay = min(self.delay.max_delay, self.delay.current_delay * 1.5)
                self.delay.update_delay(False, response_time)
                return {"error": f"Rate limit exceeded, wait {self.delay.current_delay:.1f}s", "status": 429}

            else:
                self.delay.update_delay(False, response_time)
                return {"error": f"API error {response.status_code}", "status": response.status_code}

        except requests.exceptions.Timeout:
            self.delay.update_delay(False, Config.REQUEST_TIMEOUT)
            return {"error": "Request timeout"}
        except Exception as e:
            self.delay.update_delay(False, 0)
            return {"error": f"Request failed: {str(e)}"}

class CrossrefClient(APIClient):
    def __init__(self, cache_manager: SmartCacheManager, delay_manager: AdaptiveDelayManager):
        super().__init__(cache_manager, delay_manager)
        self.base_url = Config.CROSSREF_URL

    def fetch_article(self, doi: str) -> Dict:
        clean_doi = self._clean_doi(doi)
        if not clean_doi:
            return {"error": "Invalid DOI"}

        url = f"{self.base_url}{clean_doi}"
        return self.make_request(url, f"crossref:{clean_doi}", category="crossref")

    def fetch_references(self, doi: str) -> List[str]:
        clean_doi = self._clean_doi(doi)
        if not clean_doi:
            return []

        data = self.fetch_article(clean_doi)
        references = []

        if 'message' in data and 'reference' in data['message']:
            for ref in data['message']['reference']:
                if 'DOI' in ref and ref['DOI']:
                    ref_doi = self._clean_doi(ref['DOI'])
                    if ref_doi:
                        references.append(ref_doi)

        return references

    def fetch_citations(self, doi: str) -> List[str]:
        clean_doi = self._clean_doi(doi)
        if not clean_doi:
            return []

        citing_dois = []
        try:
            url = f"{self.base_url}{clean_doi}"
            params = {'filter': 'has-reference:1'}
            data = self.make_request(url, f"crossref_citations:{clean_doi}", params=params)

            if 'message' in data and 'is-referenced-by' in data['message']:
                references = data['message']['is-referenced-by']
                for ref in references:
                    if isinstance(ref, dict) and 'DOI' in ref:
                        citing_doi = self._clean_doi(ref['DOI'])
                        if citing_doi:
                            citing_dois.append(citing_doi)

        except Exception as e:
            st.warning(f"Crossref citations error for {doi}: {e}")

        return citing_dois

    def _clean_doi(self, doi: str) -> str:
        if not doi or not isinstance(doi, str):
            return ""

        doi = doi.strip()
        prefixes = ['doi:', 'DOI:', 'https://doi.org/', 'http://doi.org/', 'https://dx.doi.org/', 'http://dx.doi.org/']

        for prefix in prefixes:
            if doi.lower().startswith(prefix.lower()):
                doi = doi[len(prefix):]

        return doi.strip()

# ============================================================================
# üåê –ö–õ–ê–°–° –ö–õ–ò–ï–ù–¢–û–í API (–û–ë–ù–û–í–õ–ï–ù–ù–´–ô)
# ============================================================================

class OpenAlexClient(APIClient):
    def __init__(self, cache_manager: SmartCacheManager, delay_manager: AdaptiveDelayManager):
        super().__init__(cache_manager, delay_manager)
        self.base_url = Config.OPENALEX_URL
        self.works_url = Config.OPENALEX_WORKS_URL

    def fetch_article(self, doi: str) -> Dict:
        clean_doi = self._clean_doi(doi)
        if not clean_doi:
            return {"error": "Invalid DOI"}

        url = f"{self.base_url}{clean_doi}"
        return self.make_request(url, f"openalex:{clean_doi}", category="openalex")

    def fetch_citations(self, doi: str, max_pages: int = 10) -> List[str]:
        """
        –°—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ —Å–±–æ—Ä–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è reference –∏ citing articles
        –°–æ–±–∏—Ä–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–æ 2000 —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π (200 * max_pages)
        """
        clean_doi = self._clean_doi(doi)
        if not clean_doi:
            return []

        citing_dois = []

        try:
            article_data = self.fetch_article(clean_doi)
            if 'error' in article_data:
                return []

            article_id = article_data.get('id', '').split('/')[-1]
            if not article_id:
                return []

            params = {
                'filter': f'cites:{article_id}',
                'per-page': 200,
                'select': 'doi,title,publication_year'
            }

            page = 1
            has_more = True

            while has_more and page <= max_pages:
                self.delay.wait_if_needed()

                response = self.session.get(self.works_url, params=params)
                if response.status_code == 200:
                    data = response.json()

                    for work in data.get('results', []):
                        if work.get('doi'):
                            citing_doi = self._clean_doi(work['doi'])
                            if citing_doi:
                                citing_dois.append(citing_doi)

                    if 'meta' in data and data['meta'].get('next_cursor'):
                        params['cursor'] = data['meta']['next_cursor']
                        page += 1
                        time.sleep(0.1)
                    else:
                        has_more = False
                else:
                    has_more = False

        except Exception as e:
            st.warning(f"OpenAlex citations error for {doi}: {e}")

        return list(set(citing_dois))

    def fetch_all_citations_for_analyzed_article(self, doi: str) -> List[str]:
        """
        –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ü–æ–ª–Ω—ã–π —Å–±–æ—Ä –í–°–ï–• —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –¥–ª—è analyzed articles
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç cursor-based –ø–∞–≥–∏–Ω–∞—Ü–∏—é –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        """
        clean_doi = self._clean_doi(doi)
        if not clean_doi:
            return []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è –ø–æ–ª–Ω—ã—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        cache_key = f"full_citations:{clean_doi}"
        cached_result = self.cache.get("full_citations", cache_key)
        if cached_result is not None:
            return cached_result

        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º work_id –∏–∑ DOI
            article_data = self.fetch_article(clean_doi)
            if 'error' in article_data:
                return []

            article_id = article_data.get('id', '').split('/')[-1]
            if not article_id:
                return []

            all_citing_dois = []
            cursor = "*"
            page_num = 1
            max_retries = 3
            total_collected = 0

            while cursor:
                for attempt in range(max_retries):
                    try:
                        url = f"{self.works_url}?filter=cites:{article_id}&per-page=200&cursor={cursor}"
                        self.delay.wait_if_needed()

                        start_time = time.time()
                        response = self.session.get(url, timeout=45)
                        response_time = time.time() - start_time

                        if response.status_code == 200:
                            self.delay.update_delay(True, response_time)
                            data = response.json()

                            if not isinstance(data, dict):
                                st.warning(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num} –¥–ª—è {clean_doi}")
                                break

                            works = data.get('results', [])

                            if not works:
                                cursor = None
                                break

                            page_citing_dois = []
                            for work in works:
                                if isinstance(work, dict) and work.get('doi'):
                                    citing_doi = self._clean_doi(work['doi'])
                                    if citing_doi:
                                        page_citing_dois.append(citing_doi)

                            all_citing_dois.extend(page_citing_dois)
                            total_collected += len(page_citing_dois)

                            # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∫—É—Ä—Å–æ—Ä
                            meta = data.get('meta', {})
                            next_cursor = meta.get('next_cursor')

                            if next_cursor:
                                cursor = next_cursor
                                page_num += 1
                                time.sleep(0.5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è rate limits
                            else:
                                cursor = None

                            break  # –£—Å–ø–µ—à–Ω–æ, –≤—ã—Ö–æ–¥–∏–º –∏–∑ retry —Ü–∏–∫–ª–∞

                        elif response.status_code == 429:
                            self.delay.update_delay(False, response_time)
                            wait_time = 2 ** (attempt + 1)  # Exponential backoff
                            time.sleep(wait_time)
                            continue

                        elif response.status_code == 404:
                            st.warning(f"‚ö†Ô∏è –°—Ç–∞—Ç—å—è {clean_doi} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ OpenAlex")
                            cursor = None
                            break

                        else:
                            self.delay.update_delay(False, response_time)
                            time.sleep(5)
                            continue

                    except requests.exceptions.Timeout:
                        time.sleep(5)
                        continue

                    except Exception as e:
                        time.sleep(5)
                        continue

                else:  # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
                    break

            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            unique_citing_dois = list(set(all_citing_dois))

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à —Å –æ—Ç–¥–µ–ª—å–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π –¥–ª—è –ø–æ–ª–Ω—ã—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
            self.cache.set("full_citations", cache_key, unique_citing_dois, category="full_citations_analyzed")

            return unique_citing_dois

        except Exception as e:
            st.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –¥–ª—è {clean_doi}: {str(e)}")
            return []

    def _safe_get(self, data, *keys, default=''):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ —Å–ª–æ–≤–∞—Ä—è (–≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è)"""
        if not isinstance(data, dict):
            return default

        current = data
        for key in keys:
            if isinstance(current, dict):
                current = current.get(key)
            else:
                return default

        return current if current is not None else default

    def _clean_doi(self, doi: str) -> str:
        if not doi or not isinstance(doi, str):
            return ""

        doi = doi.strip()
        prefixes = ['doi:', 'DOI:', 'https://doi.org/', 'http://doi.org/', 'https://dx.doi.org/', 'http://dx.doi.org/']

        for prefix in prefixes:
            if doi.lower().startswith(prefix.lower()):
                doi = doi[len(prefix):]

        return doi.strip()

class RORClient:
    def __init__(self, cache_manager: SmartCacheManager):
        self.cache = cache_manager
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ArticleAnalyzer-ROR/3.0 (colab-user@example.com)',
            'Accept': 'application/json'
        })
        self.last_request_time = 0
        self.min_delay = 0.3

    def _respect_delay(self):
        elapsed = time.time() - self.last_request_time
        if elapsed < self.min_delay:
            time.sleep(self.min_delay - elapsed)
        self.last_request_time = time.time()

    def search_organization(self, query: str, category: str = "summary") -> Dict[str, str]:
        if not query or len(query.strip()) < 2:
            return self._create_empty_result()

        cache_key = f"ror_search:{query.strip().lower()}"

        if category != "summary":
            cached = self.cache.get_ror_cache(category, cache_key)
            if cached is not None:
                return cached

        cached = self.cache.get("ror_search", cache_key)
        if cached is not None:
            if cached.get('ror_id'):
                if category != "summary":
                    self.cache.set_ror_cache(category, cache_key, cached)
                return cached

        self._respect_delay()

        try:
            response = self.session.get(
                Config.ROR_API_URL,
                params={'query': query.strip()},
                timeout=10
            )

            if response.status_code != 200:
                return self._create_empty_result()

            data = response.json()
            items = data.get('items', [])

            if not items:
                return self._create_empty_result()

            best = self._improved_find_best_match(query.strip(), items)
            if not best:
                return self._create_empty_result()

            colab_url = ""
            try:
                ror_id = best['id'].split('/')[-1]
                colab_url = f"https://colab.ws/organizations/{ror_id}"
            except:
                pass

            website = ""
            try:
                links = best.get('links', []) or []
                for link in links:
                    url = (link.get('value') or link.get('url') if isinstance(link, dict) else str(link)) if link else None
                    if url and isinstance(url, str):
                        url = url.strip()
                        website = url if url.startswith('http') else 'https://' + url
                        break
            except:
                pass

            result = {
                'ror_id': colab_url,
                'website': website,
                'score': best.get('score', 0),
                'name': best.get('name', ''),
                'acronyms': best.get('acronyms', [])
            }

            if colab_url:
                self.cache.set("ror_search", cache_key, result, category="ror_search")
                if category != "summary":
                    self.cache.set_ror_cache(category, cache_key, result)

            return result

        except Exception as e:
            st.warning(f"ROR error for query '{query}': {e}")
            return self._create_empty_result()

    def _improved_find_best_match(self, query: str, items: List[Dict]) -> Optional[Dict]:
        if not items:
            return None

        q = query.strip().lower()
        best_item = None
        best_score = -1

        strategies = [
            self._strategy_exact_match,
            self._strategy_partial_match,
            self._strategy_acronym_match,
            self._strategy_fuzzy_match
        ]

        for item in items:
            score = 0
            name = item.get('name', '').lower()
            aliases = [a.lower() for a in item.get('aliases', [])]
            acronyms = [a.lower() for a in item.get('acronyms', []) if a]

            for strategy in strategies:
                strategy_score = strategy(q, name, aliases, acronyms)
                if strategy_score > score:
                    score = strategy_score

            ror_score = item.get('score', 0) * 50

            final_score = max(score, ror_score)

            if final_score > best_score:
                best_score = final_score
                best_item = item

        return best_item

    def _strategy_exact_match(self, query: str, name: str, aliases: List[str], acronyms: List[str]) -> int:
        if query == name or query in aliases:
            return 10000
        return 0

    def _strategy_partial_match(self, query: str, name: str, aliases: List[str], acronyms: List[str]) -> int:
        all_texts = [name] + aliases + acronyms
        for text in all_texts:
            if query in text or text in query:
                return 9000
        return 0

    def _strategy_acronym_match(self, query: str, name: str, aliases: List[str], acronyms: List[str]) -> int:
        if query in acronyms:
            return 9500
        return 0

    def _strategy_fuzzy_match(self, query: str, name: str, aliases: List[str], acronyms: List[str]) -> int:
        all_texts = [name] + aliases
        best_fuzzy = 0
        for text in all_texts:
            if text:
                score = fuzz.token_set_ratio(query, text)
                if score > best_fuzzy:
                    best_fuzzy = score
        return best_fuzzy

    def _create_empty_result(self) -> Dict[str, str]:
        return {
            'ror_id': '',
            'website': '',
            'score': 0,
            'name': '',
            'acronyms': []
        }

# ============================================================================
# üõ†Ô∏è –ö–õ–ê–°–° –û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–•
# ============================================================================

class DataProcessor:
    def __init__(self, cache_manager: SmartCacheManager):
        self.cache = cache_manager
        self.country_codes = Config.COUNTRY_CODES

    def extract_article_info(self, crossref_data: Dict, openalex_data: Dict,
                           doi: str, references: List[str], citations: List[str]) -> Dict:

        pub_info = self._extract_publication_info(crossref_data, openalex_data)
        authors, countries_from_auth = self._extract_authors_info(crossref_data, openalex_data)
        countries = self._extract_countries_info(authors, openalex_data)

        country_codes = [self._country_to_code(c) for c in countries]
        country_codes = list(set(filter(None, country_codes)))

        orcid_urls = []
        for author in authors:
            if author.get('orcid'):
                orcid_url = self._format_orcid_id(author['orcid'])
                if orcid_url:
                    orcid_urls.append(orcid_url)

        pages_field = pub_info['pages']
        if not pages_field and pub_info['article_number']:
            pages_field = f"Article {pub_info['article_number']}"

        quick_insights = self._extract_quick_insights(
            authors, countries, references, citations, pub_info
        )

        return {
            'doi': doi,
            'publication_info': pub_info,
            'authors': authors,
            'countries': country_codes,
            'orcid_urls': orcid_urls,
            'references': references,
            'citations': citations,
            'pages_formatted': pages_field,
            'status': 'success',
            'quick_insights': quick_insights
        }

    def _extract_quick_insights(self, authors: List[Dict], countries: List[str],
                               references: List[str], citations: List[str],
                               pub_info: Dict) -> Dict:
        current_year = datetime.now().year

        try:
            pub_year = int(pub_info.get('year', current_year))
            article_age = current_year - pub_year
        except:
            article_age = 0

        insights = {
            'author_count': len(authors),
            'country_count': len(countries),
            'reference_count': len(references),
            'citation_count': len(citations),
            'publication_year': pub_info.get('year', ''),
            'article_age': article_age,
            'citation_velocity': 0,
            'geographic_diversity': len(countries) / max(1, len(authors)),
            'self_citation_risk': 0,
            'intra_affiliation_citation_ratio': 0
        }

        if article_age > 0 and citations:
            insights['citation_velocity'] = len(citations) / article_age

        return insights

    def _extract_publication_info(self, crossref_data: Dict, openalex_data: Dict) -> Dict:
        pub_info = {
            'title': '',
            'journal': '',
            'publication_date': '',
            'year': '',
            'volume': '',
            'pages': '',
            'article_number': '',
            'citation_count_crossref': 0,
            'citation_count_openalex': 0,
            'doi': ''
        }

        if 'message' in crossref_data:
            msg = crossref_data['message']

            pub_info['doi'] = msg.get('DOI', '')
            pub_info['title'] = msg.get('title', [''])[0] if msg.get('title') else ''
            pub_info['journal'] = msg.get('container-title', [''])[0] if msg.get('container-title') else ''

            # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ Crossref
            pub_date = None
            if 'created' in msg and 'date-parts' in msg['created']:
                created_date = msg.get('created', {})
                if 'date-parts' in created_date and created_date['date-parts']:
                    pub_date = created_date['date-parts'][0]

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ created, —Ç–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º license –∫–∞–∫ fallback
            if not pub_date and 'license' in msg:
                for license_item in msg['license']:
                    if isinstance(license_item, dict) and 'start' in license_item:
                        start_date = license_item.get('start', {})
                        if 'date-parts' in start_date and start_date['date-parts']:
                            pub_date = start_date['date-parts'][0]
                            break

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ license, –∏—â–µ–º –≤ created
            if not pub_date and 'created' in msg:
                created_date = msg.get('created', {})
                if 'date-parts' in created_date and created_date['date-parts']:
                    pub_date = created_date['date-parts'][0]

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ created, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É
            if not pub_date and 'published' in msg and 'date-parts' in msg['published']:
                pub_date = msg['published']['date-parts'][0]

            if pub_date:
                pub_info['year'] = str(pub_date[0])
                if len(pub_date) >= 2:
                    month = str(pub_date[1]).zfill(2)
                    if len(pub_date) >= 3:
                        day = str(pub_date[2]).zfill(2)
                        pub_info['publication_date'] = f"{pub_info['year']}-{month}-{day}"
                    else:
                        pub_info['publication_date'] = f"{pub_info['year']}-{month}-15"
                else:
                    pub_info['publication_date'] = f"{pub_info['year']}-01-01"

            pub_info['volume'] = msg.get('volume', '')
            pub_info['pages'] = msg.get('page', '')
            pub_info['article_number'] = msg.get('article-number', '')
            pub_info['citation_count_crossref'] = msg.get('is-referenced-by-count', 0)

        if 'title' in openalex_data:
            pub_info['title'] = pub_info['title'] or openalex_data.get('title', '')

        if 'primary_location' in openalex_data:
            source = openalex_data['primary_location'].get('source', {})
            pub_info['journal'] = pub_info['journal'] or source.get('display_name', '')

        if 'publication_year' in openalex_data:
            pub_info['year'] = pub_info['year'] or str(openalex_data.get('publication_year', ''))

        if 'biblio' in openalex_data:
            biblio = openalex_data['biblio']
            pub_info['volume'] = pub_info['volume'] or biblio.get('volume', '')
            if not pub_info['pages']:
                pub_info['pages'] = biblio.get('first_page', '') + '-' + biblio.get('last_page', '') \
                                    if biblio.get('first_page') else biblio.get('pages', '')

        pub_info['citation_count_openalex'] = openalex_data.get('cited_by_count', 0)

        return pub_info

    def _extract_authors_info(self, crossref_data: Dict, openalex_data: Dict) -> Tuple[List[Dict], List[str]]:
        authors = []
        countries = []

        try:
            if openalex_data and 'authorships' in openalex_data:
                for authorship in openalex_data['authorships']:
                    if not authorship:
                        continue

                    author_display = authorship.get('author', {})
                    full_name = authorship.get('raw_author_name') or author_display.get('display_name', '')

                    if not full_name:
                        continue

                    author_info = {
                        'name': full_name,
                        'affiliation': [],
                        'orcid': author_display.get('orcid', '')
                    }

                    institutions = authorship.get('institutions', [])
                    if institutions:
                        for inst in institutions:
                            if inst and isinstance(inst, dict):
                                display_name = inst.get('display_name')
                                if display_name:
                                    clean_aff = self._clean_affiliation(display_name)
                                    if clean_aff:
                                        author_info['affiliation'].append(clean_aff)

                                country_code = inst.get('country_code')
                                if country_code:
                                    countries.append(country_code)

                    authors.append(author_info)
        except Exception as e:
            st.warning(f"‚ö†Ô∏è OpenAlex author extraction error: {e}")

        if not authors and crossref_data:
            try:
                message = crossref_data.get('message', {})
                crossref_authors = message.get('author', [])

                if crossref_authors:
                    for author_obj in crossref_authors:
                        if not author_obj:
                            continue

                        given = author_obj.get('given', '')
                        family = author_obj.get('family', '')
                        full_name = f"{given} {family}".strip()

                        if not full_name:
                            continue

                        author_info = {
                            'name': full_name,
                            'affiliation': [],
                            'orcid': author_obj.get('ORCID', '')
                        }

                        affiliations = author_obj.get('affiliation', [])
                        if affiliations:
                            for affil in affiliations:
                                if affil and isinstance(affil, dict):
                                    affil_name = affil.get('name')
                                    if affil_name:
                                        clean_aff = self._clean_affiliation(affil_name)
                                        if clean_aff:
                                            author_info['affiliation'].append(clean_aff)

                        authors.append(author_info)
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Crossref author extraction error: {e}")

        return authors, list(set(countries))

    def _extract_author_from_crossref(self, full_name: Optional[str], crossref_data: Dict, author_obj: Dict = None) -> Optional[Dict]:
        if author_obj is None:
            return None

        given = author_obj.get('given', '')
        family = author_obj.get('family', '')
        name = f"{given} {family}".strip() if given or family else full_name

        if not name:
            return None

        author_info = {
            'name': name,
            'affiliation': [],
            'orcid': author_obj.get('ORCID', '')
        }

        if 'affiliation' in author_obj:
            for affil in author_obj['affiliation']:
                if 'name' in affil:
                    clean_aff = self._clean_affiliation(affil['name'])
                    if clean_aff and clean_aff not in author_info['affiliation']:
                        author_info['affiliation'].append(clean_aff)

        return author_info

    def _clean_affiliation(self, affiliation: str) -> str:
        if not affiliation:
            return ""

        patterns_to_remove = [
            r',\s*[A-Z]{2}$',
            r',\s*[A-Z]{2}\s*\d+',
            r',\s*USA$', r',\s*United States$',
            r',\s*UK$', r',\s*United Kingdom$',
            r',\s*China$', r',\s*–†–æ—Å—Å–∏—è$', r',\s*Russia$',
            r'\s*\([^)]*[Cc]ountry[^)]*\)',
            r'\s*\[[^\]]*[Cc]ountry[^\]]*\]',
            r'\b\d{5,6}(-\d{4})?\b',
        ]

        clean_aff = affiliation
        for pattern in patterns_to_remove:
            clean_aff = re.sub(pattern, '', clean_aff, flags=re.IGNORECASE)

        clean_aff = re.sub(r',\s*,', ',', clean_aff)
        clean_aff = clean_aff.strip(' ,;')

        return clean_aff if clean_aff and len(clean_aff) > 2 else affiliation

    def _extract_countries_info(self, authors: List[Dict], openalex_data: Dict) -> List[str]:
        countries = []

        if 'authorships' in openalex_data:
            for authorship in openalex_data['authorships']:
                for inst in authorship.get('institutions', []):
                    if 'country_code' in inst and inst['country_code']:
                        countries.append(inst['country_code'])

        if not countries:
            for author in authors:
                for affil in author['affiliation']:
                    for country_name, country_code in self.country_codes.items():
                        if country_name.lower() in affil.lower():
                            countries.append(country_code)
                            break

        return list(set(countries))

    def _country_to_code(self, country_name: str) -> str:
        if not country_name:
            return ""

        for name, code in self.country_codes.items():
            if country_name.lower() == name.lower():
                return code

        for name, code in self.country_codes.items():
            if name.lower() in country_name.lower():
                return code

        return country_name[:2].upper() if len(country_name) >= 2 else country_name.upper()

    def _format_orcid_id(self, orcid_id: str) -> str:
        if not orcid_id:
            return ""

        if orcid_id.startswith('https://orcid.org/'):
            return orcid_id

        clean_id = re.sub(r'[^\dXx-]', '', orcid_id.strip())

        if '-' in clean_id:
            return f"https://orcid.org/{clean_id}"
        elif len(clean_id) == 16:
            formatted = f"{clean_id[:4]}-{clean_id[4:8]}-{clean_id[8:12]}-{clean_id[12:]}"
            return f"https://orcid.org/{formatted}"
        else:
            return f"https://orcid.org/{clean_id}"

    def normalize_author_name(self, full_name: str) -> str:
        if not full_name:
            return ""

        name = re.sub(r'\s+', ' ', full_name.strip().replace(',', ' '))
        parts = name.split()

        if len(parts) == 0:
            return ""
        if len(parts) == 1:
            return parts[0]

        family = parts[-1]

        for part in parts[:-1]:
            clean = re.sub(r'[^A-Za-z–ê-—è–Å—ë]', '', part)
            if clean and clean[0].isalpha():
                initial = clean[0].upper()
                return f"{family} {initial}"

        return family

# ============================================================================
# üéØ –ö–õ–ê–°–° –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–ò DOI (–ù–û–í–´–ô)
# ============================================================================

class OptimizedDOIProcessor:
    def __init__(self, cache_manager: SmartCacheManager,
                 delay_manager: AdaptiveDelayManager,
                 data_processor: DataProcessor,
                 failed_tracker: FailedDOITracker):

        self.cache = cache_manager
        self.delay = delay_manager
        self.data_processor = data_processor
        self.failed_tracker = failed_tracker

        self.crossref_client = CrossrefClient(cache_manager, delay_manager)
        self.openalex_client = OpenAlexClient(cache_manager, delay_manager)
        self.ror_client = RORClient(cache_manager)

        self.processed_dois = {}
        self.reference_relationships = defaultdict(list)
        self.citation_relationships = defaultdict(list)

        self.author_affiliation_map = defaultdict(set)
        self.doi_author_map = defaultdict(list)
        self.doi_affiliation_map = defaultdict(set)

        self.terminology_analyzer = None  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–∑–∂–µ

        self.stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'cached_hits': 0,
            'api_calls': 0
        }

    def set_terminology_analyzer(self, terminology_analyzer):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏"""
        self.terminology_analyzer = terminology_analyzer

    def process_doi_batch(self, dois: List[str], source_type: str = "analyzed",
                         original_doi: str = None, fetch_refs: bool = True,
                         fetch_cites: bool = True, batch_size: int = Config.BATCH_SIZE,
                         progress_container=None) -> Dict[str, Dict]:

        results = {}
        total_batches = (len(dois) + batch_size - 1) // batch_size

        if progress_container:
            status_text = progress_container.text(f"üîß –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(dois)} DOI (–∏—Å—Ç–æ—á–Ω–∏–∫: {source_type})")
            progress_bar = progress_container.progress(0)
        else:
            status_text = None
            progress_bar = None

        monitor = ProgressMonitor(len(dois), f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {source_type}", progress_bar, status_text)

        for batch_idx in range(0, len(dois), batch_size):
            batch = dois[batch_idx:batch_idx + batch_size]
            batch_results = self._process_single_batch(
                batch, source_type, original_doi, True, True
            )

            results.update(batch_results)

            monitor.update(len(batch), 'processed')

            batch_success = sum(1 for r in batch_results.values() if r.get('status') == 'success')

        monitor.complete()

        successful = sum(1 for r in results.values() if r.get('status') == 'success')
        failed = len(dois) - successful

        self.stats['total_processed'] += len(dois)
        self.stats['successful'] += successful
        self.stats['failed'] += failed

        return results

    def _process_single_batch(self, batch: List[str], source_type: str,
                             original_doi: str, fetch_refs: bool, fetch_cites: bool) -> Dict[str, Dict]:
        results = {}

        with ThreadPoolExecutor(max_workers=min(Config.MAX_WORKERS, len(batch))) as executor:
            future_to_doi = {}

            for doi in batch:
                future = executor.submit(
                    self._process_single_doi_wrapper,
                    doi, source_type, original_doi, True, True
                )
                future_to_doi[future] = doi

            for future in as_completed(future_to_doi):
                doi = future_to_doi[future]
                try:
                    results[doi] = future.result(timeout=60)
                except Exception as e:
                    self._handle_processing_error(doi, str(e), source_type, original_doi)
                    results[doi] = {
                        'doi': doi,
                        'status': 'failed',
                        'error': f"–¢–∞–π–º–∞—É—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
                    }

        return results

    def _process_single_doi_wrapper(self, doi: str, source_type: str,
                                   original_doi: str, fetch_refs: bool, fetch_cites: bool) -> Dict:
        try:
            return self._process_single_doi_optimized(
                doi, source_type, original_doi, True, True
            )
        except Exception as e:
            self._handle_processing_error(doi, str(e), source_type, original_doi)
            return {
                'doi': doi,
                'status': 'failed',
                'error': f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
            }

    def _process_single_doi_optimized(self, doi: str, source_type: str,
                                     original_doi: str, fetch_refs: bool, fetch_cites: bool) -> Dict:

        cache_key = f"full_result:{doi}"
        cached_result = self.cache.get("full_analysis", cache_key)

        if cached_result is not None:
            self.stats['cached_hits'] += 1
            return cached_result

        crossref_data = {}
        openalex_data = {}

        try:
            crossref_data = self.crossref_client.fetch_article(doi)
            openalex_data = self.openalex_client.fetch_article(doi)
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {str(e)}"
            self._handle_processing_error(doi, error_msg, source_type, original_doi)
            return {
                'doi': doi,
                'status': 'failed',
                'error': error_msg
            }

        crossref_error = None
        openalex_error = None

        if isinstance(crossref_data, dict):
            crossref_error = crossref_data.get('error')
        if isinstance(openalex_data, dict):
            openalex_error = openalex_data.get('error')

        if crossref_error and openalex_error:
            error_msg = f"–û—à–∏–±–∫–∏ API: Crossref - {crossref_error}, OpenAlex - {openalex_error}"
            self._handle_processing_error(doi, error_msg, source_type, original_doi)
            return {
                'doi': doi,
                'status': 'failed',
                'error': error_msg
            }

        crossref_data = crossref_data if isinstance(crossref_data, dict) else {}
        openalex_data = openalex_data if isinstance(openalex_data, dict) else {}

        references = []
        try:
            refs = self.crossref_client.fetch_references(doi)
            references = refs if isinstance(refs, list) else []

            if references:
                self.reference_relationships[doi] = references
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error fetching references for {doi}: {e}")

        citations = []
        try:
            # –í–ê–ñ–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –†–∞–∑–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å–±–æ—Ä–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Å—Ç–∞—Ç—å–∏
            if source_type == "analyzed":
                # –î–ª—è analyzed articles: —Å–æ–±–∏—Ä–∞–µ–º –í–°–ï —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –Ω–æ–≤—É—é –ª–æ–≥–∏–∫—É
                cites_openalex = self.openalex_client.fetch_all_citations_for_analyzed_article(doi)

                # –¢–∞–∫–∂–µ –ø–æ–ª—É—á–∞–µ–º —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ Crossref –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö
                cites_crossref = self.crossref_client.fetch_citations(doi)
                cites_crossref = cites_crossref if isinstance(cites_crossref, list) else []

                citations = list(set(cites_openalex + cites_crossref))

                if citations:
                    self.citation_relationships[doi] = citations

            else:
                # –î–ª—è reference –∏ citing articles: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É (—Ç–æ–ª—å–∫–æ –¥–æ 2000)
                cites_openalex = self.openalex_client.fetch_citations(doi)
                cites_crossref = self.crossref_client.fetch_citations(doi)

                cites_openalex = cites_openalex if isinstance(cites_openalex, list) else []
                cites_crossref = cites_crossref if isinstance(cites_crossref, list) else []

                citations = list(set(cites_openalex + cites_crossref))

                if citations:
                    self.citation_relationships[doi] = citations
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error fetching citations for {doi}: {e}")
            # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –¥–ª—è analyzed —Å—Ç–∞—Ç—å–∏,
            # –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É –∫–∞–∫ fallback
            if source_type == "analyzed":
                try:
                    cites_openalex = self.openalex_client.fetch_citations(doi)
                    cites_crossref = self.crossref_client.fetch_citations(doi)
                    cites_openalex = cites_openalex if isinstance(cites_openalex, list) else []
                    cites_crossref = cites_crossref if isinstance(cites_crossref, list) else []
                    citations = list(set(cites_openalex + cites_crossref))

                    if citations:
                        self.citation_relationships[doi] = citations
                except Exception as e2:
                    st.warning(f"‚ùå Fallback —Ç–æ–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª –¥–ª—è {doi}: {e2}")

        result = self.data_processor.extract_article_info(
            crossref_data, openalex_data, doi, references, citations
        )

        # –ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏ - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤
        if result.get('status') == 'success' and self.terminology_analyzer:
            title = result['publication_info'].get('title', '')
            year = result['publication_info'].get('year', '')
            if title and year:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
                terms = self.terminology_analyzer.extract_terms_from_title(title)
                if terms:
                    # –ü–µ—Ä–µ–¥–∞—á–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
                    self.terminology_analyzer.process_terms(
                        doi=doi,
                        terms=terms,
                        year=year,
                        article_type=source_type
                    )

        if result.get('status') == 'success':
            for author in result.get('authors', []):
                author_name = author.get('name', '')
                if author_name:
                    self.doi_author_map[doi].append(author_name)
                    for affiliation in author.get('affiliation', []):
                        if affiliation:
                            self.author_affiliation_map[author_name].add(affiliation)
                            self.doi_affiliation_map[doi].add(affiliation)

        if result.get('status') == 'success':
            self.stats['successful'] += 1

            self.cache.set("full_analysis", cache_key, result, category="full_analysis")

            self.cache.update_popularity(doi)
        else:
            self.stats['failed'] += 1

        self.stats['api_calls'] += 2

        return result

    def _handle_processing_error(self, doi: str, error: str, source_type: str, original_doi: str):

        related_dois = []
        if original_doi:
            related_dois.append(original_doi)

        self.failed_tracker.add_failed_doi(
            doi=doi,
            error=error,
            source_type=source_type,
            related_dois=related_dois,
            original_doi=original_doi
        )

        self.cache.mark_as_failed("full_analysis", doi, error)

    def collect_all_references(self, results: Dict[str, Dict]) -> List[str]:
        all_refs = []

        for doi, result in results.items():
            if result.get('status') == 'success':
                refs = result.get('references', [])
                if refs:
                    all_refs.extend(refs)

        for doi, result in results.items():
            if result.get('status') == 'success':
                refs = result.get('references', [])
                if refs:
                    for ref_doi in refs:
                        if ref_doi not in self.reference_relationships:
                            self.reference_relationships[ref_doi] = []
                        if doi not in self.reference_relationships[ref_doi]:
                            self.reference_relationships[ref_doi].append(doi)

        return all_refs

    def collect_all_citations(self, results: Dict[str, Dict]) -> List[str]:
        all_cites = []

        for doi, result in results.items():
            if result.get('status') == 'success':
                cites = result.get('citations', [])
                if cites:
                    all_cites.extend(cites)

        return all_cites

    def get_relationships(self) -> Dict[str, Any]:
        return {
            'reference_relationships': dict(self.reference_relationships),
            'citation_relationships': dict(self.citation_relationships),
            'total_relationships': len(self.reference_relationships) + len(self.citation_relationships)
        }

    def get_insights_data(self) -> Dict[str, Any]:
        return {
            'author_affiliation_map': dict(self.author_affiliation_map),
            'doi_author_map': dict(self.doi_author_map),
            'doi_affiliation_map': dict(self.doi_affiliation_map)
        }

    def get_stats(self) -> Dict[str, Any]:
        return {
            'total_processed': self.stats['total_processed'],
            'successful': self.stats['successful'],
            'failed': self.stats['failed'],
            'cached_hits': self.stats['cached_hits'],
            'api_calls': self.stats['api_calls'],
            'cache_efficiency': round((self.stats['cached_hits'] / max(1, self.stats['total_processed'])) * 100, 1),
            'success_rate': round((self.stats['successful'] / max(1, self.stats['total_processed'])) * 100, 1)
        }

    def retry_failed_dois(self, failed_tracker: FailedDOITracker, max_retries: int = 1) -> Dict[str, Dict]:
        retry_results = {}

        rate_limit_dois = []
        for doi, info in failed_tracker.failed_dois.items():
            if 'Rate limit exceeded' in info.get('error', ''):
                rate_limit_dois.append(doi)

        if not rate_limit_dois:
            return retry_results

        original_delay = self.delay.current_delay
        self.delay.current_delay = min(Config.MAX_DELAY, original_delay * 1.5)

        retry_results = self.process_doi_batch(
            rate_limit_dois, "retry", None, True, True, Config.BATCH_SIZE
        )

        self.delay.current_delay = original_delay

        successful_retries = sum(1 for r in retry_results.values() if r.get('status') == 'success')

        return retry_results

# ============================================================================
# üîç –ö–õ–ê–°–° –ò–ï–†–ê–†–•–ò–ß–ï–°–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê –î–ê–ù–ù–´–• (–ù–û–í–´–ô)
# ============================================================================

class HierarchicalDataAnalyzer:
    def __init__(self, cache_manager: SmartCacheManager, data_processor: DataProcessor,
                 doi_processor: OptimizedDOIProcessor):
        self.cache = cache_manager
        self.processor = data_processor
        self.doi_processor = doi_processor

        # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —É—Ä–æ–≤–Ω–∏ –¥–∞–Ω–Ω—ã—Ö
        self.data_levels = {
            'level_0': set(),  # DOI –∏ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            'level_1': set(),  # + –∞–≤—Ç–æ—Ä—ã, –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏, –≥–æ–¥—ã
            'level_2': set(),  # + –ø–æ–ª–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ü–∏—Ç–∏—Ä—É—é—â–∏—Ö
            'level_3': set()   # + —Å–µ—Ç–µ–≤–æ–π –∞–Ω–∞–ª–∏–∑ –∏ ML
        }

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.citation_timestamps = defaultdict(list)
        self.journal_citation_counts = defaultdict(Counter)
        self.author_citation_network = defaultdict(set)
        self.affiliation_citation_network = defaultdict(set)

        # ML –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
        self.isolation_forest = None
        self.scaler = StandardScaler()

    def analyze_quick_checks(self, analyzed_results: Dict[str, Dict],
                            citing_results: Dict[str, Dict]) -> List[Dict]:
        """–ë—ã—Å—Ç—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ (5-10 —Å–µ–∫—É–Ω–¥ –Ω–∞ —Å—Ç–∞—Ç—å—é)"""
        quick_checks = []

        for doi, result in analyzed_results.items():
            if result.get('status') != 'success':
                continue

            # –ü–æ–ª—É—á–∞–µ–º —Ü–∏—Ç–∏—Ä—É—é—â–∏–µ —Å—Ç–∞—Ç—å–∏ –¥–ª—è —ç—Ç–æ–π DOI
            citing_dois = result.get('citations', [])
            citing_data = {}
            for cite_doi in citing_dois:
                if cite_doi in citing_results and citing_results[cite_doi].get('status') == 'success':
                    citing_data[cite_doi] = citing_results[cite_doi]

            # –ê–Ω–∞–ª–∏–∑
            analysis = self._perform_quick_check_analysis(doi, result, citing_data)
            quick_checks.append(analysis)

            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self.cache.set_ethical_analysis('quick_checks', doi, analysis)

        return sorted(quick_checks, key=lambda x: x['Quick_Risk_Score'], reverse=True)

    def _perform_quick_check_analysis(self, doi: str, result: Dict,
                                     citing_data: Dict[str, Dict]) -> Dict:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –±—ã—Å—Ç—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""

        pub_info = result['publication_info']
        authors = result.get('authors', [])
        analyzed_countries = result.get('countries', [])

        # 1. Journal Citation Concentration
        journal_concentration = self._calculate_journal_concentration(citing_data)

        # 2. Author Self-Citation Rate
        author_self_citation = self._calculate_author_self_citation(authors, citing_data)

        # 3. Affiliation Self-Citation
        affiliation_self_citation = self._calculate_affiliation_self_citation(authors, citing_data)

        # 4. Single Country Concentration
        single_country = self._calculate_single_country_concentration(citing_data, analyzed_countries)

        # 5. Citation Velocity
        citation_velocity = self._calculate_citation_velocity(result, citing_data)

        # 6. First Year Share
        first_year_share = self._calculate_first_year_share(result, citing_data)

        # 7. Future Citations
        future_citations = self._check_future_citations(result, citing_data)

        # –ü–æ–¥—Å—á–µ—Ç –∫—Ä–∞—Å–Ω—ã—Ö —Ñ–ª–∞–≥–æ–≤
        red_flags = 0
        flags = []

        if journal_concentration > Config.QUICK_CHECK_THRESHOLDS['journal_concentration']:
            red_flags += 1
            flags.append(f"Journal concentration: {journal_concentration:.1%}")

        if author_self_citation > Config.QUICK_CHECK_THRESHOLDS['author_self_citation']:
            red_flags += 1
            flags.append(f"Author self-citation: {author_self_citation:.1%}")

        if affiliation_self_citation > Config.QUICK_CHECK_THRESHOLDS['affiliation_self_citation']:
            red_flags += 1
            flags.append(f"Affiliation self-citation: {affiliation_self_citation:.1%}")

        if single_country > Config.QUICK_CHECK_THRESHOLDS['single_country']:
            red_flags += 1
            flags.append(f"Single country: {single_country:.1%}")

        if citation_velocity > Config.QUICK_CHECK_THRESHOLDS['citation_velocity']:
            red_flags += 1
            flags.append(f"Citation velocity: {citation_velocity:.1f}/year")

        if first_year_share > Config.QUICK_CHECK_THRESHOLDS['first_year_share']:
            red_flags += 1
            flags.append(f"First year share: {first_year_share:.1%}")

        if future_citations > 0:
            red_flags += 1
            flags.append(f"Future citations: {future_citations}")

        # –†–∞—Å—á–µ—Ç —Ä–∏—Å–∫–∞
        quick_risk_score = self._calculate_quick_risk_score(
            journal_concentration, author_self_citation, affiliation_self_citation,
            single_country, citation_velocity, first_year_share, future_citations
        )

        # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        recommended_action = self._determine_recommended_action(quick_risk_score, red_flags)

        return {
            'DOI': doi,
            'Title': pub_info.get('title', '')[:50] + ('...' if len(pub_info.get('title', '')) > 50 else ''),
            'Quick_Risk_Score': quick_risk_score,
            'Red_Flags': red_flags,
            'Flag_1_Journal_Concentration': journal_concentration > Config.QUICK_CHECK_THRESHOLDS['journal_concentration'],
            'Flag_2_Author_Self_Citation': author_self_citation > Config.QUICK_CHECK_THRESHOLDS['author_self_citation'],
            'Flag_3_Affiliation_Self_Citation': affiliation_self_citation > Config.QUICK_CHECK_THRESHOLDS['affiliation_self_citation'],
            'Flag_4_Single_Country': single_country > Config.QUICK_CHECK_THRESHOLDS['single_country'],
            'Flag_5_Citation_Velocity': citation_velocity > Config.QUICK_CHECK_THRESHOLDS['citation_velocity'],
            'Flag_6_First_Year_Share': first_year_share > Config.QUICK_CHECK_THRESHOLDS['first_year_share'],
            'Flag_7_Future_Citations': future_citations > 0,
            'Future_Citations_Count': future_citations,
            'Journal_Concentration_Rate': round(journal_concentration * 100, 1),
            'Author_Self_Citation_Rate': round(author_self_citation * 100, 1),
            'Affiliation_Self_Citation_Rate': round(affiliation_self_citation * 100, 1),
            'Single_Country_Percent': round(single_country * 100, 1),
            'Citation_Velocity_Annual': round(citation_velocity, 1),
            'First_Year_Share_Percent': round(first_year_share * 100, 1),
            'Recommended_Action': recommended_action,
            'Flags_Details': '; '.join(flags) if flags else 'None'
        }

    def _calculate_journal_concentration(self, citing_data: Dict[str, Dict]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –ø–æ –∂—É—Ä–Ω–∞–ª–∞–º"""
        if not citing_data:
            return 0.0

        journal_counter = Counter()
        for cite_result in citing_data.values():
            journal = cite_result.get('publication_info', {}).get('journal', '')
            if journal:
                journal_counter[journal] += 1

        if not journal_counter:
            return 0.0

        total_citations = sum(journal_counter.values())
        top_journal_count = journal_counter.most_common(1)[0][1]

        return top_journal_count / total_citations

    def _calculate_author_self_citation(self, analyzed_authors: List[Dict],
                                       citing_data: Dict[str, Dict]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç self-citation –ø–æ –∞–≤—Ç–æ—Ä–∞–º"""
        if not citing_data or not analyzed_authors:
            return 0.0

        analyzed_author_names = {author['name'] for author in analyzed_authors}
        total_citations = len(citing_data)
        self_citations = 0

        for cite_result in citing_data.values():
            citing_authors = cite_result.get('authors', [])
            citing_author_names = {author['name'] for author in citing_authors}

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—â–∏—Ö –∞–≤—Ç–æ—Ä–æ–≤
            common_authors = analyzed_author_names.intersection(citing_author_names)
            if common_authors:
                self_citations += 1

        return self_citations / total_citations if total_citations > 0 else 0.0

    def _calculate_affiliation_self_citation(self, analyzed_authors: List[Dict],
                                           citing_data: Dict[str, Dict]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç self-citation –ø–æ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏—è–º"""
        if not citing_data or not analyzed_authors:
            return 0.0

        # –°–æ–±–∏—Ä–∞–µ–º –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–π —Å—Ç–∞—Ç—å–∏
        analyzed_affiliations = set()
        for author in analyzed_authors:
            analyzed_affiliations.update(author.get('affiliation', []))

        if not analyzed_affiliations:
            return 0.0

        total_citations = len(citing_data)
        self_citations = 0

        for cite_result in citing_data.values():
            citing_authors = cite_result.get('authors', [])
            citing_affiliations = set()

            for author in citing_authors:
                citing_affiliations.update(author.get('affiliation', []))

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—â–∏—Ö –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–π
            common_affiliations = analyzed_affiliations.intersection(citing_affiliations)
            if common_affiliations:
                self_citations += 1

        return self_citations / total_citations if total_citations > 0 else 0.0

    def _calculate_single_country_concentration(self, citing_data: Dict[str, Dict],
                                              analyzed_countries: List[str]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –ø–æ —Å—Ç—Ä–∞–Ω–∞–º"""
        if not citing_data:
            return 0.0

        country_counter = Counter()
        for cite_result in citing_data.values():
            countries = cite_result.get('countries', [])
            for country in countries:
                if country:
                    country_counter[country] += 1

        if not country_counter:
            return 0.0

        total_citations = sum(country_counter.values())
        top_country_count = country_counter.most_common(1)[0][1]

        return top_country_count / total_citations

    def _calculate_citation_velocity(self, result: Dict, citing_data: Dict[str, Dict]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (—Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ –≥–æ–¥)"""
        if not citing_data:
            return 0.0

        pub_year_str = result.get('publication_info', {}).get('year', '')
        if not pub_year_str:
            return 0.0

        try:
            pub_year = int(pub_year_str)
            current_year = datetime.now().year
            years_passed = max(1, current_year - pub_year)

            return len(citing_data) / years_passed
        except:
            return 0.0

    def _calculate_first_year_share(self, result: Dict, citing_data: Dict[str, Dict]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –¥–æ–ª—é —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ –ø–µ—Ä–≤—ã–π –≥–æ–¥"""
        if not citing_data:
            return 0.0

        pub_year_str = result.get('publication_info', {}).get('year', '')
        if not pub_year_str:
            return 0.0

        try:
            pub_year = int(pub_year_str)
            first_year_citations = 0
            total_citations = len(citing_data)

            for cite_doi, cite_result in citing_data.items():
                cite_year_str = cite_result.get('publication_info', {}).get('year', '')
                if cite_year_str:
                    try:
                        cite_year = int(cite_year_str)
                        if cite_year == pub_year:
                            first_year_citations += 1
                    except:
                        pass

            return first_year_citations / total_citations if total_citations > 0 else 0.0
        except:
            return 0.0

    def _check_future_citations(self, result: Dict, citing_data: Dict[str, Dict]) -> int:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ –±—É–¥—É—â–µ–≥–æ"""
        if not citing_data:
            return 0

        pub_date_str = result.get('publication_info', {}).get('publication_date', '')
        if not pub_date_str:
            return 0

        try:
            pub_date = datetime.strptime(pub_date_str[:10], '%Y-%m-%d')
            future_citations = 0

            for cite_result in citing_data.values():
                cite_date_str = cite_result.get('publication_info', {}).get('publication_date', '')
                if cite_date_str:
                    try:
                        cite_date = datetime.strptime(cite_date_str[:10], '%Y-%m-%d')
                        if cite_date < pub_date:
                            future_citations += 1
                    except:
                        pass

            return future_citations
        except:
            return 0

    def _calculate_quick_risk_score(self, *metrics) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—â–∏–π —Å–∫–æ—Ä–∏–Ω–≥–æ–≤—ã–π —Ä–∏—Å–∫"""
        score = 0

        # –í–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        weights = [20, 15, 15, 10, 10, 15, 15]

        for metric, weight in zip(metrics, weights):
            if isinstance(metric, float):
                score += int(metric * weight)
            elif isinstance(metric, int):
                score += metric * 5

        return min(100, score)

    def _determine_recommended_action(self, risk_score: int, red_flags: int) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–∞"""
        if risk_score > 80 or red_flags >= 5:
            return "IMMEDIATE INVESTIGATION"
        elif risk_score > 60 or red_flags >= 3:
            return "DETAILED REVIEW REQUIRED"
        elif risk_score > 40 or red_flags >= 2:
            return "MONITOR AND REVIEW"
        elif risk_score > 20:
            return "MINOR REVIEW SUGGESTED"
        else:
            return "ETHICALLY ACCEPTABLE"

    def analyze_medium_insights(self, analyzed_results: Dict[str, Dict],
                               citing_results: Dict[str, Dict]) -> List[Dict]:
        """–°—Ä–µ–¥–Ω–∏–µ –∏–Ω—Å–∞–π—Ç—ã (15-30 —Å–µ–∫—É–Ω–¥ –Ω–∞ —Å—Ç–∞—Ç—å—é)"""
        medium_insights = []

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∂—É—Ä–Ω–∞–ª–∞–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        journal_stats = self._collect_journal_statistics(analyzed_results, citing_results)

        for doi, result in analyzed_results.items():
            if result.get('status') != 'success':
                continue

            # –ü–æ–ª—É—á–∞–µ–º —Ü–∏—Ç–∏—Ä—É—é—â–∏–µ —Å—Ç–∞—Ç—å–∏
            citing_dois = result.get('citations', [])
            citing_data = {}
            for cite_doi in citing_dois:
                if cite_doi in citing_results and citing_results[cite_doi].get('status') == 'success':
                    citing_data[cite_doi] = citing_results[cite_doi]

            # –ê–Ω–∞–ª–∏–∑
            analysis = self._perform_medium_insight_analysis(doi, result, citing_data, journal_stats)
            medium_insights.append(analysis)

            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self.cache.set_ethical_analysis('medium_insights', doi, analysis)

        return sorted(medium_insights, key=lambda x: x['Anomaly_Score'], reverse=True)

    def _collect_journal_statistics(self, analyzed_results: Dict[str, Dict],
                                   citing_results: Dict[str, Dict]) -> Dict[str, Dict]:
        """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∂—É—Ä–Ω–∞–ª–∞–º –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
        journal_data = defaultdict(list)

        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º —Å—Ç–∞—Ç—å—è–º
        all_results = list(analyzed_results.values()) + list(citing_results.values())

        for result in all_results:
            if result.get('status') != 'success':
                continue

            pub_info = result.get('publication_info', {})
            journal = pub_info.get('journal', '')
            citation_count = pub_info.get('citation_count_crossref', 0)
            year_str = pub_info.get('year', '')

            if journal and year_str:
                try:
                    year = int(year_str)
                    current_year = datetime.now().year
                    age = max(1, current_year - year)
                    annual_citations = citation_count / age

                    journal_data[journal].append({
                        'annual_citations': annual_citations,
                        'citation_count': citation_count,
                        'year': year
                    })
                except:
                    continue

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ–¥–∏–∞–Ω—ã –∏ –∫–≤–∞—Ä—Ç–∏–ª–∏
        journal_stats = {}
        for journal, data_list in journal_data.items():
            if len(data_list) >= 3:  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 3 —Å—Ç–∞—Ç—å–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                annual_citations = [d['annual_citations'] for d in data_list]
                annual_citations.sort()

                median_index = len(annual_citations) // 2
                q1_index = len(annual_citations) // 4
                q3_index = 3 * len(annual_citations) // 4

                journal_stats[journal] = {
                    'median_annual_citations': annual_citations[median_index],
                    'q1_annual_citations': annual_citations[q1_index],
                    'q3_annual_citations': annual_citations[q3_index],
                    'count': len(data_list),
                    'min': min(annual_citations),
                    'max': max(annual_citations),
                    # –î–æ–±–∞–≤–ª—è–µ–º 'median' –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º –∫–æ–¥–æ–º
                    'median': annual_citations[median_index]
                }

        return journal_stats

    def _perform_medium_insight_analysis(self, doi: str, result: Dict,
                                        citing_data: Dict[str, Dict],
                                        journal_stats: Dict[str, Dict]) -> Dict:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —Å—Ä–µ–¥–Ω–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""

        pub_info = result['publication_info']
        authors = result.get('authors', [])
        countries = result.get('countries', [])

        # 1. Temporal Citation Pattern
        temporal_pattern = self._analyze_temporal_pattern(result, citing_data)

        # 2. Journal Concentration Analysis
        journal_concentration = self._analyze_journal_concentration(citing_data)

        # 3. Author Network Analysis
        author_network = self._analyze_author_network(authors, citing_data)

        # 4. Geographic Bias Analysis
        geographic_bias = self._analyze_geographic_bias(countries, citing_data)

        # 5. Comparison with Journal Norms
        journal_comparison = self._compare_with_journal_norms(pub_info, journal_stats)

        # –†–∞—Å—á–µ—Ç –∞–Ω–æ–º–∞–ª—å–Ω–æ–≥–æ —Å–∫–æ—Ä–∞
        anomaly_score = self._calculate_anomaly_score(
            temporal_pattern, journal_concentration, author_network,
            geographic_bias, journal_comparison
        )

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ä–∏—Å–∫–∞
        risk_category, investigation_priority = self._determine_risk_category(anomaly_score)

        return {
            'Article_DOI': doi,
            'Publication_Year': pub_info.get('year', ''),
            'Total_Citations': len(citing_data),
            'Annual_Citation_Rate': round(temporal_pattern.get('annual_rate', 0), 2),
            'Citations_Year_1': temporal_pattern.get('year_1', 0),
            'Citations_Year_2': temporal_pattern.get('year_2', 0),
            'First_2_Years_Percent': round(temporal_pattern.get('first_2_years_percent', 0) * 100, 1),
            'Temporal_Anomaly_Index': round(temporal_pattern.get('anomaly_index', 0), 3),
            'Top_Journal_Citing': journal_concentration.get('top_journal', ''),
            'Top_Journal_Percent': round(journal_concentration.get('top_journal_percent', 0) * 100, 1),
            'Journal_Concentration_Index': round(journal_concentration.get('concentration_index', 0), 3),
            'Journal_Diversity_Index': round(journal_concentration.get('diversity_index', 0), 3),
            'Author_Self_Citation_Rate': round(author_network.get('self_citation_rate', 0) * 100, 1),
            'Author_Cluster_Coefficient': round(author_network.get('cluster_coefficient', 0), 3),
            'Author_Network_Density': round(author_network.get('network_density', 0), 3),
            'Top_Country': geographic_bias.get('top_country', ''),
            'Top_Country_Percent': round(geographic_bias.get('top_country_percent', 0) * 100, 1),
            'Country_Diversity_Index': round(geographic_bias.get('diversity_index', 0), 3),
            'Geographic_Bias_Index': round(geographic_bias.get('bias_index', 0), 3),
            'Journal_Median_Annual_Cite': round(journal_comparison.get('journal_median', 0), 2),
            'Citation_Ratio_vs_Journal': round(journal_comparison.get('citation_ratio', 0), 2),
            'Journal_Percentile': round(journal_comparison.get('percentile', 0), 1),
            'Anomaly_Score': round(anomaly_score, 1),
            'Risk_Category': risk_category,
            'Investigation_Priority': investigation_priority,
            'Temporal_Red_Flags': temporal_pattern.get('red_flags', 0),
            'Journal_Red_Flags': journal_concentration.get('red_flags', 0),
            'Author_Red_Flags': author_network.get('red_flags', 0),
            'Geographic_Red_Flags': geographic_bias.get('red_flags', 0)
        }

    def _analyze_temporal_pattern(self, result: Dict, citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if not citing_data:
            return {'annual_rate': 0, 'year_1': 0, 'year_2': 0,
                    'first_2_years_percent': 0, 'anomaly_index': 0, 'red_flags': 0}

        pub_year_str = result.get('publication_info', {}).get('year', '')
        if not pub_year_str:
            return {'annual_rate': 0, 'year_1': 0, 'year_2': 0,
                    'first_2_years_percent': 0, 'anomaly_index': 0, 'red_flags': 0}

        try:
            pub_year = int(pub_year_str)
            current_year = datetime.now().year
            years_passed = max(1, current_year - pub_year)

            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥–∞–º
            year_distribution = Counter()
            for cite_result in citing_data.values():
                cite_year_str = cite_result.get('publication_info', {}).get('year', '')
                if cite_year_str:
                    try:
                        cite_year = int(cite_year_str)
                        if cite_year >= pub_year:
                            year_distribution[cite_year] += 1
                    except:
                        pass

            # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            total_citations = len(citing_data)
            annual_rate = total_citations / years_passed

            year_1 = year_distribution.get(pub_year, 0)
            year_2 = year_distribution.get(pub_year + 1, 0)

            first_2_years = year_1 + year_2
            first_2_years_percent = first_2_years / total_citations if total_citations > 0 else 0

            # –ò–Ω–¥–µ–∫—Å –∞–Ω–æ–º–∞–ª–∏–∏ (—á–µ–º –≤—ã—à–µ, —Ç–µ–º –±–æ–ª–µ–µ –∞–Ω–æ–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
            expected_per_year = total_citations / max(1, len(year_distribution))
            anomaly_sum = 0
            for year, count in year_distribution.items():
                if expected_per_year > 0:
                    anomaly_sum += abs(count - expected_per_year) / expected_per_year

            anomaly_index = anomaly_sum / len(year_distribution) if year_distribution else 0

            # –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏
            red_flags = 0
            if first_2_years_percent > Config.MEDIUM_INSIGHT_THRESHOLDS['first_two_years']:
                red_flags += 1
            if anomaly_index > 0.5:  # –°–∏–ª—å–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                red_flags += 1

            return {
                'annual_rate': annual_rate,
                'year_1': year_1,
                'year_2': year_2,
                'first_2_years_percent': first_2_years_percent,
                'anomaly_index': anomaly_index,
                'red_flags': red_flags
            }

        except Exception as e:
            st.warning(f"‚ö†Ô∏è Temporal pattern analysis error: {e}")
            return {'annual_rate': 0, 'year_1': 0, 'year_2': 0,
                    'first_2_years_percent': 0, 'anomaly_index': 0, 'red_flags': 0}

    def _analyze_journal_concentration(self, citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –ø–æ –∂—É—Ä–Ω–∞–ª–∞–º"""
        if not citing_data:
            return {'top_journal': '', 'top_journal_percent': 0,
                    'concentration_index': 0, 'diversity_index': 0, 'red_flags': 0}

        journal_counter = Counter()
        for cite_result in citing_data.values():
            journal = cite_result.get('publication_info', {}).get('journal', '')
            if journal:
                journal_counter[journal] += 1

        if not journal_counter:
            return {'top_journal': '', 'top_journal_percent': 0,
                    'concentration_index': 0, 'diversity_index': 0, 'red_flags': 0}

        total_citations = sum(journal_counter.values())

        # –¢–æ–ø –∂—É—Ä–Ω–∞–ª –∏ –µ–≥–æ –¥–æ–ª—è
        top_journal, top_count = journal_counter.most_common(1)[0]
        top_journal_percent = top_count / total_citations

        # –ò–Ω–¥–µ–∫—Å –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ –•–µ—Ä—Ñ–∏–Ω–¥–∞–ª—è-–•–∏—Ä—à–º–∞–Ω–∞
        hhi = sum((count / total_citations) ** 2 for count in journal_counter.values())
        concentration_index = hhi

        # –ò–Ω–¥–µ–∫—Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è (1 - HHI)
        diversity_index = 1 - hhi

        # –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏
        red_flags = 0
        if top_journal_percent > Config.MEDIUM_INSIGHT_THRESHOLDS['top_journal_share']:
            red_flags += 1
        if concentration_index > 0.25:  # –í—ã—Å–æ–∫–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è
            red_flags += 1

        return {
            'top_journal': top_journal[:50],
            'top_journal_percent': top_journal_percent,
            'concentration_index': concentration_index,
            'diversity_index': diversity_index,
            'red_flags': red_flags
        }

    def _analyze_author_network(self, analyzed_authors: List[Dict],
                               citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Ç—å –∞–≤—Ç–æ—Ä–æ–≤ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π"""
        if not citing_data or not analyzed_authors:
            return {'self_citation_rate': 0, 'cluster_coefficient': 0,
                    'network_density': 0, 'red_flags': 0}

        analyzed_author_names = {author['name'] for author in analyzed_authors}

        # –°—Ç—Ä–æ–∏–º —Å–µ—Ç—å –∞–≤—Ç–æ—Ä–æ–≤ —Ü–∏—Ç–∏—Ä—É—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π
        author_network = defaultdict(set)
        all_citing_authors = set()

        for cite_result in citing_data.values():
            citing_authors = cite_result.get('authors', [])
            citing_author_names = {author['name'] for author in citing_authors}
            all_citing_authors.update(citing_author_names)

            # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑–∏ –º–µ–∂–¥—É –≤—Å–µ–º–∏ –∞–≤—Ç–æ—Ä–∞–º–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏
            author_list = list(citing_author_names)
            for i in range(len(author_list)):
                for j in range(i + 1, len(author_list)):
                    author_network[author_list[i]].add(author_list[j])
                    author_network[author_list[j]].add(author_list[i])

        # Self-citation rate
        total_citations = len(citing_data)
        self_citations = 0

        for cite_result in citing_data.values():
            citing_authors = cite_result.get('authors', [])
            citing_author_names = {author['name'] for author in citing_authors}

            if analyzed_author_names.intersection(citing_author_names):
                self_citations += 1

        self_citation_rate = self_citations / total_citations if total_citations > 0 else 0

        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
        if len(author_network) > 0:
            total_possible_connections = len(author_network) * (len(author_network) - 1) / 2
            actual_connections = sum(len(neighbors) for neighbors in author_network.values()) / 2

            if total_possible_connections > 0:
                network_density = actual_connections / total_possible_connections

                # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
                cluster_coefficient = network_density
            else:
                network_density = 0
                cluster_coefficient = 0
        else:
            network_density = 0
            cluster_coefficient = 0

        # –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏
        red_flags = 0
        if self_citation_rate > 0.3:
            red_flags += 1
        if cluster_coefficient > Config.MEDIUM_INSIGHT_THRESHOLDS['cluster_coefficient']:
            red_flags += 1

        return {
            'self_citation_rate': self_citation_rate,
            'cluster_coefficient': cluster_coefficient,
            'network_density': network_density,
            'red_flags': red_flags
        }

    def _analyze_geographic_bias(self, analyzed_countries: List[str],
                                citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫—É—é –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å"""
        if not citing_data:
            return {'top_country': '', 'top_country_percent': 0,
                    'diversity_index': 0, 'bias_index': 0, 'red_flags': 0}

        country_counter = Counter()
        for cite_result in citing_data.values():
            countries = cite_result.get('countries', [])
            for country in countries:
                if country:
                    country_counter[country] += 1

        if not country_counter:
            return {'top_country': '', 'top_country_percent': 0,
                    'diversity_index': 0, 'bias_index': 0, 'red_flags': 0}

        total_citations = sum(country_counter.values())

        # –¢–æ–ø —Å—Ç—Ä–∞–Ω–∞ –∏ –µ–µ –¥–æ–ª—è
        top_country, top_count = country_counter.most_common(1)[0]
        top_country_percent = top_count / total_citations

        # –ò–Ω–¥–µ–∫—Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        hhi = sum((count / total_citations) ** 2 for count in country_counter.values())
        diversity_index = 1 - hhi

        # –ò–Ω–¥–µ–∫—Å –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏
        # (–¥–æ–ª—è –∏–∑ —Ç–æ–π –∂–µ —Å—Ç—Ä–∞–Ω—ã, —á—Ç–æ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–∞—è —Å—Ç–∞—Ç—å—è)
        same_country_share = 0
        if analyzed_countries:
            for country in analyzed_countries:
                same_country_share += country_counter.get(country, 0) / total_citations

        bias_index = same_country_share

        # –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏
        red_flags = 0
        if top_country_percent > 0.8:
            red_flags += 1
        if bias_index > Config.MEDIUM_INSIGHT_THRESHOLDS['geographic_bias']:
            red_flags += 1

        return {
            'top_country': top_country,
            'top_country_percent': top_country_percent,
            'diversity_index': diversity_index,
            'bias_index': bias_index,
            'red_flags': red_flags
        }

    def _compare_with_journal_norms(self, pub_info: Dict,
                                   journal_stats: Dict[str, Dict]) -> Dict:
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å –Ω–æ—Ä–º–∞–º–∏ –∂—É—Ä–Ω–∞–ª–∞"""
        journal = pub_info.get('journal', '')
        citation_count = pub_info.get('citation_count_crossref', 0)
        year_str = pub_info.get('year', '')

        if not journal or not year_str or journal not in journal_stats:
            return {'journal_median': 0, 'citation_ratio': 0, 'percentile': 50}

        try:
            year = int(year_str)
            current_year = datetime.now().year
            age = max(1, current_year - year)
            annual_citations = citation_count / age

            stats = journal_stats[journal]
            journal_median = stats.get('median_annual_citations', 0.1)

            if journal_median > 0:
                citation_ratio = annual_citations / journal_median
            else:
                citation_ratio = 0

            # –ü—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∂—É—Ä–Ω–∞–ª—å–Ω—ã—Ö –Ω–æ—Ä–º
            all_citations = [annual_citations]
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∂—É—Ä–Ω–∞–ª—å–Ω—ã—Ö –Ω–æ—Ä–º
            all_citations.append(stats.get('min', annual_citations * 0.5))
            all_citations.append(stats.get('median_annual_citations', annual_citations))
            all_citations.append(stats.get('max', annual_citations * 2))
            all_citations.sort()

            position = all_citations.index(annual_citations) + 1
            percentile = (position / len(all_citations)) * 100

            return {
                'journal_median': journal_median,
                'citation_ratio': citation_ratio,
                'percentile': percentile
            }

        except Exception as e:
            st.warning(f"‚ö†Ô∏è Journal comparison error: {e}")
            return {'journal_median': 0, 'citation_ratio': 0, 'percentile': 50}

    def _calculate_anomaly_score(self, temporal_pattern: Dict, journal_concentration: Dict,
                                author_network: Dict, geographic_bias: Dict,
                                journal_comparison: Dict) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—â–∏–π –∞–Ω–æ–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä"""
        score = 0

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ (–º–∞–∫—Å 25)
        score += min(25, temporal_pattern.get('anomaly_index', 0) * 50)
        if temporal_pattern.get('first_2_years_percent', 0) > 0.7:
            score += 15

        # –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –∂—É—Ä–Ω–∞–ª–æ–≤ (–º–∞–∫—Å 20)
        score += min(20, journal_concentration.get('concentration_index', 0) * 80)
        if journal_concentration.get('top_journal_percent', 0) > 0.6:
            score += 10

        # –°–µ—Ç—å –∞–≤—Ç–æ—Ä–æ–≤ (–º–∞–∫—Å 25)
        score += min(25, author_network.get('self_citation_rate', 0) * 83)
        score += min(15, author_network.get('cluster_coefficient', 0) * 20)

        # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∞—è –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å (–º–∞–∫—Å 15)
        score += min(15, geographic_bias.get('bias_index', 0) * 30)
        if geographic_bias.get('top_country_percent', 0) > 0.8:
            score += 5

        # –û—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –∂—É—Ä–Ω–∞–ª—å–Ω—ã—Ö –Ω–æ—Ä–º (–º–∞–∫—Å 15)
        citation_ratio = journal_comparison.get('citation_ratio', 0)
        if citation_ratio > 3:
            score += 15
        elif citation_ratio > 2:
            score += 10
        elif citation_ratio > 1.5:
            score += 5

        return min(100, score)

    def _determine_risk_category(self, anomaly_score: float) -> Tuple[str, int]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ä–∏—Å–∫–∞ –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        if anomaly_score > 80:
            return "CRITICAL", 5
        elif anomaly_score > 60:
            return "HIGH", 4
        elif anomaly_score > 40:
            return "MEDIUM", 3
        elif anomaly_score > 20:
            return "LOW", 2
        else:
            return "MINIMAL", 1

    def analyze_deep_analysis(self, analyzed_results: Dict[str, Dict],
                             citing_results: Dict[str, Dict],
                             ref_results: Dict[str, Dict] = None) -> List[Dict]:
        """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ (60-120 —Å–µ–∫—É–Ω–¥ –Ω–∞ —Å—Ç–∞—Ç—å—é)"""
        deep_analysis = []

        # –°—Ç—Ä–æ–∏–º –ø–æ–ª–Ω—É—é —Å–µ—Ç—å –¥–ª—è —Å–µ—Ç–µ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        full_network = self._build_citation_network(analyzed_results, citing_results, ref_results)

        for doi, result in analyzed_results.items():
            if result.get('status') != 'success':
                continue

            # –ü–æ–ª—É—á–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            citing_dois = result.get('citations', [])
            citing_data = {}
            for cite_doi in citing_dois:
                if cite_doi in citing_results and citing_results[cite_doi].get('status') == 'success':
                    citing_data[cite_doi] = citing_results[cite_doi]

            # –í—ã–ø–æ–ª–Ω—è–µ–º –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑
            analysis = self._perform_deep_analysis(doi, result, citing_data, full_network)
            deep_analysis.append(analysis)

            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self.cache.set_ethical_analysis('deep_analysis', doi, analysis)

        return sorted(deep_analysis, key=lambda x: x['Machine_Learning_Risk_Score'], reverse=True)

    def _build_citation_network(self, analyzed_results: Dict[str, Dict],
                               citing_results: Dict[str, Dict],
                               ref_results: Dict[str, Dict] = None) -> nx.DiGraph:
        """–°—Ç—Ä–æ–∏—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π"""
        G = nx.DiGraph()

        # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–µ —Å—Ç–∞—Ç—å–∏
        for doi, result in analyzed_results.items():
            if result.get('status') == 'success':
                G.add_node(doi, type='analyzed',
                          year=result.get('publication_info', {}).get('year', ''))

        # –î–æ–±–∞–≤–ª—è–µ–º —Ü–∏—Ç–∏—Ä—É—é—â–∏–µ —Å—Ç–∞—Ç—å–∏
        for doi, result in citing_results.items():
            if result.get('status') == 'success':
                G.add_node(doi, type='citing',
                          year=result.get('publication_info', {}).get('year', ''))

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ—Ñ–µ—Ä–µ–Ω—Å—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        if ref_results:
            for doi, result in ref_results.items():
                if result.get('status') == 'success':
                    G.add_node(doi, type='reference',
                              year=result.get('publication_info', {}).get('year', ''))

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–±—Ä–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        for analyzed_doi, result in analyzed_results.items():
            if result.get('status') == 'success':
                citing_dois = result.get('citations', [])
                for cite_doi in citing_dois:
                    if cite_doi in G:
                        G.add_edge(cite_doi, analyzed_doi)  # cite_doi ‚Üí analyzed_doi

        return G

    def _perform_deep_analysis(self, doi: str, result: Dict,
                              citing_data: Dict[str, Dict],
                              citation_network: nx.DiGraph) -> Dict:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""

        # 1. Network Analysis
        network_metrics = self._analyze_network_metrics(doi, citation_network)

        # 2. Temporal Pattern Mining
        temporal_patterns = self._mine_temporal_patterns(result, citing_data)

        # 3. Geographic Cluster Analysis
        geographic_clusters = self._analyze_geographic_clusters(result, citing_data)

        # 4. Journal Network Analysis
        journal_network = self._analyze_journal_network(result, citing_data)

        # 5. Statistical Anomaly Detection
        statistical_anomalies = self._detect_statistical_anomalies(result, citing_data)

        # 6. Machine Learning Risk Assessment
        ml_risk_score = self._calculate_ml_risk_score(
            network_metrics, temporal_patterns, geographic_clusters,
            journal_network, statistical_anomalies
        )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –æ–±–∑–æ—Ä–∞
        expert_review_required = self._determine_expert_review_requirement(
            network_metrics, ml_risk_score, len(citing_data)
        )

        return {
            'Article_DOI': doi,
            'Author_Cluster_ID': network_metrics.get('author_cluster_id', 'N/A'),
            'Cluster_Size': network_metrics.get('cluster_size', 0),
            'Internal_Citation_Density': round(network_metrics.get('internal_density', 0), 3),
            'Cross_Cluster_Citations': network_metrics.get('cross_cluster_citations', 0),
            'Betweenness_Centrality': round(network_metrics.get('betweenness_centrality', 0), 4),
            'Clustering_Coefficient': round(network_metrics.get('clustering_coefficient', 0), 3),
            'Eigenvector_Centrality': round(network_metrics.get('eigenvector_centrality', 0), 4),
            'Quarterly_Citation_Peaks': temporal_patterns.get('quarterly_peaks', 0),
            'Seasonal_Pattern_Detected': temporal_patterns.get('seasonal_pattern', False),
            'Citation_Wave_Length': temporal_patterns.get('wave_length', 0),
            'Burst_Detection_Score': round(temporal_patterns.get('burst_score', 0), 3),
            'Geographic_Cluster_Strength': round(geographic_clusters.get('cluster_strength', 0), 3),
            'Cross_Country_Citation_Bias': round(geographic_clusters.get('cross_country_bias', 0), 3),
            'Region_Homophily_Index': round(geographic_clusters.get('homophily_index', 0), 3),
            'Journal_Citation_Circle': journal_network.get('citation_circle', False),
            'Journal_Reciprocity_Index': round(journal_network.get('reciprocity_index', 0), 3),
            'Predatory_Journal_Flags': journal_network.get('predatory_flags', 0),
            'Journal_Network_Modularity': round(journal_network.get('modularity', 0), 3),
            'Citation_Gini_Coefficient': round(statistical_anomalies.get('gini_coefficient', 0), 3),
            'Z_Score_Anomaly': round(statistical_anomalies.get('z_score', 0), 2),
            'Power_Law_Fit': round(statistical_anomalies.get('power_law_fit', 0), 3),
            'Statistical_Outlier_Flag': statistical_anomalies.get('outlier_flag', False),
            'Temporal_Anomaly_Score': round(temporal_patterns.get('temporal_anomaly_score', 0), 1),
            'Network_Centrality_Score': round(network_metrics.get('centrality_score', 0), 1),
            'Pattern_Anomaly_Score': round(temporal_patterns.get('pattern_anomaly_score', 0), 1),
            'Machine_Learning_Risk_Score': round(ml_risk_score, 1),
            'Expert_Review_Required': expert_review_required,
            'Suggested_Audit_Focus': self._suggest_audit_focus(network_metrics, temporal_patterns,
                                                             geographic_clusters, journal_network),
            'Confidence_Level': self._calculate_confidence_level(len(citing_data), ml_risk_score)
        }

    def _analyze_network_metrics(self, doi: str, citation_network: nx.DiGraph) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Ç–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏"""
        if doi not in citation_network:
            return {'author_cluster_id': 'N/A', 'cluster_size': 0, 'internal_density': 0,
                    'cross_cluster_citations': 0, 'betweenness_centrality': 0,
                    'clustering_coefficient': 0, 'eigenvector_centrality': 0,
                    'centrality_score': 0}

        try:
            # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏
            betweenness = nx.betweenness_centrality(citation_network, normalized=True).get(doi, 0)
            clustering = nx.clustering(citation_network.to_undirected()).get(doi, 0)

            # Eigenvector centrality (—Ç—Ä–µ–±—É–µ—Ç —Å–≤—è–∑–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞)
            try:
                eigenvector = nx.eigenvector_centrality_numpy(citation_network.to_undirected()).get(doi, 0)
            except:
                eigenvector = 0

            # –ê–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ—Å—Ç–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º greedy modularity communities
                communities = list(nx.algorithms.community.greedy_modularity_communities(
                    citation_network.to_undirected()))

                # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ–±—â–µ—Å—Ç–≤–æ —Ç–µ–∫—É—â–µ–π —Å—Ç–∞—Ç—å–∏
                article_community = None
                for i, community in enumerate(communities):
                    if doi in community:
                        article_community = i
                        break

                if article_community is not None:
                    community_nodes = communities[article_community]
                    cluster_size = len(community_nodes)

                    # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞
                    subgraph = citation_network.subgraph(community_nodes)
                    internal_edges = subgraph.number_of_edges()
                    possible_edges = len(community_nodes) * (len(community_nodes) - 1)
                    internal_density = internal_edges / possible_edges if possible_edges > 0 else 0

                    # –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ–∂–¥—É —Å–æ–æ–±—â–µ—Å—Ç–≤–∞–º–∏
                    cross_cluster_citations = 0
                    for node in community_nodes:
                        for neighbor in citation_network.neighbors(node):
                            if neighbor not in community_nodes:
                                cross_cluster_citations += 1

                    author_cluster_id = f"COMM_{article_community:03d}"
                else:
                    cluster_size = 1
                    internal_density = 0
                    cross_cluster_citations = 0
                    author_cluster_id = "ISOLATED"

            except:
                cluster_size = 1
                internal_density = 0
                cross_cluster_citations = 0
                author_cluster_id = "UNKNOWN"

            # –û–±—â–∏–π —Å–∫–æ—Ä–∏–Ω–≥ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏
            centrality_score = min(100, (
                betweenness * 40 +
                eigenvector * 30 +
                (1 - clustering) * 30  # –ù–∏–∑–∫–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ = –≤—ã—à–µ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å
            ))

            return {
                'author_cluster_id': author_cluster_id,
                'cluster_size': cluster_size,
                'internal_density': internal_density,
                'cross_cluster_citations': cross_cluster_citations,
                'betweenness_centrality': betweenness,
                'clustering_coefficient': clustering,
                'eigenvector_centrality': eigenvector,
                'centrality_score': centrality_score
            }

        except Exception as e:
            st.warning(f"‚ö†Ô∏è Network analysis error for {doi}: {e}")
            return {'author_cluster_id': 'N/A', 'cluster_size': 0, 'internal_density': 0,
                    'cross_cluster_citations': 0, 'betweenness_centrality': 0,
                    'clustering_coefficient': 0, 'eigenvector_centrality': 0,
                    'centrality_score': 0}

    def _mine_temporal_patterns(self, result: Dict, citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if not citing_data:
            return {'quarterly_peaks': 0, 'seasonal_pattern': False,
                    'wave_length': 0, 'burst_score': 0,
                    'temporal_anomaly_score': 0, 'pattern_anomaly_score': 0}

        pub_date_str = result.get('publication_info', {}).get('publication_date', '')
        if not pub_date_str:
            return {'quarterly_peaks': 0, 'seasonal_pattern': False,
                    'wave_length': 0, 'burst_score': 0,
                    'temporal_anomaly_score': 0, 'pattern_anomaly_score': 0}

        try:
            pub_date = datetime.strptime(pub_date_str[:10], '%Y-%m-%d')

            # –°–æ–±–∏—Ä–∞–µ–º –¥–∞—Ç—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
            citation_dates = []
            for cite_result in citing_data.values():
                cite_date_str = cite_result.get('publication_info', {}).get('publication_date', '')
                if cite_date_str:
                    try:
                        cite_date = datetime.strptime(cite_date_str[:10], '%Y-%m-%d')
                        if cite_date >= pub_date:  # –¢–æ–ª—å–∫–æ –±—É–¥—É—â–∏–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                            citation_dates.append(cite_date)
                    except:
                        pass

            if not citation_dates:
                return {'quarterly_peaks': 0, 'seasonal_pattern': False,
                    'wave_length': 0, 'burst_score': 0,
                    'temporal_anomaly_score': 0, 'pattern_anomaly_score': 0}

            # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–≤–∞—Ä—Ç–∞–ª–∞–º
            quarterly_counts = Counter()
            for date in citation_dates:
                quarter = f"{date.year}-Q{(date.month - 1) // 3 + 1}"
                quarterly_counts[quarter] += 1

            # –ü–∏–∫–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π (–∫–≤–∞—Ä—Ç–∞–ª—ã —Å >30% –æ—Ç –æ–±—â–µ–≥–æ)
            total_citations = len(citation_dates)
            quarterly_peaks = 0
            for quarter, count in quarterly_counts.items():
                if count / total_citations > 0.3:
                    quarterly_peaks += 1

            # –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å (—Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É—é—Ç—Å—è –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –º–µ—Å—è—Ü—ã)
            monthly_counts = Counter()
            for date in citation_dates:
                monthly_counts[date.month] += 1

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å (–±–æ–ª–µ–µ 40% –≤ 2 –º–µ—Å—è—Ü–∞)
            sorted_months = sorted(monthly_counts.items(), key=lambda x: x[1], reverse=True)
            top_2_months_share = sum(count for _, count in sorted_months[:2]) / total_citations
            seasonal_pattern = top_2_months_share > 0.4

            # –î–ª–∏–Ω–∞ "–≤–æ–ª–Ω—ã" —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π (–≤ –¥–Ω—è—Ö)
            if len(citation_dates) >= 2:
                citation_dates.sort()
                time_spread = (citation_dates[-1] - citation_dates[0]).days

                # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ –≤–æ–ª–Ω—ã (0-1)
                if time_spread > 0:
                    wave_length = min(1.0, total_citations / (time_spread / 30.44))  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ –º–µ—Å—è—Ü–∞–º
                else:
                    wave_length = 1.0
            else:
                wave_length = 0

            # –û—Ü–µ–Ω–∫–∞ "burst" –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            if len(citation_dates) >= 3:
                # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è–º–∏
                citation_dates.sort()
                time_diffs = []
                for i in range(1, len(citation_dates)):
                    diff = (citation_dates[i] - citation_dates[i-1]).days
                    time_diffs.append(diff)

                if time_diffs:
                    avg_diff = sum(time_diffs) / len(time_diffs)
                    # Burst score: —á–µ–º –±–æ–ª—å—à–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ, —Ç–µ–º –≤—ã—à–µ
                    burst_variance = sum((d - avg_diff) ** 2 for d in time_diffs) / len(time_diffs)
                    burst_score = min(1.0, burst_variance / (avg_diff ** 2) if avg_diff > 0 else 0)
                else:
                    burst_score = 0
            else:
                burst_score = 0

            # –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–æ–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä
            temporal_anomaly_score = min(100, (
                quarterly_peaks * 20 +
                (1 if seasonal_pattern else 0) * 30 +
                wave_length * 25 +
                burst_score * 25
            ))

            # –ü–∞—Ç—Ç–µ—Ä–Ω –∞–Ω–æ–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä
            pattern_anomaly_score = min(100, (
                (quarterly_peaks / max(1, len(quarterly_counts))) * 40 +
                top_2_months_share * 30 +
                burst_score * 30
            ))

            return {
                'quarterly_peaks': quarterly_peaks,
                'seasonal_pattern': seasonal_pattern,
                'wave_length': round(wave_length, 3),
                'burst_score': round(burst_score, 3),
                'temporal_anomaly_score': temporal_anomaly_score,
                'pattern_anomaly_score': pattern_anomaly_score
            }

        except Exception as e:
            st.warning(f"‚ö†Ô∏è Temporal pattern mining error: {e}")
            return {'quarterly_peaks': 0, 'seasonal_pattern': False,
                    'wave_length': 0, 'burst_score': 0,
                    'temporal_anomaly_score': 0, 'pattern_anomaly_score': 0}

    def _analyze_geographic_clusters(self, result: Dict, citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã"""
        if not citing_data:
            return {'cluster_strength': 0, 'cross_country_bias': 0,
                    'homophily_index': 0}

        analyzed_countries = set(result.get('countries', []))

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞–Ω—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        citation_countries = []
        country_counter = Counter()

        for cite_result in citing_data.values():
            countries = cite_result.get('countries', [])
            citation_countries.append(set(countries))
            for country in countries:
                if country:
                    country_counter[country] += 1

        if not country_counter:
            return {'cluster_strength': 0, 'cross_country_bias': 0,
                    'homophily_index': 0}

        total_citations = len(citation_countries)

        # –°–∏–ª–∞ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        # (–¥–æ–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–π —Å—Ç—Ä–∞–Ω—ã)
        top_country_share = country_counter.most_common(1)[0][1] / total_citations
        cluster_strength = top_country_share

        # –ú–µ–∂—Å—Ç—Ä–∞–Ω–æ–≤–æ–π bias (–¥–æ–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –∏–∑ —Ç–µ—Ö –∂–µ —Å—Ç—Ä–∞–Ω)
        same_country_citations = 0
        for countries in citation_countries:
            if analyzed_countries.intersection(countries):
                same_country_citations += 1

        cross_country_bias = same_country_citations / total_citations if total_citations > 0 else 0

        # –ò–Ω–¥–µ–∫—Å –≥–æ–º–æ—Ñ–∏–ª–∏–∏ (–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ —Å–≤–æ–µ–π –≥—Ä—É–ø–ø—ã)
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–∞–∫ –¥–æ–ª—è –≤–Ω—É—Ç—Ä–∏–≥—Ä—É–ø–ø–æ–≤—ã—Ö —Å–≤—è–∑–µ–π
        homophily_index = cross_country_bias

        return {
            'cluster_strength': round(cluster_strength, 3),
            'cross_country_bias': round(cross_country_bias, 3),
            'homophily_index': round(homophily_index, 3)
        }

    def _analyze_journal_network(self, result: Dict, citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Ç—å –∂—É—Ä–Ω–∞–ª–æ–≤"""
        if not citing_data:
            return {'citation_circle': False, 'reciprocity_index': 0,
                    'predatory_flags': 0, 'modularity': 0}

        analyzed_journal = result.get('publication_info', {}).get('journal', '')

        # –°–æ–±–∏—Ä–∞–µ–º –∂—É—Ä–Ω–∞–ª—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        journal_counter = Counter()
        journal_pairs = set()

        for cite_result in citing_data.values():
            journal = cite_result.get('publication_info', {}).get('journal', '')
            if journal:
                journal_counter[journal] += 1

                # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –ø–∞—Ä—É –∂—É—Ä–Ω–∞–ª–æ–≤
                if analyzed_journal and journal != analyzed_journal:
                    journal_pair = tuple(sorted([analyzed_journal, journal]))
                    journal_pairs.add(journal_pair)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ü–∏—Ç–∞—Ç–Ω—ã–µ –∫—Ä—É–≥–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        # –ï—Å–ª–∏ –µ—Å—Ç—å –≤–∑–∞–∏–º–Ω–æ–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ–∂–¥—É –Ω–µ–±–æ–ª—å—à–∏–º –Ω–∞–±–æ—Ä–æ–º –∂—É—Ä–Ω–∞–ª–æ–≤
        citation_circle = False
        if len(journal_counter) <= 3 and sum(journal_counter.values()) > 5:
            # –ú–∞–ª–æ –∂—É—Ä–Ω–∞–ª–æ–≤, –º–Ω–æ–≥–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
            citation_circle = True

        # –ò–Ω–¥–µ–∫—Å –≤–∑–∞–∏–º–Ω–æ—Å—Ç–∏ (—Å–∫–æ–ª—å–∫–æ –∂—É—Ä–Ω–∞–ª–æ–≤ –∏–º–µ—é—Ç –æ–±—Ä–∞—Ç–Ω—ã–µ —Å–≤—è–∑–∏)
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç
        total_journals = len(journal_counter)
        if total_journals > 0:
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –µ—Å–ª–∏ –∂—É—Ä–Ω–∞–ª —Ü–∏—Ç–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—å—é,
            # —Ç–æ –≤–æ–∑–º–æ–∂–Ω–∞ –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –≤ –¥—Ä—É–≥–∏—Ö —Å—Ç–∞—Ç—å—è—Ö
            reciprocity_index = min(1.0, total_journals / 10)
        else:
            reciprocity_index = 0

        # –§–ª–∞–≥–∏ —Ö–∏—â–Ω–∏—á–µ—Å–∫–∏—Ö –∂—É—Ä–Ω–∞–ª–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é)
        predatory_keywords = ['international journal', 'advances in', 'research journal',
                            'journal of', 'annals of', 'archives of', 'european journal']

        predatory_flags = 0
        for journal in journal_counter:
            journal_lower = journal.lower()
            for keyword in predatory_keywords:
                if keyword in journal_lower:
                    predatory_flags += 1
                    break

        # –ú–æ–¥—É–ª—è—Ä–Ω–æ—Å—Ç—å —Å–µ—Ç–∏ –∂—É—Ä–Ω–∞–ª–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–∞–∫ 1 - (–¥–æ–ª—è –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞)
        if journal_counter:
            top_journal_share = journal_counter.most_common(1)[0][1] / sum(journal_counter.values())
            modularity = 1 - top_journal_share
        else:
            modularity = 0

        return {
            'citation_circle': citation_circle,
            'reciprocity_index': round(reciprocity_index, 3),
            'predatory_flags': predatory_flags,
            'modularity': round(modularity, 3)
        }

    def _detect_statistical_anomalies(self, result: Dict, citing_data: Dict[str, Dict]) -> Dict:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏"""
        if not citing_data:
            return {'gini_coefficient': 0, 'z_score': 0,
                    'power_law_fit': 0, 'outlier_flag': False}

        # –°–æ–±–∏—Ä–∞–µ–º –≥–æ–¥—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        citation_years = []
        for cite_result in citing_data.values():
            year_str = cite_result.get('publication_info', {}).get('year', '')
            if year_str:
                try:
                    year = int(year_str)
                    citation_years.append(year)
                except:
                    pass

        if not citation_years:
            return {'gini_coefficient': 0, 'z_score': 0,
                    'power_law_fit': 0, 'outlier_flag': False}

        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–∏ –¥–ª—è –Ω–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        citation_years.sort()
        n = len(citation_years)

        if n > 1:
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥–∞–º
            year_counts = Counter(citation_years)
            values = list(year_counts.values())
            values.sort()

            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–∏
            cum_values = np.cumsum(values).astype(float)
            gini = (n + 1 - 2 * np.sum(cum_values) / cum_values[-1]) / n
        else:
            gini = 0

        # Z-score –¥–ª—è –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        pub_year_str = result.get('publication_info', {}).get('year', '')
        if pub_year_str and len(citation_years) >= 3:
            try:
                pub_year = int(pub_year_str)
                current_year = datetime.now().year

                # –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ –≥–æ–¥
                year_range = range(pub_year, current_year + 1)
                citations_per_year = []

                for year in year_range:
                    count = citation_years.count(year)
                    citations_per_year.append(count)

                mean_citations = np.mean(citations_per_year)
                std_citations = np.std(citations_per_year)

                if std_citations > 0:
                    # Z-score –¥–ª—è –≥–æ–¥–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
                    max_year_count = max(citations_per_year)
                    z_score = (max_year_count - mean_citations) / std_citations
                else:
                    z_score = 0
            except:
                z_score = 0
        else:
            z_score = 0

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ power-law —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ª–µ—Ç —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        if len(citation_years) >= 5:
            year_counts = Counter(citation_years)
            sorted_counts = sorted(year_counts.values(), reverse=True)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É–±—ã–≤–∞–µ—Ç –ª–∏ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ
            if len(sorted_counts) >= 3:
                ratios = []
                for i in range(len(sorted_counts) - 1):
                    if sorted_counts[i+1] > 0:
                        ratios.append(sorted_counts[i] / sorted_counts[i+1])

                if ratios:
                    avg_ratio = np.mean(ratios)
                    # –ß–µ–º –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ, —Ç–µ–º –±–ª–∏–∂–µ –∫ power-law
                    power_law_fit = min(1.0, avg_ratio / 3)
                else:
                    power_law_fit = 0
            else:
                power_law_fit = 0
        else:
            power_law_fit = 0

        # –§–ª–∞–≥ –≤—ã–±—Ä–æ—Å–∞
        outlier_flag = (z_score > 3) or (gini > 0.7) or (power_law_fit > 0.8)

        return {
            'gini_coefficient': round(gini, 3),
            'z_score': round(z_score, 2),
            'power_law_fit': round(power_law_fit, 3),
            'outlier_flag': outlier_flag
        }

    def _calculate_ml_risk_score(self, network_metrics: Dict, temporal_patterns: Dict,
                                geographic_clusters: Dict, journal_network: Dict,
                                statistical_anomalies: Dict) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç ML-based —Ä–∏—Å–∫ —Å–∫–æ—Ä–∏–Ω–≥"""
        score = 0

        # –°–µ—Ç–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–º–∞–∫—Å 30)
        score += min(30, network_metrics.get('centrality_score', 0) * 0.3)

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–º–∞–∫—Å 25)
        score += min(25, temporal_patterns.get('temporal_anomaly_score', 0) * 0.25)

        # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (–º–∞–∫—Å 20)
        cluster_strength = geographic_clusters.get('cluster_strength', 0)
        cross_country_bias = geographic_clusters.get('cross_country_bias', 0)
        score += min(20, (cluster_strength + cross_country_bias) * 10)

        # –°–µ—Ç—å –∂—É—Ä–Ω–∞–ª–æ–≤ (–º–∞–∫—Å 15)
        if journal_network.get('citation_circle', False):
            score += 10
        score += min(5, journal_network.get('predatory_flags', 0) * 2.5)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏ (–º–∞–∫—Å 10)
        if statistical_anomalies.get('outlier_flag', False):
            score += 10

        return min(100, score)

    def _determine_expert_review_requirement(self, network_metrics: Dict,
                                           ml_risk_score: float,
                                           citation_count: int) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ª–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ"""
        if ml_risk_score > 70:
            return True

        if citation_count > 50 and network_metrics.get('centrality_score', 0) > 60:
            return True

        if network_metrics.get('cluster_size', 0) > 20:
            return True

        return False

    def _suggest_audit_focus(self, network_metrics: Dict, temporal_patterns: Dict,
                           geographic_clusters: Dict, journal_network: Dict) -> str:
        """–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ñ–æ–∫—É—Å –¥–ª—è –∞—É–¥–∏—Ç–∞"""
        factors = []

        if network_metrics.get('centrality_score', 0) > 60:
            factors.append(('Network', network_metrics.get('centrality_score', 0)))

        if temporal_patterns.get('temporal_anomaly_score', 0) > 60:
            factors.append(('Temporal', temporal_patterns.get('temporal_anomaly_score', 0)))

        if geographic_clusters.get('cluster_strength', 0) > 0.7:
            factors.append(('Geographic', geographic_clusters.get('cluster_strength', 0) * 100))

        if journal_network.get('citation_circle', False):
            factors.append(('Journal', 100))

        if factors:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é –∏ –±–µ—Ä–µ–º —Ç–æ–ø
            factors.sort(key=lambda x: x[1], reverse=True)
            return factors[0][0]
        else:
            return 'Normal'

    def _calculate_confidence_level(self, citation_count: int,
                                  ml_risk_score: float) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –æ—Ü–µ–Ω–∫–µ"""
        if citation_count == 0:
            return 50

        base_confidence = min(90, citation_count * 2)

        if ml_risk_score > 80:
            # –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ = –≤—ã—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏
            confidence = min(95, base_confidence + 10)
        elif ml_risk_score > 60:
            confidence = min(90, base_confidence + 5)
        elif ml_risk_score > 40:
            confidence = base_confidence
        elif ml_risk_score > 20:
            confidence = max(60, base_confidence - 10)
        else:
            confidence = max(50, base_confidence - 20)

        return confidence

    def analyze_citing_relationships(self, analyzed_results: Dict[str, Dict],
                                   citing_results: Dict[str, Dict]) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–≤—è–∑–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–µ ‚Üî —Ü–∏—Ç–∏—Ä—É—é—â–∏–µ (30-60 —Å–µ–∫)"""
        relationships = []

        # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ –¥–ª—è —Å–µ—Ç–µ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        citation_graph = self._build_citation_network(analyzed_results, citing_results)

        for analyzed_doi, analyzed_result in analyzed_results.items():
            if analyzed_result.get('status') != 'success':
                continue

            citing_dois = analyzed_result.get('citations', [])

            for citing_doi in citing_dois:
                if citing_doi in citing_results and citing_results[citing_doi].get('status') == 'success':
                    citing_result = citing_results[citing_doi]

                    # –ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–∏
                    analysis = self._perform_relationship_analysis(
                        analyzed_doi, analyzed_result,
                        citing_doi, citing_result,
                        citation_graph
                    )

                    relationships.append(analysis)

        return sorted(relationships, key=lambda x: x['Gift_Citation_Probability'], reverse=True)

    def _perform_relationship_analysis(self, analyzed_doi: str, analyzed_result: Dict,
                                     citing_doi: str, citing_result: Dict,
                                     citation_graph: nx.DiGraph) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç–∞—Ç—å—è–º–∏"""

        # 1. –í—Ä–µ–º–µ–Ω–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞
        time_diff = self._calculate_time_difference(analyzed_result, citing_result)

        # 2. –û–±—â–∏–µ –∞–≤—Ç–æ—Ä—ã
        common_authors = self._find_common_authors(analyzed_result, citing_result)

        # 3. –û–±—â–∏–µ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏
        common_affiliations = self._find_common_affiliations(analyzed_result, citing_result)

        # 4. –°–µ—Ç–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        network_metrics = self._calculate_relationship_network_metrics(
            analyzed_doi, citing_doi, citation_graph
        )

        # 5. –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å "–ø–æ–¥–∞—Ä–æ—á–Ω–æ–≥–æ" —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        gift_probability = self._calculate_gift_citation_probability(
            time_diff, common_authors, common_affiliations, network_metrics
        )

        # 6. –†–æ–ª—å –≤ —Å–µ—Ç–∏
        network_role = self._determine_network_role(analyzed_doi, citing_doi, citation_graph)

        # 7. –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
        time_sync = self._calculate_time_synchronization(analyzed_result, citing_result, citation_graph)

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Ä–∏—Å–∫–∞
        relationship_risk, action_required = self._determine_relationship_risk(gift_probability)

        return {
            'Analyzed_DOI': analyzed_doi,
            'Citing_DOI': citing_doi,
            'Time_Difference_Days': time_diff,
            'Same_Authors': len(common_authors),
            'Same_Affiliations': len(common_affiliations),
            'Common_Authors_List': '; '.join(common_authors),
            'Common_Affiliations_List': '; '.join(common_affiliations),
            'Connection_Strength': network_metrics.get('connection_strength', 0),
            'Reciprocity_Flag': network_metrics.get('reciprocity', False),
            'Temporal_Anomaly': self._determine_temporal_anomaly(time_diff),
            'Group_Citation_Cluster': network_metrics.get('cluster_id', 'N/A'),
            'Cluster_Size': network_metrics.get('cluster_size', 1),
            'Intra_Cluster_Density': round(network_metrics.get('intra_cluster_density', 0), 3),
            'Citation_Wave_Position': network_metrics.get('wave_position', 'Normal'),
            'Time_Sync_Score': round(time_sync, 3),
            'Batch_Citation_Flag': network_metrics.get('batch_citation', False),
            'Bridge_Role': network_role,
            'Betweenness_Centrality': round(network_metrics.get('betweenness', 0), 4),
            'Clustering_Coefficient': round(network_metrics.get('clustering', 0), 3),
            'Gift_Citation_Probability': round(gift_probability, 1),
            'Citation_Circle_Member': network_metrics.get('citation_circle', False),
            'Artificial_Boost_Flag': gift_probability > 70,
            'Journal_Pair_Frequency': network_metrics.get('journal_pair_freq', 1),
            'Country_Pair': self._create_country_pair(analyzed_result, citing_result),
            'Aff_Pair_Strength': len(common_affiliations),
            'Relationship_Risk': relationship_risk,
            'Action_Required': action_required,
            'Notes': self._generate_relationship_notes(
                common_authors, common_affiliations, time_diff, gift_probability
            )
        }

    def _calculate_time_difference(self, analyzed_result: Dict, citing_result: Dict) -> Optional[int]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Ä–∞–∑–Ω–∏—Ü—É –≤ –¥–Ω—è—Ö"""
        analyzed_date_str = analyzed_result.get('publication_info', {}).get('publication_date', '')
        citing_date_str = citing_result.get('publication_info', {}).get('publication_date', '')

        if not analyzed_date_str or not citing_date_str:
            return None

        try:
            analyzed_date = datetime.strptime(analyzed_date_str[:10], '%Y-%m-%d')
            citing_date = datetime.strptime(citing_date_str[:10], '%Y-%m-%d')

            return (citing_date - analyzed_date).days
        except:
            return None

    def _find_common_authors(self, analyzed_result: Dict, citing_result: Dict) -> Set[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–±—â–∏—Ö –∞–≤—Ç–æ—Ä–æ–≤"""
        analyzed_authors = {author['name'] for author in analyzed_result.get('authors', [])}
        citing_authors = {author['name'] for author in citing_result.get('authors', [])}

        return analyzed_authors.intersection(citing_authors)

    def _find_common_affiliations(self, analyzed_result: Dict, citing_result: Dict) -> Set[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–±—â–∏–µ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏"""
        analyzed_affiliations = set()
        for author in analyzed_result.get('authors', []):
            analyzed_affiliations.update(author.get('affiliation', []))

        citing_affiliations = set()
        for author in citing_result.get('authors', []):
            citing_affiliations.update(author.get('affiliation', []))

        return analyzed_affiliations.intersection(citing_affiliations)

    def _calculate_relationship_network_metrics(self, analyzed_doi: str, citing_doi: str,
                                              citation_graph: nx.DiGraph) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ—Ç–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å–≤—è–∑–∏"""
        metrics = {
            'connection_strength': 1,
            'reciprocity': False,
            'cluster_id': 'N/A',
            'cluster_size': 1,
            'intra_cluster_density': 0,
            'wave_position': 'Normal',
            'batch_citation': False,
            'betweenness': 0,
            'clustering': 0,
            'citation_circle': False,
            'journal_pair_freq': 1
        }

        if analyzed_doi not in citation_graph or citing_doi not in citation_graph:
            return metrics

        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∑–∞–∏–º–Ω–æ—Å—Ç–∏
            if citation_graph.has_edge(analyzed_doi, citing_doi):
                metrics['reciprocity'] = True

            # –°–∏–ª–∞ —Å–≤—è–∑–∏ (–Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–µ–π)
            try:
                betweenness = nx.betweenness_centrality(citation_graph, normalized=True)
                metrics['betweenness'] = betweenness.get(analyzed_doi, 0) + betweenness.get(citing_doi, 0)

                # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å–∏–ª–∞ —Å–≤—è–∑–∏
                metrics['connection_strength'] = min(10, int(metrics['betweenness'] * 20 + 1))
            except:
                pass

            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            try:
                undirected_graph = citation_graph.to_undirected()
                clustering = nx.clustering(undirected_graph)
                metrics['clustering'] = (clustering.get(analyzed_doi, 0) + clustering.get(citing_doi, 0)) / 2
            except:
                pass

            # –ê–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ—Å—Ç–≤
            try:
                communities = list(nx.algorithms.community.greedy_modularity_communities(
                    citation_graph.to_undirected()))

                # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ–±—â–µ—Å—Ç–≤–æ
                for i, community in enumerate(communities):
                    if analyzed_doi in community and citing_doi in community:
                        metrics['cluster_id'] = f"CLUSTER_{i:03d}"
                        metrics['cluster_size'] = len(community)

                        # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞
                        subgraph = citation_graph.subgraph(community)
                        possible_edges = len(community) * (len(community) - 1)
                        if possible_edges > 0:
                            metrics['intra_cluster_density'] = subgraph.number_of_edges() / possible_edges

                        break
            except:
                pass

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ batch citation
            # (–º–Ω–æ–≥–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ –∫–æ—Ä–æ—Ç–∫–∏–π –ø–µ—Ä–∏–æ–¥)
            analyzed_neighbors = list(citation_graph.predecessors(analyzed_doi))
            if len(analyzed_neighbors) > 10:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≥—Ä—É–ø–ø—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π —Å –±–ª–∏–∑–∫–∏–º–∏ –¥–∞—Ç–∞–º–∏
                metrics['batch_citation'] = True

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ü–∏—Ç–∞—Ç–Ω—ã–µ –∫—Ä—É–≥–∏
            try:
                # –ò—â–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ü–∏–∫–ª—ã
                for path in nx.all_simple_paths(citation_graph, citing_doi, analyzed_doi, cutoff=3):
                    if len(path) <= 3:
                        metrics['citation_circle'] = True
                        break
            except:
                pass

        except Exception as e:
            st.warning(f"‚ö†Ô∏è Network metrics error for {analyzed_doi}-{citing_doi}: {e}")

        return metrics

    def _calculate_gift_citation_probability(self, time_diff: Optional[int],
                                           common_authors: Set[str],
                                           common_affiliations: Set[str],
                                           network_metrics: Dict) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å "–ø–æ–¥–∞—Ä–æ—á–Ω–æ–≥–æ" —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        probability = 0

        # –û–±—â–∏–µ –∞–≤—Ç–æ—Ä—ã (—Å–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª)
        if common_authors:
            probability += min(50, len(common_authors) * 20)

        # –û–±—â–∏–µ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏ (—Å—Ä–µ–¥–Ω–∏–π —Å–∏–≥–Ω–∞–ª)
        if common_affiliations:
            probability += min(40, len(common_affiliations) * 15)

        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å (—Å–ª–∞–±—ã–π —Å–∏–≥–Ω–∞–ª)
        if time_diff is not None:
            if abs(time_diff) < 30:  # –ú–µ–Ω—å—à–µ –º–µ—Å—è—Ü–∞
                probability += 20
            elif abs(time_diff) < 90:  # –ú–µ–Ω—å—à–µ 3 –º–µ—Å—è—Ü–µ–≤
                probability += 10

        # –°–µ—Ç–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if network_metrics.get('reciprocity', False):
            probability += 15

        if network_metrics.get('citation_circle', False):
            probability += 20

        if network_metrics.get('batch_citation', False):
            probability += 10

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        return min(100, probability)

    def _determine_network_role(self, analyzed_doi: str, citing_doi: str,
                               citation_graph: nx.DiGraph) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–æ–ª—å –≤ —Å–µ—Ç–∏"""
        if analyzed_doi not in citation_graph or citing_doi not in citation_graph:
            return "Normal"

        try:
            # –°—Ç–µ–ø–µ–Ω–∏ –≤–µ—Ä—à–∏–Ω
            analyzed_in_degree = citation_graph.in_degree(analyzed_doi)
            analyzed_out_degree = citation_graph.out_degree(analyzed_doi)
            citing_in_degree = citation_graph.in_degree(citing_doi)
            citing_out_degree = citation_graph.out_degree(citing_doi)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–æ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–µ–ø–µ–Ω–µ–π
            if analyzed_in_degree > 10 or citing_in_degree > 10:
                return "Hub"
            elif analyzed_out_degree > 5 or citing_out_degree > 5:
                return "Connector"
            else:
                return "Normal"

        except:
            return "Normal"

    def _calculate_time_synchronization(self, analyzed_result: Dict, citing_result: Dict,
                                      citation_graph: nx.DiGraph) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç
        time_diff = self._calculate_time_difference(analyzed_result, citing_result)

        if time_diff is None:
            return 0.5

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if abs(time_diff) < 30:
            return 0.8  # –í—ã—Å–æ–∫–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
        elif abs(time_diff) < 90:
            return 0.6  # –°—Ä–µ–¥–Ω—è—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
        elif abs(time_diff) < 365:
            return 0.4  # –ù–∏–∑–∫–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
        else:
            return 0.2  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è

    def _determine_temporal_anomaly(self, time_diff: Optional[int]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é –∞–Ω–æ–º–∞–ª–∏—é"""
        if time_diff is None:
            return "Unknown"

        if time_diff < 0:
            return "Future citation"
        elif time_diff < 30:
            return "Rapid citation"
        elif time_diff < 90:
            return "Prompt citation"
        else:
            return "Normal"

    def _create_country_pair(self, analyzed_result: Dict, citing_result: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –ø–∞—Ä—ã —Å—Ç—Ä–∞–Ω"""
        analyzed_countries = analyzed_result.get('countries', [''])[:1]
        citing_countries = citing_result.get('countries', [''])[:1]

        analyzed_country = analyzed_countries[0] if analyzed_countries else 'Unknown'
        citing_country = citing_countries[0] if citing_countries else 'Unknown'

        return f"{analyzed_country}‚Üí{citing_country}"

    def _determine_relationship_risk(self, gift_probability: float) -> Tuple[str, str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞ —Å–≤—è–∑–∏"""
        if gift_probability > 80:
            return "CRITICAL", "IMMEDIATE VALIDATION REQUIRED"
        elif gift_probability > 60:
            return "HIGH", "DETAILED REVIEW REQUIRED"
        elif gift_probability > 40:
            return "MEDIUM", "MONITOR AND REVIEW"
        elif gift_probability > 20:
            return "LOW", "MINOR REVIEW SUGGESTED"
        else:
            return "MINIMAL", "ETHICALLY ACCEPTABLE"

    def _generate_relationship_notes(self, common_authors: Set[str],
                                   common_affiliations: Set[str],
                                   time_diff: Optional[int],
                                   gift_probability: float) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–º–µ—Ç–∫–∏ –æ —Å–≤—è–∑–∏"""
        notes = []

        if common_authors:
            notes.append(f"Common authors: {len(common_authors)}")

        if common_affiliations:
            notes.append(f"Common affiliations: {len(common_affiliations)}")

        if time_diff is not None:
            if time_diff < 0:
                notes.append(f"Future citation: {abs(time_diff)} days before")
            else:
                notes.append(f"Time gap: {time_diff} days")

        notes.append(f"Gift citation probability: {gift_probability:.1f}%")

        return "; ".join(notes)

# ============================================================================
# üß† –ö–õ–ê–°–° –¢–ï–†–ú–ò–ù–û–õ–û–ì–ò–ß–ï–°–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê (–ù–û–í–´–ô)
# ============================================================================

class TerminologyAnalyzer:
    def __init__(self, cache_manager: SmartCacheManager, data_processor: DataProcessor):
        self.cache = cache_manager
        self.processor = data_processor
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        self.term_frequency = defaultdict(lambda: defaultdict(int))  # —Ç–µ—Ä–º–∏–Ω -> –≥–æ–¥ -> —á–∞—Å—Ç–æ—Ç–∞
        self.term_cooccurrence = defaultdict(set)  # —Ç–µ—Ä–º–∏–Ω -> —Å–æ—Å–µ–¥–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        self.term_articles = defaultdict(list)  # —Ç–µ—Ä–º–∏–Ω -> —Å–ø–∏—Å–æ–∫ DOI —Å—Ç–∞—Ç–µ–π
        
        # –°–µ—Ç–µ–≤—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–≤
        self.term_network = nx.Graph()
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        self.stop_words = set([
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'are', 'was', 'were', 'be',
            'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
            'would', 'should', 'could', 'can', 'may', 'might', 'must', 'about',
            'against', 'between', 'into', 'through', 'during', 'before', 'after',
            'above', 'below', 'under', 'over', 'again', 'further', 'then', 'once',
            'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both',
            'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor',
            'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't',
            'can', 'will', 'just', 'don', 'should', 'now', 'using', 'based',
            'study', 'analysis', 'research', 'paper', 'article', 'journal',
            'approach', 'method', 'framework', 'model', 'system', 'application',
            'evaluation', 'experiment', 'result', 'conclusion', 'discussion'
        ])
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤
        self.term_stats = defaultdict(lambda: {
            'first_year': None,
            'last_year': None,
            'total_count': 0,
            'yearly_growth': {},
            'related_terms': set(),
            'article_types': set()
        })
        
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.cached_results = {}

    def extract_terms_from_title(self, title: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç–∞—Ç—å–∏"""
        if not title:
            return []
        
        # –û—á–∏—Å—Ç–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        clean_title = re.sub(r'[^\w\s-]', ' ', title.lower())
        words = clean_title.split()
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤
        filtered_words = [w for w in words if w not in self.stop_words and len(w) > 2]
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∏–≥—Ä–∞–º–º –∏ —Ç—Ä–∏–≥—Ä–∞–º–º
        terms = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ (–µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ)
        for word in filtered_words:
            if len(word) > 3 and not word.isdigit():
                terms.append(word)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∏–≥—Ä–∞–º–º—ã
        if len(filtered_words) >= 2:
            for i in range(len(filtered_words) - 1):
                bigram = f"{filtered_words[i]} {filtered_words[i+1]}"
                terms.append(bigram)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∏–≥—Ä–∞–º–º—ã
        if len(filtered_words) >= 3:
            for i in range(len(filtered_words) - 2):
                trigram = f"{filtered_words[i]} {filtered_words[i+1]} {filtered_words[i+2]}"
                terms.append(trigram)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        return list(set(terms))

    def process_terms(self, doi: str, terms: List[str], year: str, article_type: str = "analyzed"):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ —Å—Ç–∞—Ç—å–∏"""
        if not terms or not year:
            return
        
        try:
            year_int = int(year)
        except:
            return
        
        for term in terms:
            # –û–±–Ω–æ–≤–ª—è–µ–º —á–∞—Å—Ç–æ—Ç—É —Ç–µ—Ä–º–∏–Ω–∞
            self.term_frequency[term][year_int] += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ—Ä–º–∏–Ω–∞
            term_info = self.term_stats[term]
            if term_info['first_year'] is None or year_int < term_info['first_year']:
                term_info['first_year'] = year_int
            if term_info['last_year'] is None or year_int > term_info['last_year']:
                term_info['last_year'] = year_int
            
            term_info['total_count'] += 1
            term_info['article_types'].add(article_type)
            
            # –î–æ–±–∞–≤–ª—è–µ–º DOI —Å—Ç–∞—Ç—å–∏
            if doi not in self.term_articles[term]:
                self.term_articles[term].append(doi)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ-–≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤
            for other_term in terms:
                if term != other_term:
                    self.term_cooccurrence[term].add(other_term)
                    term_info['related_terms'].add(other_term)

    def build_term_network(self):
        """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–µ—Ä–º–∏–Ω–∞–º–∏"""
        for term, co_terms in self.term_cooccurrence.items():
            for co_term in co_terms:
                # –í–µ—Å —Å–≤—è–∑–∏ = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –ø–æ—è–≤–ª–µ–Ω–∏–π
                weight = len(set(self.term_articles[term]) & set(self.term_articles[co_term]))
                if weight > 0:
                    self.term_network.add_edge(term, co_term, weight=weight)

    def detect_emerging_terms(self, window_years: int = 3) -> List[Dict]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º —Ä–æ—Å—Ç–æ–º"""
        cache_key = f"emerging_terms_{window_years}"
        cached = self.cache.get_terminology_cache('emerging_terms', cache_key)
        if cached is not None:
            return cached
        
        emerging = []
        current_year = datetime.now().year
        
        for term, year_counts in self.term_frequency.items():
            if len(year_counts) < 2:
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ window_years –ª–µ—Ç
            recent_years = sorted([y for y in year_counts.keys() if y >= current_year - window_years])
            if len(recent_years) < 2:
                continue
            
            recent_counts = [year_counts[y] for y in recent_years]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç
            if self._has_exponential_growth(recent_counts):
                growth_rate = self._calculate_growth_rate(recent_counts)
                first_year = min(year_counts.keys())
                
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                total_articles = len(self.term_articles[term])
                avg_articles_per_year = sum(year_counts.values()) / len(year_counts)
                
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–∏–ø–æ–≤ —Å—Ç–∞—Ç–µ–π
                type_diversity = len(self.term_stats[term]['article_types'])
                
                emerging.append({
                    'Term': term,
                    'First_Year': first_year,
                    'Total_Articles': total_articles,
                    'Avg_Articles_Per_Year': round(avg_articles_per_year, 2),
                    'Recent_Growth_Rate': round(growth_rate * 100, 1),  # –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
                    'Growth_Factor': round(self._calculate_growth_factor(recent_counts), 2),
                    'Type_Diversity': type_diversity,
                    'Network_Centrality': self._calculate_term_centrality(term),
                    'Maturity_Level': self._determine_maturity_level(year_counts),
                    'Predicted_Peak_Year': self._predict_peak_year(year_counts),
                    'Confidence_Score': round(self._calculate_confidence_score(term, year_counts), 1)
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ç–µ–º–ø—É —Ä–æ—Å—Ç–∞
        emerging_sorted = sorted(emerging, key=lambda x: x['Recent_Growth_Rate'], reverse=True)
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.cache.set_terminology_cache('emerging_terms', cache_key, emerging_sorted)
        
        return emerging_sorted

    def _has_exponential_growth(self, counts: List[int]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç"""
        if len(counts) < 2:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–Ω–æ—Ç–æ–Ω–Ω—ã–π —Ä–æ—Å—Ç
        if not all(counts[i] < counts[i+1] for i in range(len(counts)-1)):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–º–ø —Ä–æ—Å—Ç–∞ (–º–∏–Ω–∏–º—É–º —É–¥–≤–æ–µ–Ω–∏–µ –∑–∞ –ø–µ—Ä–∏–æ–¥)
        if counts[-1] / counts[0] < 2:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–∫–æ—Ä–µ–Ω–∏–µ —Ä–æ—Å—Ç–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–∏—Ä–æ—Å—Ç –±–æ–ª—å—à–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ)
        if len(counts) >= 3:
            last_increase = counts[-1] - counts[-2]
            prev_increase = counts[-2] - counts[-3]
            if last_increase <= prev_increase:
                return False
        
        return True

    def _calculate_growth_rate(self, counts: List[int]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–µ–º–ø —Ä–æ—Å—Ç–∞"""
        if len(counts) < 2 or counts[0] == 0:
            return 0.0
        
        # –°–ª–æ–∂–Ω—ã–π —Ç–µ–º–ø —Ä–æ—Å—Ç–∞
        periods = len(counts) - 1
        if periods > 0:
            growth_rate = (counts[-1] / counts[0]) ** (1/periods) - 1
            return growth_rate
        return 0.0

    def _calculate_growth_factor(self, counts: List[int]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ñ–∞–∫—Ç–æ—Ä —Ä–æ—Å—Ç–∞ (–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∫ –ø–µ—Ä–≤–æ–º—É)"""
        if len(counts) < 2 or counts[0] == 0:
            return 1.0
        return counts[-1] / counts[0]

    def _calculate_term_centrality(self, term: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–∞ –≤ —Å–µ—Ç–∏"""
        if term not in self.term_network:
            return 0.0
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–µ–ø–µ–Ω—å —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏
            degree = self.term_network.degree(term, weight='weight')
            max_degree = max([d for _, d in self.term_network.degree(weight='weight')], default=1)
            return degree / max_degree
        except:
            return 0.0

    def _determine_maturity_level(self, year_counts: Dict[int, int]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å –∑—Ä–µ–ª–æ—Å—Ç–∏ —Ç–µ—Ä–º–∏–Ω–∞"""
        years = sorted(year_counts.keys())
        if len(years) < 2:
            return "EMERGING"
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∏–Ω–∞–º–∏–∫—É
        counts = [year_counts[y] for y in years]
        
        if len(years) <= 2:
            if counts[-1] / counts[0] > 3:
                return "RAPID_GROWTH"
            else:
                return "EMERGING"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—é
        recent_counts = counts[-3:] if len(counts) >= 3 else counts
        avg_recent = sum(recent_counts) / len(recent_counts)
        std_recent = np.std(recent_counts) if len(recent_counts) >= 2 else 0
        
        if std_recent / avg_recent < 0.2:  # –ù–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è
            return "MATURE"
        elif counts[-1] > 2 * counts[-2]:  # –°–∏–ª—å–Ω—ã–π —Ä–æ—Å—Ç
            return "RAPID_GROWTH"
        else:
            return "GROWING"

    def _predict_peak_year(self, year_counts: Dict[int, int]) -> int:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≥–æ–¥ –ø–∏–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏"""
        years = sorted(year_counts.keys())
        if len(years) < 3:
            return years[-1] + 2 if years else datetime.now().year + 2
        
        counts = [year_counts[y] for y in years]
        
        try:
            # –ü—Ä–æ—Å—Ç–∞—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è
            x = np.array(years)
            y = np.array(counts)
            
            # –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
            coeffs = np.polyfit(x, y, 1)
            future_years = np.array([years[-1] + 1, years[-1] + 2, years[-1] + 3])
            predictions = coeffs[0] * future_years + coeffs[1]
            
            # –ù–∞—Ö–æ–¥–∏–º –≥–æ–¥ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞
            peak_idx = np.argmax(predictions)
            return int(future_years[peak_idx])
        except:
            return years[-1] + 2

    def _calculate_confidence_score(self, term: str, year_counts: Dict[int, int]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–æ–≥–Ω–æ–∑–µ"""
        score = 0.0
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–µ—Ç –Ω–∞–±–ª—é–¥–µ–Ω–∏—è
        years_count = len(year_counts)
        if years_count >= 3:
            score += 30
        elif years_count == 2:
            score += 20
        else:
            score += 10
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π
        total_articles = len(self.term_articles[term])
        if total_articles >= 10:
            score += 30
        elif total_articles >= 5:
            score += 20
        elif total_articles >= 2:
            score += 10
        
        # –¢–µ–º–ø —Ä–æ—Å—Ç–∞
        counts = list(year_counts.values())
        if len(counts) >= 2:
            growth_rate = self._calculate_growth_rate(counts)
            score += min(30, growth_rate * 100)
        
        # –°–µ—Ç–µ–≤–∞—è —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å
        centrality = self._calculate_term_centrality(term)
        score += centrality * 10
        
        return min(100, score)

    def find_convergence_zones(self) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–µ—Ä–º–∏–Ω—ã, —Å–≤—è–∑—ã–≤–∞—é—â–∏–µ —Ä–∞–∑–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã"""
        cache_key = "convergence_zones"
        cached = self.cache.get_terminology_cache('convergence_zones', cache_key)
        if cached is not None:
            return cached
        
        if not self.term_network.nodes():
            return []
        
        convergence_terms = []
        
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º betweenness centrality
            centrality = nx.betweenness_centrality(self.term_network, normalized=True)
            
            for term, score in centrality.items():
                if score > 0.1:  # –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–≤—è–∑—ã–≤–∞–µ—Ç –ª–∏ —Ç–µ—Ä–º–∏–Ω —Ä–∞–∑–Ω—ã–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞
                    if self._connects_multiple_communities(term):
                        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                        degree = self.term_network.degree(term)
                        clustering = nx.clustering(self.term_network, term)
                        
                        convergence_terms.append({
                            'Term': term,
                            'Betweenness_Centrality': round(score, 4),
                            'Degree_Centrality': degree,
                            'Clustering_Coefficient': round(clustering, 3),
                            'Bridge_Score': round(self._calculate_bridge_score(term), 3),
                            'Community_Connections': len(self._get_connected_communities(term)),
                            'Semantic_Diversity': self._calculate_semantic_diversity(term),
                            'Strategic_Importance': self._determine_strategic_importance(term, score)
                        })
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error finding convergence zones: {e}")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ betweenness centrality
        convergence_sorted = sorted(convergence_terms, key=lambda x: x['Betweenness_Centrality'], reverse=True)
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.cache.set_terminology_cache('convergence_zones', cache_key, convergence_sorted)
        
        return convergence_sorted

    def _connects_multiple_communities(self, term: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–≤—è–∑—ã–≤–∞–µ—Ç –ª–∏ —Ç–µ—Ä–º–∏–Ω —Ä–∞–∑–Ω—ã–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞"""
        if term not in self.term_network:
            return False
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º Louvain –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å–æ–æ–±—â–µ—Å—Ç–≤
            communities = nx.algorithms.community.louvain_communities(self.term_network)
            
            # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ–±—â–µ—Å—Ç–≤–æ —Ç–µ—Ä–º–∏–Ω–∞
            term_community = None
            for i, community in enumerate(communities):
                if term in community:
                    term_community = i
                    break
            
            if term_community is None:
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞–º–∏
            neighbors = list(self.term_network.neighbors(term))
            neighbor_communities = set()
            
            for neighbor in neighbors:
                for i, community in enumerate(communities):
                    if neighbor in community and i != term_community:
                        neighbor_communities.add(i)
            
            return len(neighbor_communities) >= 2
            
        except:
            return False

    def _get_connected_communities(self, term: str) -> Set[int]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω–¥–µ–∫—Å—ã —Å–æ–æ–±—â–µ—Å—Ç–≤, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ —Å–≤—è–∑–∞–Ω —Ç–µ—Ä–º–∏–Ω"""
        if term not in self.term_network:
            return set()
        
        try:
            communities = nx.algorithms.community.louvain_communities(self.term_network)
            
            # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ–±—â–µ—Å—Ç–≤–æ —Ç–µ—Ä–º–∏–Ω–∞
            term_community = None
            for i, community in enumerate(communities):
                if term in community:
                    term_community = i
                    break
            
            if term_community is None:
                return set()
            
            # –ù–∞—Ö–æ–¥–∏–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞
            neighbors = list(self.term_network.neighbors(term))
            connected_communities = set()
            
            for neighbor in neighbors:
                for i, community in enumerate(communities):
                    if neighbor in community:
                        connected_communities.add(i)
            
            # –£–¥–∞–ª—è–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ
            connected_communities.discard(term_community)
            
            return connected_communities
            
        except:
            return set()

    def _calculate_bridge_score(self, term: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç score –º–æ—Å—Ç–æ–≤–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞"""
        if term not in self.term_network:
            return 0.0
        
        try:
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ—Å—Ç–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ —Ç–µ—Ä–º–∏–Ω
            connected_communities = self._get_connected_communities(term)
            
            # –°—Ä–µ–¥–Ω–∏–π –≤–µ—Å —Å–≤—è–∑–µ–π —Å –¥—Ä—É–≥–∏–º–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞–º–∏
            neighbors = list(self.term_network.neighbors(term))
            inter_community_weights = []
            
            for neighbor in neighbors:
                weight = self.term_network[term][neighbor].get('weight', 1)
                inter_community_weights.append(weight)
            
            if not inter_community_weights:
                return 0.0
            
            avg_weight = sum(inter_community_weights) / len(inter_community_weights)
            
            # –û–±—â–∏–π score
            bridge_score = len(connected_communities) * avg_weight
            
            return bridge_score
            
        except:
            return 0.0

    def _calculate_semantic_diversity(self, term: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–≤—è–∑–µ–π —Ç–µ—Ä–º–∏–Ω–∞"""
        if term not in self.term_network:
            return 0.0
        
        try:
            neighbors = list(self.term_network.neighbors(term))
            if len(neighbors) < 2:
                return 0.0
            
            # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–µ–ø–µ–Ω–µ–π —Å–æ—Å–µ–¥–µ–π
            neighbor_degrees = [self.term_network.degree(n) for n in neighbors]
            if not neighbor_degrees:
                return 0.0
            
            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ —Å—Ç–µ–ø–µ–Ω–µ–π —Å–æ—Å–µ–¥–µ–π
            mean_degree = np.mean(neighbor_degrees)
            std_degree = np.std(neighbor_degrees)
            
            if mean_degree > 0:
                cv = std_degree / mean_degree
                return min(1.0, cv)
            else:
                return 0.0
                
        except:
            return 0.0

    def _determine_strategic_importance(self, term: str, betweenness: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫—É—é –≤–∞–∂–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–∞"""
        if betweenness > 0.3:
            return "CRITICAL_BRIDGE"
        elif betweenness > 0.2:
            return "IMPORTANT_CONNECTOR"
        elif betweenness > 0.1:
            return "MODERATE_BRIDGE"
        else:
            return "MINOR_CONNECTOR"

    def predict_frontiers(self, top_n: int = 10) -> List[Dict]:
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç –Ω–∞—É—á–Ω—ã–µ —Ñ—Ä–æ–Ω—Ç–∏—Ä—ã"""
        cache_key = f"frontier_predictions_{top_n}"
        cached = self.cache.get_terminology_cache('frontier_predictions', cache_key)
        if cached is not None:
            return cached
        
        frontiers = []
        emerging_terms = self.detect_emerging_terms()
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø emerging terms
        top_emerging = emerging_terms[:min(top_n * 2, len(emerging_terms))]
        
        for term_info in top_emerging:
            term = term_info['Term']
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ç–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            network_metrics = self._analyze_term_network_characteristics(term)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            temporal_metrics = self._analyze_term_temporal_patterns(term)
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º frontier score
            frontier_score = self._calculate_frontier_score(term_info, network_metrics, temporal_metrics)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ—Ä–æ–Ω—Ç–∏—Ä–∞
            frontier_type = self._determine_frontier_type(term_info, network_metrics, temporal_metrics)
            
            # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º –≤—Ä–µ–º—è –¥–æ –º–∞—Å—Å–æ–≤–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è
            time_to_mass = self._predict_time_to_mass_adoption(term_info, temporal_metrics)
            
            # –ù–∞—Ö–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
            key_related_terms = self._get_key_related_terms(term, 5)
            
            frontiers.append({
                'Term': term,
                'Frontier_Score': round(frontier_score, 1),
                'Frontier_Type': frontier_type,
                'Emergence_Level': term_info['Maturity_Level'],
                'Growth_Rate_Percent': term_info['Recent_Growth_Rate'],
                'Network_Centrality': term_info['Network_Centrality'],
                'Time_To_Mass_Adoption_Years': time_to_mass,
                'Predicted_Peak_Year': term_info['Predicted_Peak_Year'],
                'Confidence_Score': term_info['Confidence_Score'],
                'Key_Related_Terms': '; '.join(key_related_terms),
                'Strategic_Recommendation': self._generate_strategic_recommendation(frontier_type, frontier_score),
                'Risk_Level': self._determine_frontier_risk_level(term_info, frontier_score),
                'Opportunity_Size': self._estimate_opportunity_size(term, network_metrics)
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ frontier score
        frontiers_sorted = sorted(frontiers, key=lambda x: x['Frontier_Score'], reverse=True)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        frontiers_final = frontiers_sorted[:top_n]
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.cache.set_terminology_cache('frontier_predictions', cache_key, frontiers_final)
        
        return frontiers_final

    def _analyze_term_network_characteristics(self, term: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Ç–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–µ—Ä–º–∏–Ω–∞"""
        if term not in self.term_network:
            return {
                'degree': 0,
                'clustering': 0,
                'eigenvector': 0,
                'coreness': 0,
                'structural_holes': 0
            }
        
        try:
            # –°—Ç–µ–ø–µ–Ω—å
            degree = self.term_network.degree(term)
            
            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            clustering = nx.clustering(self.term_network, term)
            
            # Eigenvector centrality
            try:
                eigenvector = nx.eigenvector_centrality_numpy(self.term_network).get(term, 0)
            except:
                eigenvector = 0
            
            # K-core decomposition
            try:
                k_core = nx.core_number(self.term_network).get(term, 0)
            except:
                k_core = 0
            
            # Structural holes (constraint)
            try:
                constraint = nx.constraint(self.term_network).get(term, 1)
                structural_holes = 1 - constraint
            except:
                structural_holes = 0
            
            return {
                'degree': degree,
                'clustering': round(clustering, 3),
                'eigenvector': round(eigenvector, 4),
                'coreness': k_core,
                'structural_holes': round(structural_holes, 3)
            }
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Network analysis error for term '{term}': {e}")
            return {
                'degree': 0,
                'clustering': 0,
                'eigenvector': 0,
                'coreness': 0,
                'structural_holes': 0
            }

    def _analyze_term_temporal_patterns(self, term: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–µ—Ä–º–∏–Ω–∞"""
        if term not in self.term_frequency:
            return {
                'years_count': 0,
                'total_count': 0,
                'growth_acceleration': 0,
                'seasonality': 0,
                'diffusion_speed': 0
            }
        
        year_counts = self.term_frequency[term]
        if len(year_counts) < 2:
            return {
                'years_count': len(year_counts),
                'total_count': sum(year_counts.values()),
                'growth_acceleration': 0,
                'seasonality': 0,
                'diffusion_speed': 0
            }
        
        try:
            years = sorted(year_counts.keys())
            counts = [year_counts[y] for y in years]
            
            # –£—Å–∫–æ—Ä–µ–Ω–∏–µ —Ä–æ—Å—Ç–∞ (—Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–Ω–∏–º –∏ –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–∏–º –ø—Ä–∏—Ä–æ—Å—Ç–æ–º)
            if len(counts) >= 3:
                last_increase = counts[-1] - counts[-2]
                prev_increase = counts[-2] - counts[-3]
                if prev_increase > 0:
                    growth_acceleration = (last_increase - prev_increase) / prev_increase
                else:
                    growth_acceleration = last_increase
            else:
                growth_acceleration = counts[-1] - counts[0] if counts[0] > 0 else counts[-1]
            
            # –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å (–≤–∞—Ä–∏–∞—Ü–∏—è –ø–æ –≥–æ–¥–∞–º)
            if len(counts) >= 3:
                cv = np.std(counts) / np.mean(counts) if np.mean(counts) > 0 else 0
                seasonality = cv
            else:
                seasonality = 0
            
            # –°–∫–æ—Ä–æ—Å—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ (—Å–∫–æ–ª—å–∫–æ –ª–µ—Ç –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è –¥–æ —Ç–µ–∫—É—â–µ–≥–æ)
            diffusion_years = years[-1] - years[0] + 1
            total_count = sum(counts)
            if diffusion_years > 0:
                diffusion_speed = total_count / diffusion_years
            else:
                diffusion_speed = total_count
            
            return {
                'years_count': len(years),
                'total_count': total_count,
                'growth_acceleration': round(growth_acceleration, 3),
                'seasonality': round(seasonality, 3),
                'diffusion_speed': round(diffusion_speed, 2)
            }
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Temporal analysis error for term '{term}': {e}")
            return {
                'years_count': len(year_counts),
                'total_count': sum(year_counts.values()),
                'growth_acceleration': 0,
                'seasonality': 0,
                'diffusion_speed': 0
            }

    def _calculate_frontier_score(self, term_info: Dict, network_metrics: Dict, temporal_metrics: Dict) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç frontier score"""
        score = 0.0
        
        # –¢–µ–º–ø —Ä–æ—Å—Ç–∞ (–º–∞–∫—Å 30)
        growth_rate = term_info.get('Recent_Growth_Rate', 0)
        score += min(30, growth_rate * 0.5)
        
        # –°–µ—Ç–µ–≤–∞—è —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å (–º–∞–∫—Å 20)
        centrality = term_info.get('Network_Centrality', 0)
        score += centrality * 20
        
        # –£—Å–∫–æ—Ä–µ–Ω–∏–µ —Ä–æ—Å—Ç–∞ (–º–∞–∫—Å 15)
        acceleration = temporal_metrics.get('growth_acceleration', 0)
        if acceleration > 0:
            score += min(15, acceleration * 5)
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –¥—ã—Ä—ã (–º–∞–∫—Å 15)
        structural_holes = network_metrics.get('structural_holes', 0)
        score += structural_holes * 15
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π (–º–∞–∫—Å 10)
        total_articles = term_info.get('Total_Articles', 0)
        if total_articles >= 10:
            score += 10
        elif total_articles >= 5:
            score += 7
        elif total_articles >= 2:
            score += 4
        else:
            score += 1
        
        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–∏–ø–æ–≤ —Å—Ç–∞—Ç–µ–π (–º–∞–∫—Å 10)
        type_diversity = term_info.get('Type_Diversity', 0)
        score += min(10, type_diversity * 3)
        
        return min(100, score)

    def _determine_frontier_type(self, term_info: Dict, network_metrics: Dict, temporal_metrics: Dict) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ñ—Ä–æ–Ω—Ç–∏—Ä–∞"""
        growth_rate = term_info.get('Recent_Growth_Rate', 0)
        centrality = term_info.get('Network_Centrality', 0)
        structural_holes = network_metrics.get('structural_holes', 0)
        
        if growth_rate > 50 and centrality > 0.7:
            return "BREAKTHROUGH_HOTSPOT"
        elif growth_rate > 30 and structural_holes > 0.3:
            return "INTEGRATION_NEXUS"
        elif growth_rate > 20:
            return "EMERGING_TREND"
        elif centrality > 0.6:
            return "STRATEGIC_BRIDGE"
        elif structural_holes > 0.4:
            return "INNOVATION_GAP"
        else:
            return "EARLY_SIGNAL"

    def _predict_time_to_mass_adoption(self, term_info: Dict, temporal_metrics: Dict) -> int:
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç –≤—Ä–µ–º—è –¥–æ –º–∞—Å—Å–æ–≤–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è"""
        growth_rate = term_info.get('Recent_Growth_Rate', 0) / 100  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã –≤ –¥–æ–ª—é
        current_articles = term_info.get('Total_Articles', 0)
        
        if growth_rate <= 0 or current_articles <= 0:
            return 10  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è "–º–∞—Å—Å–æ–≤–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è"
        target_articles = 100
        
        # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç: N = N0 * (1 + r)^t
        # –†–µ—à–∞–µ–º –¥–ª—è t: t = log(N/N0) / log(1 + r)
        try:
            if current_articles >= target_articles:
                return 0
            
            t = math.log(target_articles / current_articles) / math.log(1 + growth_rate)
            return max(1, min(15, int(t)))
        except:
            return 10

    def _get_key_related_terms(self, term: str, max_terms: int = 5) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã"""
        if term not in self.term_cooccurrence:
            return []
        
        related_terms = list(self.term_cooccurrence[term])
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ —Å–æ-–≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏
        sorted_terms = []
        for related in related_terms:
            if related in self.term_articles and term in self.term_articles:
                co_occurrence = len(set(self.term_articles[term]) & set(self.term_articles[related]))
                sorted_terms.append((related, co_occurrence))
        
        sorted_terms.sort(key=lambda x: x[1], reverse=True)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø —Ç–µ—Ä–º–∏–Ω—ã
        top_terms = [term for term, _ in sorted_terms[:max_terms]]
        
        return top_terms

    def _generate_strategic_recommendation(self, frontier_type: str, frontier_score: float) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        if frontier_score > 80:
            if frontier_type == "BREAKTHROUGH_HOTSPOT":
                return "IMMEDIATE INVESTMENT: High potential breakthrough area"
            elif frontier_type == "INTEGRATION_NEXUS":
                return "STRATEGIC POSITIONING: Bridge between established domains"
            else:
                return "AGGRESSIVE EXPLORATION: High-growth emerging area"
        elif frontier_score > 60:
            return "TARGETED RESEARCH: Promising area with good growth"
        elif frontier_score > 40:
            return "MONITOR CLOSELY: Early-stage opportunity"
        else:
            return "WATCHLIST: Early signal, needs validation"

    def _determine_frontier_risk_level(self, term_info: Dict, frontier_score: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞ —Ñ—Ä–æ–Ω—Ç–∏—Ä–∞"""
        growth_rate = term_info.get('Recent_Growth_Rate', 0)
        confidence = term_info.get('Confidence_Score', 0)
        
        if frontier_score > 70 and confidence > 70:
            return "LOW_RISK"
        elif frontier_score > 50 and confidence > 50:
            return "MODERATE_RISK"
        elif growth_rate > 40:
            return "HIGH_RISK_HIGH_REWARD"
        else:
            return "HIGH_RISK"

    def _estimate_opportunity_size(self, term: str, network_metrics: Dict) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏"""
        degree = network_metrics.get('degree', 0)
        eigenvector = network_metrics.get('eigenvector', 0)
        
        if degree >= 10 and eigenvector > 0.3:
            return "LARGE: Connects to established research areas"
        elif degree >= 5 and eigenvector > 0.1:
            return "MEDIUM: Growing network of connections"
        elif degree >= 2:
            return "SMALL: Niche opportunity"
        else:
            return "MICRO: Isolated concept"
        
    def get_term_statistics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ—Ä–º–∏–Ω–∞–º"""
        total_terms = len(self.term_frequency)
        total_articles = sum(len(articles) for articles in self.term_articles.values())
        
        # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        term_freqs = {}
        for term, year_counts in self.term_frequency.items():
            term_freqs[term] = sum(year_counts.values())
        
        top_terms = sorted(term_freqs.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # –î–∏–Ω–∞–º–∏–∫–∞ –ø–æ –≥–æ–¥–∞–º
        yearly_term_counts = defaultdict(int)
        for year_counts in self.term_frequency.values():
            for year, count in year_counts.items():
                yearly_term_counts[year] += count
        
        # –°—Ä–µ–¥–Ω–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        avg_clustering = 0
        if self.term_network.nodes():
            try:
                avg_clustering = nx.average_clustering(self.term_network)
            except:
                avg_clustering = 0
        
        # –ü–æ–¥—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤ –Ω–∞ —Å—Ç–∞—Ç—å—é
        total_articles_with_terms = len(self.term_articles)
        avg_terms_per_article = 0
        if total_articles_with_terms > 0:
            total_terms_in_articles = 0
            for articles_list in self.term_articles.values():
                total_terms_in_articles += len(articles_list)
            avg_terms_per_article = total_terms_in_articles / total_articles_with_terms
        
        # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        safe_top_terms = []
        for term, count in top_terms:
            safe_top_terms.append({
                'term': term,
                'count': count
            })
        
        safe_yearly_counts = []
        for year, count in sorted(yearly_term_counts.items()):
            safe_yearly_counts.append({
                'year': year,
                'count': count
            })
        
        return {
            'total_terms': total_terms,
            'total_articles_with_terms': total_articles_with_terms,
            'average_terms_per_article': round(avg_terms_per_article, 2),
            'top_terms': safe_top_terms,  # –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –≤–º–µ—Å—Ç–æ —Å–ø–∏—Å–∫–∞ –∫–æ—Ä—Ç–µ–∂–µ–π
            'yearly_term_counts': safe_yearly_counts,  # –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
            'network_nodes': self.term_network.number_of_nodes(),
            'network_edges': self.term_network.number_of_edges(),
            'average_clustering': round(avg_clustering, 3)
        }
# ============================================================================
# üìä –ö–õ–ê–°–° –≠–ö–°–ü–û–†–¢–ê –í EXCEL (–£–õ–£–ß–®–ï–ù–ù–´–ô –° –ù–û–í–´–ú–ò –§–£–ù–ö–¶–ò–Ø–ú–ò)
# ============================================================================

class ExcelExporter:
    def __init__(self, data_processor: DataProcessor, ror_client: RORClient,
                 failed_tracker: FailedDOITracker):
        self.processor = data_processor
        self.ror_client = ror_client
        self.failed_tracker = failed_tracker

        self.references_counter = Counter()
        self.citations_counter = Counter()
        self.ref_references_counter = Counter()
        self.ref_citations_counter = Counter()
        self.cite_references_counter = Counter()
        self.cite_citations_counter = Counter()

        self.analyzed_results = {}
        self.ref_results = {}
        self.citing_results = {}

        self.doi_to_source_counts = defaultdict(lambda: defaultdict(int))
        self.source_dois = {
            'analyzed': set(),
            'ref': set(),
            'citing': set()
        }

        self.ref_to_analyzed = defaultdict(list)
        self.analyzed_to_citing = defaultdict(list)

        self.author_stats = defaultdict(lambda: {
            'normalized_name': '',
            'orcid': '',
            'affiliation': '',
            'country': '',
            'total_count': 0,
            'normalized_analyzed': 0,
            'normalized_reference': 0,
            'normalized_citing': 0
        })

        self.affiliation_stats = defaultdict(lambda: {
            'colab_id': '',
            'website': '',
            'countries': [],
            'total_count': 0,
            'normalized_analyzed': 0,
            'normalized_reference': 0,
            'normalized_citing': 0
        })

        self.affiliation_country_stats = defaultdict(lambda: defaultdict(int))
        self.current_year = datetime.now().year

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
        self.hierarchical_analyzer = None
        self.terminology_analyzer = None

    def set_hierarchical_analyzer(self, hierarchical_analyzer: HierarchicalDataAnalyzer):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        self.hierarchical_analyzer = hierarchical_analyzer

    def set_terminology_analyzer(self, terminology_analyzer: TerminologyAnalyzer):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        self.terminology_analyzer = terminology_analyzer

    def _correct_country_for_author(self, author_key: str, affiliation_stats: Dict[str, Any]) -> str:
        """Correct country for author based on affiliation statistics"""
        author_info = self.author_stats[author_key]
        if not author_info['affiliation']:
            return author_info['country']

        affiliation = author_info['affiliation']
        if affiliation in affiliation_stats and affiliation_stats[affiliation]['countries']:
            countries = affiliation_stats[affiliation]['countries']
            if countries:
                country_counter = Counter(countries)
                most_common_country = country_counter.most_common(1)[0][0]

                if author_info['country'] != most_common_country:
                    website = affiliation_stats[affiliation].get('website', '')
                    if website:
                        domain_match = re.search(r'\.([a-z]{2,3})$', website.lower())
                        if domain_match:
                            domain_zone = domain_match.group(1).upper()
                            domain_to_country = {
                                'RU': 'RU', 'SU': 'RU',
                                'US': 'US', 'COM': 'US', 'ORG': 'US', 'NET': 'US',
                                'UK': 'GB', 'GB': 'GB', 'CO.UK': 'GB',
                                'DE': 'DE', 'FR': 'FR', 'IT': 'IT', 'ES': 'ES',
                                'CN': 'CN', 'JP': 'JP', 'KR': 'KR', 'IN': 'IN',
                                'AU': 'AU', 'CA': 'CA', 'BR': 'BR', 'MX': 'MX'
                            }

                            if domain_zone in domain_to_country:
                                website_country = domain_to_country[domain_zone]
                                if website_country == most_common_country:
                                    return most_common_country

                    if len(countries) >= 3:
                        country_freq = country_counter[most_common_country] / len(countries)
                        if country_freq >= 0.7:
                            return most_common_country

        return author_info['country']

    def _calculate_annual_citation_rate(self, citation_count: int, publication_year_str: str) -> float:
        """Calculate average annual citations"""
        if not citation_count or not publication_year_str:
            return 0.0

        try:
            pub_year = int(publication_year_str)
            age = self.current_year - pub_year + 1
            if age <= 0:
                age = 1

            return citation_count / age
        except:
            return 0.0

    def _analyze_ethical_insights(self, analysis_types: Dict[str, bool], progress_container=None) -> Dict[str, Any]:
        """Analyze ethical insights from collected data"""
        insights = {
            'quick_checks': [],
            'medium_insights': [],
            'deep_analysis': [],
            'analyzed_citing_relationships': []
        }

        if not self.hierarchical_analyzer:
            st.warning("‚ö†Ô∏è Hierarchical analyzer not set. Skipping ethical insights.")
            return insights

        # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ç–∏–ø—ã –∞–Ω–∞–ª–∏–∑–∞
        if analysis_types.get('quick_checks', False):
            if progress_container:
                progress_container.text("üîç Performing Quick Checks analysis...")
            insights['quick_checks'] = self.hierarchical_analyzer.analyze_quick_checks(
                self.analyzed_results, self.citing_results
            )

        if analysis_types.get('medium_insights', False):
            if progress_container:
                progress_container.text("üîç Performing Medium Insights analysis...")
            insights['medium_insights'] = self.hierarchical_analyzer.analyze_medium_insights(
                self.analyzed_results, self.citing_results
            )

        if analysis_types.get('deep_analysis', False):
            if progress_container:
                progress_container.text("üîç Performing Deep Analysis...")
            insights['deep_analysis'] = self.hierarchical_analyzer.analyze_deep_analysis(
                self.analyzed_results, self.citing_results, self.ref_results
            )

        if analysis_types.get('analyzed_citing_relationships', False):
            if progress_container:
                progress_container.text("üîç Performing Analyzed-Citing Relationships analysis...")
            insights['analyzed_citing_relationships'] = self.hierarchical_analyzer.analyze_citing_relationships(
                self.analyzed_results, self.citing_results
            )

        return insights

    def _analyze_terminology_insights(self, analysis_types: Dict[str, bool], progress_container=None) -> Dict[str, Any]:
        """Analyze terminology insights from collected data"""
        insights = {
            'emerging_terms': [],
            'convergence_zones': [],
            'frontier_predictions': [],
            'term_statistics': {}
        }

        if not self.terminology_analyzer:
            st.warning("‚ö†Ô∏è Terminology analyzer not set. Skipping terminology insights.")
            return insights

        # –°—Ç—Ä–æ–∏–º —Å–µ—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤
        if progress_container:
            progress_container.text("üî§ Building term network...")
        self.terminology_analyzer.build_term_network()

        # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ç–∏–ø—ã –∞–Ω–∞–ª–∏–∑–∞
        if analysis_types.get('emerging_terms', False):
            if progress_container:
                progress_container.text("üî§ Detecting emerging terms...")
            insights['emerging_terms'] = self.terminology_analyzer.detect_emerging_terms()

        if analysis_types.get('convergence_zones', False):
            if progress_container:
                progress_container.text("üî§ Finding convergence zones...")
            insights['convergence_zones'] = self.terminology_analyzer.find_convergence_zones()

        if analysis_types.get('frontier_predictions', False):
            if progress_container:
                progress_container.text("üî§ Predicting frontiers...")
            insights['frontier_predictions'] = self.terminology_analyzer.predict_frontiers()

        # –í—Å–µ–≥–¥–∞ —Å–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ—Ä–º–∏–Ω–æ–≤
        insights['term_statistics'] = self.terminology_analyzer.get_term_statistics()

        return insights

    def create_comprehensive_report(self, analyzed_results: Dict[str, Dict],
                                   ref_results: Dict[str, Dict] = None,
                                   citing_results: Dict[str, Dict] = None,
                                   analysis_types: Dict[str, bool] = None,
                                   filename: str = None,
                                   progress_container=None) -> BytesIO:
    
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"articles_analysis_comprehensive_{timestamp}.xlsx"
    
        if progress_container:
            progress_container.text(f"üìä Creating comprehensive report: {filename}")
    
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∏–ø—ã –∞–Ω–∞–ª–∏–∑–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã
        if analysis_types is None:
            analysis_types = {
                'quick_checks': True,
                'medium_insights': True,
                'deep_analysis': False,
                'analyzed_citing_relationships': False,
                'emerging_terms': True,
                'convergence_zones': True,
                'frontier_predictions': True
            }
    
        self.analyzed_results = analyzed_results
        self.ref_results = ref_results or {}
        self.citing_results = citing_results or {}
    
        self._prepare_summary_data()
    
        # Generate ethical insights
        ethical_insights = self._analyze_ethical_insights(analysis_types, progress_container)
    
        # Generate terminology insights
        terminology_insights = self._analyze_terminology_insights(analysis_types, progress_container)
    
        # –°–æ–∑–¥–∞–µ–º Excel —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç–∏
        output = BytesIO()
        
        try:
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                if progress_container:
                    progress_container.text("üìë Generating sheets...")
    
                # –°–æ–∑–¥–∞–µ–º –≤–∫–ª–∞–¥–∫–∏ Excel —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
                self._generate_excel_sheets(writer, analyzed_results, ref_results, citing_results, 
                                          ethical_insights, terminology_insights, analysis_types, progress_container)
        except Exception as e:
            st.error(f"Error creating Excel file: {str(e)}")
            # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            output = BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                error_df = pd.DataFrame([{'Error': str(e), 'Time': datetime.now().isoformat()}])
                error_df.to_excel(writer, sheet_name='Error_Report', index=False)
    
        output.seek(0)
        return output

    def _generate_excel_sheets(self, writer, analyzed_results, ref_results, citing_results,
                             ethical_insights, terminology_insights, analysis_types, progress_container):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –≤–∫–ª–∞–¥–∫–∏ Excel —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        sheets = [
            ('Article_Analyzed', lambda: self._prepare_analyzed_articles(analyzed_results)),
            ('Author freq_analyzed', lambda: self._prepare_author_frequency(analyzed_results, "analyzed")),
            ('Journal freq_analyzed', lambda: self._prepare_journal_frequency(analyzed_results, "analyzed")),
            ('Affiliation freq_analyzed', lambda: self._prepare_affiliation_frequency(analyzed_results, "analyzed")),
            ('Country freq_analyzed', lambda: self._prepare_country_frequency(analyzed_results, "analyzed")),
            ('Article_ref', lambda: self._prepare_article_ref()),
            ('Author freq_ref', lambda: self._prepare_author_frequency(ref_results, "ref") if ref_results else []),
            ('Journal freq_ref', lambda: self._prepare_journal_frequency(ref_results, "ref") if ref_results else []),
            ('Affiliation freq_ref', lambda: self._prepare_affiliation_frequency(ref_results, "ref") if ref_results else []),
            ('Country freq_ref', lambda: self._prepare_country_frequency(ref_results, "ref") if ref_results else []),
            ('Article_citing', lambda: self._prepare_article_citing()),
            ('Author freq_citing', lambda: self._prepare_author_frequency(citing_results, "citing") if citing_results else []),
            ('Journal freq_citing', lambda: self._prepare_journal_frequency(citing_results, "citing") if citing_results else []),
            ('Affiliation freq_citing', lambda: self._prepare_affiliation_frequency(citing_results, "citing") if citing_results else []),
            ('Country freq_citing', lambda: self._prepare_country_frequency(citing_results, "citing") if citing_results else []),
            ('Author_summary', lambda: self._prepare_author_summary()),
            ('Affiliation_summary', lambda: self._prepare_affiliation_summary()),
            ('Time (Ref,analyzed)_connections', lambda: self._prepare_time_ref_analyzed_connections()),
            ('Time (analyzed,citing)_connections', lambda: self._prepare_time_analyzed_citing_connections()),
            ('Failed_DOI', lambda: self.failed_tracker.get_failed_for_excel()),
            ('Analysis_Stats', lambda: self._prepare_analysis_stats(analyzed_results, ref_results, citing_results)),
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ª–∏—Å—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—ç—Ç–∏—á–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫ –µ—Å–ª–∏ –æ–Ω–∏ –≤–∫–ª—é—á–µ–Ω—ã
        if analysis_types.get('quick_checks', False) and ethical_insights['quick_checks']:
            sheets.append(('Quick_Checks', lambda: ethical_insights['quick_checks']))
        
        if analysis_types.get('medium_insights', False) and ethical_insights['medium_insights']:
            sheets.append(('Medium_Insights', lambda: ethical_insights['medium_insights']))
        
        if analysis_types.get('deep_analysis', False) and ethical_insights['deep_analysis']:
            sheets.append(('Deep_Analysis', lambda: ethical_insights['deep_analysis']))
        
        if analysis_types.get('analyzed_citing_relationships', False) and ethical_insights['analyzed_citing_relationships']:
            sheets.append(('Analyzed_Citing_Relationships', lambda: ethical_insights['analyzed_citing_relationships']))
    
        # –î–æ–±–∞–≤–ª—è–µ–º –ª–∏—Å—Ç—ã —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –µ—Å–ª–∏ –æ–Ω–∏ –≤–∫–ª—é—á–µ–Ω—ã
        try:
            if analysis_types.get('emerging_terms', False) and terminology_insights['emerging_terms']:
                sheets.append(('Emerging_Terms', lambda: terminology_insights['emerging_terms']))
            
            if analysis_types.get('convergence_zones', False) and terminology_insights['convergence_zones']:
                sheets.append(('Convergence_Zones', lambda: terminology_insights['convergence_zones']))
            
            if analysis_types.get('frontier_predictions', False) and terminology_insights['frontier_predictions']:
                sheets.append(('Frontier_Predictions', lambda: terminology_insights['frontier_predictions']))
            
            # –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ—Ä–º–∏–Ω–æ–≤
            if terminology_insights['term_statistics']:
                try:
                    # –ó–∞—â–∏—â–∞–µ–º –≤—ã–∑–æ–≤ _prepare_term_statistics
                    term_stats_data = self._prepare_term_statistics(terminology_insights['term_statistics'])
                    if term_stats_data:
                        sheets.append(('Term_Statistics', lambda: term_stats_data))
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Error preparing term statistics: {e}")
                    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–∏ –æ—à–∏–±–∫–µ
                    error_stats = [{
                        'Metric': 'Error',
                        'Value': str(e),
                        'Description': 'Failed to generate term statistics'
                    }]
                    sheets.append(('Term_Statistics', lambda: error_stats))
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Skipping terminology sheets due to error: {e}")
    
        for idx, (sheet_name, data_func) in enumerate(sheets):
            if progress_container:
                progress_container.text(f"  {idx+1}. {sheet_name}...")
            
            try:
                data = data_func()
                if data:
                    df = pd.DataFrame(data)
                    # –û—á–∏—Å—Ç–∫–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –¥–ª—è Excel
                    for col in df.columns:
                        if df[col].dtype == 'object':
                            df[col] = df[col].apply(lambda x: str(x)[:32767] if isinstance(x, str) else x)
                    df.to_excel(writer, sheet_name=sheet_name[:31], index=False)  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –∏–º–µ–Ω–∏ –ª–∏—Å—Ç–∞
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Error creating sheet '{sheet_name}': {e}")
                # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π –ª–∏—Å—Ç —Å –æ—à–∏–±–∫–æ–π
                error_df = pd.DataFrame([{'Sheet': sheet_name, 'Error': str(e)}])
                error_df.to_excel(writer, sheet_name=f'Error_{idx}'[:31], index=False)
                
    def _prepare_term_statistics(self, term_stats: Dict[str, Any]) -> List[Dict]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        data = []
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        data.append({
            'Metric': 'Total Terms',
            'Value': term_stats.get('total_terms', 0),
            'Description': 'Total unique terms extracted'
        })
        
        data.append({
            'Metric': 'Articles with Terms',
            'Value': term_stats.get('total_articles_with_terms', 0),
            'Description': 'Articles containing extracted terms'
        })
        
        avg_terms = term_stats.get('average_terms_per_article', 0)
        if isinstance(avg_terms, (int, float)):
            avg_terms = round(avg_terms, 2)
        elif isinstance(avg_terms, dict):  # –ï—Å–ª–∏ —ç—Ç–æ dict, –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            avg_terms = round(list(avg_terms.values())[0] if avg_terms else 0, 2)
        
        data.append({
            'Metric': 'Avg Terms per Article',
            'Value': avg_terms,
            'Description': 'Average number of terms per article'
        })
        
        data.append({
            'Metric': 'Network Nodes',
            'Value': term_stats.get('network_nodes', 0),
            'Description': 'Number of nodes in term network'
        })
        
        data.append({
            'Metric': 'Network Edges',
            'Value': term_stats.get('network_edges', 0),
            'Description': 'Number of edges in term network'
        })
        
        clustering = term_stats.get('average_clustering', 0)
        if isinstance(clustering, (int, float)):
            clustering = round(clustering, 3)
        elif isinstance(clustering, dict):  # –ï—Å–ª–∏ —ç—Ç–æ dict, –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            clustering = round(list(clustering.values())[0] if clustering else 0, 3)
        
        data.append({
            'Metric': 'Average Clustering',
            'Value': clustering,
            'Description': 'Average clustering coefficient'
        })
        
        # –¢–æ–ø —Ç–µ—Ä–º–∏–Ω—ã - –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        top_terms = term_stats.get('top_terms', [])
        if isinstance(top_terms, (list, tuple)):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–∂–Ω–æ –≤–∑—è—Ç—å —Å—Ä–µ–∑
            try:
                for i, term_item in enumerate(top_terms[:10], 1):
                    if isinstance(term_item, (list, tuple)) and len(term_item) >= 2:
                        term, count = term_item[0], term_item[1]
                        data.append({
                            'Metric': f'Top Term #{i}',
                            'Value': str(term)[:100] if term is not None else '',
                            'Description': f'Frequency: {count} articles'
                        })
                    elif isinstance(term_item, dict):
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ term_item - —ç—Ç–æ dict
                        for term_key, count in list(term_item.items())[:1]:
                            data.append({
                                'Metric': f'Top Term #{i}',
                                'Value': str(term_key)[:100] if term_key is not None else '',
                                'Description': f'Frequency: {count} articles'
                            })
                            break
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Error processing top terms: {e}")
        elif isinstance(top_terms, dict):
            # –ï—Å–ª–∏ top_terms - —ç—Ç–æ dict, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å
            try:
                for i, (term, count) in enumerate(list(top_terms.items())[:10], 1):
                    data.append({
                        'Metric': f'Top Term #{i}',
                        'Value': str(term)[:100] if term is not None else '',
                        'Description': f'Frequency: {count} articles'
                    })
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Error processing top terms dict: {e}")
        
        # –ì–æ–¥–æ–≤–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ - –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        yearly_counts = term_stats.get('yearly_term_counts', {})
        if isinstance(yearly_counts, dict):
            for year, count in sorted(yearly_counts.items(), key=lambda x: str(x[0])):
                data.append({
                    'Metric': f'Year {year}',
                    'Value': count,
                    'Description': f'Terms appeared in {year}'
                })
        elif isinstance(yearly_counts, (list, tuple)):
            # –ï—Å–ª–∏ yearly_counts - —ç—Ç–æ —Å–ø–∏—Å–æ–∫/–∫–æ—Ä—Ç–µ–∂
            try:
                for i, (year, count) in enumerate(yearly_counts):
                    data.append({
                        'Metric': f'Year {year}',
                        'Value': count,
                        'Description': f'Terms appeared in {year}'
                    })
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Error processing yearly counts: {e}")
        
        return data

    def _prepare_summary_data(self):
        total_analyzed_articles = len([r for r in self.analyzed_results.values() if r.get('status') == 'success'])
        total_ref_articles = len([r for r in self.ref_results.values() if r.get('status') == 'success'])
        total_citing_articles = len([r for r in self.citing_results.values() if r.get('status') == 'success'])

        for doi, result in self.analyzed_results.items():
            if result.get('status') != 'success':
                continue

            self.source_dois['analyzed'].add(doi)

            for ref_doi in result.get('references', []):
                self.ref_to_analyzed[ref_doi].append(doi)
                self.doi_to_source_counts[ref_doi]['ref'] += 1
                self.source_dois['ref'].add(ref_doi)

            for cite_doi in result.get('citations', []):
                self.analyzed_to_citing[doi].append(cite_doi)
                self.doi_to_source_counts[cite_doi]['citing'] += 1
                self.source_dois['citing'].add(cite_doi)

            # Update author stats with normalized values
            for author in result.get('authors', []):
                full_name = author.get('name', '')
                if not full_name:
                    continue

                normalized_name = self.processor.normalize_author_name(full_name)
                key = normalized_name

                if author.get('orcid'):
                    key = f"{normalized_name}_{author['orcid']}"

                # Calculate normalized value for analyzed articles
                normalized_value = 1 / total_analyzed_articles if total_analyzed_articles > 0 else 0
                self.author_stats[key]['normalized_analyzed'] += normalized_value
                self.author_stats[key]['total_count'] += normalized_value

                if not self.author_stats[key]['orcid'] and author.get('orcid'):
                    self.author_stats[key]['orcid'] = self.processor._format_orcid_id(author.get('orcid', ''))

                if not self.author_stats[key]['affiliation'] and author.get('affiliation'):
                    self.author_stats[key]['affiliation'] = author.get('affiliation')[0] if author.get('affiliation') else ''

                if result.get('countries'):
                    country = result.get('countries')[0] if result.get('countries') else ''
                    if country and not self.author_stats[key]['country']:
                        self.author_stats[key]['country'] = country

                    if self.author_stats[key]['affiliation']:
                        self.affiliation_country_stats[self.author_stats[key]['affiliation']][country] += 1

                self.author_stats[key]['normalized_name'] = normalized_name

            # Update affiliation stats with normalized values
            unique_affiliations_in_article = set()
            for author in result.get('authors', []):
                for affiliation in author.get('affiliation', []):
                    if affiliation:
                        unique_affiliations_in_article.add(affiliation)

            normalized_aff_value = 1 / total_analyzed_articles if total_analyzed_articles > 0 else 0
            for affiliation in unique_affiliations_in_article:
                self.affiliation_stats[affiliation]['normalized_analyzed'] += normalized_aff_value
                self.affiliation_stats[affiliation]['total_count'] += normalized_aff_value

                if result.get('countries'):
                    for country in result.get('countries'):
                        if country:
                            self.affiliation_stats[affiliation]['countries'].append(country)

        # Process ref results
        for doi, result in self.ref_results.items():
            if result.get('status') != 'success':
                continue

            # Update author stats for ref articles
            for author in result.get('authors', []):
                full_name = author.get('name', '')
                if not full_name:
                    continue

                normalized_name = self.processor.normalize_author_name(full_name)
                key = normalized_name

                if author.get('orcid'):
                    key = f"{normalized_name}_{author['orcid']}"

                # Calculate normalized value for ref articles
                normalized_value = 1 / total_ref_articles if total_ref_articles > 0 else 0
                self.author_stats[key]['normalized_reference'] += normalized_value
                self.author_stats[key]['total_count'] += normalized_value

                if not self.author_stats[key]['orcid'] and author.get('orcid'):
                    self.author_stats[key]['orcid'] = self.processor._format_orcid_id(author.get('orcid', ''))

                if not self.author_stats[key]['affiliation'] and author.get('affiliation'):
                    self.author_stats[key]['affiliation'] = author.get('affiliation')[0] if author.get('affiliation') else ''

                if not self.author_stats[key]['country'] and result.get('countries'):
                    self.author_stats[key]['country'] = result.get('countries')[0] if result.get('countries') else ''

                self.author_stats[key]['normalized_name'] = normalized_name

            # Update affiliation stats for ref articles
            unique_affiliations_in_article = set()
            for author in result.get('authors', []):
                for affiliation in author.get('affiliation', []):
                    if affiliation:
                        unique_affiliations_in_article.add(affiliation)

            normalized_aff_value = 1 / total_ref_articles if total_ref_articles > 0 else 0
            for affiliation in unique_affiliations_in_article:
                self.affiliation_stats[affiliation]['normalized_reference'] += normalized_aff_value
                self.affiliation_stats[affiliation]['total_count'] += normalized_aff_value

        # Process citing results
        for doi, result in self.citing_results.items():
            if result.get('status') != 'success':
                continue

            # Update author stats for citing articles
            for author in result.get('authors', []):
                full_name = author.get('name', '')
                if not full_name:
                    continue

                normalized_name = self.processor.normalize_author_name(full_name)
                key = normalized_name

                if author.get('orcid'):
                    key = f"{normalized_name}_{author['orcid']}"

                # Calculate normalized value for citing articles
                normalized_value = 1 / total_citing_articles if total_citing_articles > 0 else 0
                self.author_stats[key]['normalized_citing'] += normalized_value
                self.author_stats[key]['total_count'] += normalized_value

                if not self.author_stats[key]['orcid'] and author.get('orcid'):
                    self.author_stats[key]['orcid'] = self.processor._format_orcid_id(author.get('orcid', ''))

                if not self.author_stats[key]['affiliation'] and author.get('affiliation'):
                    self.author_stats[key]['affiliation'] = author.get('affiliation')[0] if author.get('affiliation') else ''

                if not self.author_stats[key]['country'] and result.get('countries'):
                    self.author_stats[key]['country'] = result.get('countries')[0] if result.get('countries') else ''

                self.author_stats[key]['normalized_name'] = normalized_name

            # Update affiliation stats for citing articles
            unique_affiliations_in_article = set()
            for author in result.get('authors', []):
                for affiliation in author.get('affiliation', []):
                    if affiliation:
                        unique_affiliations_in_article.add(affiliation)

            normalized_aff_value = 1 / total_citing_articles if total_citing_articles > 0 else 0
            for affiliation in unique_affiliations_in_article:
                self.affiliation_stats[affiliation]['normalized_citing'] += normalized_aff_value
                self.affiliation_stats[affiliation]['total_count'] += normalized_aff_value

        affiliations_list = list(self.affiliation_stats.keys())

        for aff in affiliations_list:
            ror_info = self.ror_client.search_organization(aff, category="summary")
            if ror_info.get('ror_id'):
                self.affiliation_stats[aff]['colab_id'] = ror_info.get('ror_id', '')
                self.affiliation_stats[aff]['website'] = ror_info.get('website', '')

    def _prepare_analyzed_articles(self, results: Dict[str, Dict]) -> List[Dict]:
        return self._prepare_article_sheet(results, "analyzed")

    def _prepare_article_ref(self) -> List[Dict]:
        data = []

        processed_refs = {}
        for ref_doi, ref_result in self.ref_results.items():
            if ref_result.get('status') == 'success':
                processed_refs[ref_doi] = ref_result

        for ref_doi, ref_result in processed_refs.items():
            count = len(self.ref_to_analyzed.get(ref_doi, []))

            pub_info = ref_result.get('publication_info', {})
            authors = ref_result.get('authors', [])

            orcid_urls = ref_result.get('orcid_urls', [])
            affiliations = list(set([aff for author in authors for aff in author.get('affiliation', []) if aff]))
            countries = ref_result.get('countries', [])

            annual_cr = self._calculate_annual_citation_rate(
                pub_info.get('citation_count_crossref', 0),
                pub_info.get('year', '')
            )
            annual_oa = self._calculate_annual_citation_rate(
                pub_info.get('citation_count_openalex', 0),
                pub_info.get('year', '')
            )

            row = {
                'doi': ref_doi,
                'publication_date': pub_info.get('publication_date', ''),
                'title': pub_info.get('title', ''),  # –ù–û–í–ê–Ø –ö–û–õ–û–ù–ö–ê
                'authors': '; '.join([a['name'] for a in authors]),
                'ORCID ID 1; ORCID ID 2... ORCID ID last': '; '.join(orcid_urls),
                'author count': len(authors),
                'affiliations {aff 1; aff 2... aff last}': '; '.join(affiliations),
                'countries {country 1; ... country last}': '; '.join(countries),
                'Full journal Name': pub_info.get('journal', ''),
                'year': pub_info.get('year', ''),
                'Volume': pub_info.get('volume', ''),
                'Pages (or article number)': ref_result.get('pages_formatted', ''),
                'Citation counts (CR)': pub_info.get('citation_count_crossref', 0),
                'Citation counts (OA)': pub_info.get('citation_count_openalex', 0),
                'Annual cit counts (CR)': round(annual_cr, 2),
                'Annual cit counts (OA)': round(annual_oa, 2),
                'references_count': len(ref_result.get('references', [])),
                'count': count
            }

            data.append(row)

        for ref_doi in self.ref_to_analyzed:
            if ref_doi not in processed_refs:
                count = len(self.ref_to_analyzed.get(ref_doi, []))
                row = {
                    'doi': ref_doi,
                    'publication_date': '',
                    'title': '',  # –ù–û–í–ê–Ø –ö–û–õ–û–ù–ö–ê
                    'authors': '',
                    'ORCID ID 1; ORCID ID 2... ORCID ID last': '',
                    'author count': 0,
                    'affiliations {aff 1; aff 2... aff last}': '',
                    'countries {country 1; ... country last}': '',
                    'Full journal Name': '',
                    'year': '',
                    'Volume': '',
                    'Pages (or article number)': '',
                    'Citation counts (CR)': 0,
                    'Citation counts (OA)': 0,
                    'Annual cit counts (CR)': 0.0,
                    'Annual cit counts (OA)': 0.0,
                    'references_count': 0,
                    'count': count
                }
                data.append(row)

        data = self._sort_article_data_by_count_and_date(data)

        return data

    def _prepare_article_citing(self) -> List[Dict]:
        data = []

        processed_cites = {}
        for cite_doi, cite_result in self.citing_results.items():
            if cite_result.get('status') == 'success':
                processed_cites[cite_doi] = cite_result

        for cite_doi, cite_result in processed_cites.items():
            count = sum(1 for analyzed_list in self.analyzed_to_citing.values() if cite_doi in analyzed_list)

            pub_info = cite_result.get('publication_info', {})
            authors = cite_result.get('authors', [])

            orcid_urls = cite_result.get('orcid_urls', [])
            affiliations = list(set([aff for author in authors for aff in author.get('affiliation', []) if aff]))
            countries = cite_result.get('countries', [])

            annual_cr = self._calculate_annual_citation_rate(
                pub_info.get('citation_count_crossref', 0),
                pub_info.get('year', '')
            )
            annual_oa = self._calculate_annual_citation_rate(
                pub_info.get('citation_count_openalex', 0),
                pub_info.get('year', '')
            )

            row = {
                'doi': cite_doi,
                'publication_date': pub_info.get('publication_date', ''),
                'title': pub_info.get('title', ''),  # –ù–û–í–ê–Ø –ö–û–õ–û–ù–ö–ê
                'authors': '; '.join([a['name'] for a in authors]),
                'ORCID ID 1; ORCID ID 2... ORCID ID last': '; '.join(orcid_urls),
                'author count': len(authors),
                'affiliations {aff 1; aff 2... aff last}': '; '.join(affiliations),
                'countries {country 1; ... country last}': '; '.join(countries),
                'Full journal Name': pub_info.get('journal', ''),
                'year': pub_info.get('year', ''),
                'Volume': pub_info.get('volume', ''),
                'Pages (or article number)': cite_result.get('pages_formatted', ''),
                'Citation counts (CR)': pub_info.get('citation_count_crossref', 0),
                'Citation counts (OA)': pub_info.get('citation_count_openalex', 0),
                'Annual cit counts (CR)': round(annual_cr, 2),
                'Annual cit counts (OA)': round(annual_oa, 2),
                'references_count': len(cite_result.get('references', [])),
                'count': count
            }

            data.append(row)

        all_citing_dois = set()
        for analyzed_list in self.analyzed_to_citing.values():
            all_citing_dois.update(analyzed_list)

        for cite_doi in all_citing_dois:
            if cite_doi not in processed_cites:
                count = sum(1 for analyzed_list in self.analyzed_to_citing.values() if cite_doi in analyzed_list)
                row = {
                    'doi': cite_doi,
                    'publication_date': '',
                    'title': '',  # –ù–û–í–ê–Ø –ö–û–õ–û–ù–ö–ê
                    'authors': '',
                    'ORCID ID 1; ORCID ID 2... ORCID ID last': '',
                    'author count': 0,
                    'affiliations {aff 1; aff 2... aff last}': '',
                    'countries {country 1; ... country last}': '',
                    'Full journal Name': '',
                    'year': '',
                    'Volume': '',
                    'Pages (or article number)': '',
                    'Citation counts (CR)': 0,
                    'Citation counts (OA)': 0,
                    'Annual cit counts (CR)': 0.0,
                    'Annual cit counts (OA)': 0.0,
                    'references_count': 0,
                    'count': count
                }
                data.append(row)

        data = self._sort_article_data_by_count_and_date(data)

        return data

    def _sort_article_data_by_count_and_date(self, data: List[Dict]) -> List[Dict]:
        def sort_key(row):
            count = row.get('count', 0)
            date_str = row.get('publication_date', '')

            date_value = None
            if date_str:
                try:
                    for fmt in ['%Y-%m-%d', '%Y-%m', '%Y']:
                        try:
                            date_value = datetime.strptime(date_str[:len(fmt)], fmt)
                            break
                        except:
                            continue
                except:
                    date_value = None

            count_sort = -count

            if date_value:
                date_sort = -date_value.timestamp()
            else:
                date_sort = 0

            return (count_sort, date_sort)

        return sorted(data, key=sort_key)

    def _prepare_article_sheet(self, results: Dict[str, Dict], source_type: str) -> List[Dict]:
        data = []

        for doi, result in results.items():
            if result.get('status') != 'success':
                continue

            pub_info = result['publication_info']
            authors = result['authors']

            orcid_urls = result.get('orcid_urls', [])
            affiliations = list(set([aff for author in authors for aff in author.get('affiliation', []) if aff]))
            countries = result.get('countries', [])

            annual_cr = self._calculate_annual_citation_rate(
                pub_info.get('citation_count_crossref', 0),
                pub_info.get('year', '')
            )
            annual_oa = self._calculate_annual_citation_rate(
                pub_info.get('citation_count_openalex', 0),
                pub_info.get('year', '')
            )

            row = {
                'doi': doi,
                'publication_date': pub_info.get('publication_date', ''),
                'authors': '; '.join([a['name'] for a in authors]),
                'ORCID ID 1; ORCID ID 2... ORCID ID last': '; '.join(orcid_urls),
                'author count': len(authors),
                'affiliations {aff 1; aff 2... aff last}': '; '.join(affiliations),
                'countries {country 1; ... country last}': '; '.join(countries),
                'Full journal Name': pub_info.get('journal', ''),
                'year': pub_info.get('year', ''),
                'Volume': pub_info.get('volume', ''),
                'Pages (or article number)': result.get('pages_formatted', ''),
                'Citation counts (CR)': pub_info.get('citation_count_crossref', 0),
                'Citation counts (OA)': pub_info.get('citation_count_openalex', 0),
                'Annual cit counts (CR)': round(annual_cr, 2),
                'Annual cit counts (OA)': round(annual_oa, 2),
                'references_count': len(result.get('references', []))
            }

            data.append(row)

        return data

    def _prepare_author_frequency(self, results: Dict[str, Dict], source_type: str) -> List[Dict]:
        author_counter = Counter()
        author_details = {}

        for doi, result in results.items():
            if result.get('status') != 'success':
                continue

            for author in result['authors']:
                full_name = author['name']
                normalized_name = self.processor.normalize_author_name(full_name)

                key = normalized_name
                if author.get('orcid'):
                    key = f"{normalized_name}_{author['orcid']}"

                author_counter[key] += 1

                if key not in author_details:
                    affiliation = author['affiliation'][0] if author.get('affiliation') else ""
                    orcid = author.get('orcid', '')

                    author_details[key] = {
                        'orcid': self.processor._format_orcid_id(orcid) if orcid else '',
                        'affiliation': affiliation,
                        'country': result.get('countries', [''])[0] if result.get('countries') else '',
                        'normalized_name': normalized_name
                    }

        sorted_authors = sorted(author_counter.items(), key=lambda x: x[1], reverse=True)

        data = []
        for key, count in sorted_authors:
            details = author_details.get(key, {})

            if source_type == "analyzed":
                frequency_column = 'Frequency count {in the analyzed articles}'
            elif source_type == "ref":
                frequency_column = 'Frequency count {in the reference articles}'
            elif source_type == "citing":
                frequency_column = 'Frequency count {in the citing articles}'
            else:
                frequency_column = f'Frequency count {{{source_type}}}'

            row = {
                'Surname + Initial_normalized': details.get('normalized_name', ''),
                frequency_column: count,
                'ORCID ID': details.get('orcid', ''),
                'Affiliation': details.get('affiliation', ''),
                'Country': details.get('country', '')
            }
            data.append(row)

        return data

    def _prepare_author_summary(self) -> List[Dict]:
        data = []

        for key, stats in self.author_stats.items():
            if stats['total_count'] == 0:
                continue

            # Calculate total count as sum of normalized values (as requested)
            total_count = stats['total_count']

            # Correct country
            corrected_country = self._correct_country_for_author(key, self.affiliation_stats)

            # Determine risk level based on normalized values
            risk_level = "NORMAL"
            risk_description = "Minimal author overlap between article types. Ethically acceptable."

            if stats['normalized_reference'] > 0.21:
                risk_level = "HIGH"
                risk_description = "Potential high self-citing for reference works"
            elif stats['normalized_citing'] > 0.5:
                risk_level = "HIGH"
                risk_description = "Potential high self-citing for citing works"
            elif total_count > 0.3:
                risk_level = "HIGH"
                risk_description = "HIGH RISK OF SELF-CITATION/CROWDING: author is present in analyzed articles and actively cites them or is cited in them. Thorough review recommended."
            elif total_count > 0.15:
                risk_level = "MEDIUM"
                risk_description = "MEDIUM LEVEL: moderate author presence in different article types. Possible normal academic interaction."
            elif total_count > 0.05:
                risk_level = "LOW"
                risk_description = "LOW LEVEL: small author presence in various article types. Likely normal academic practice."

            row = {
                'Surname + Initial_normalized': stats['normalized_name'],
                'ORCID ID': stats['orcid'],
                'Affiliation': stats['affiliation'],
                'Country': corrected_country,
                'Total Count': round(total_count, 4),
                'Normalized Analyzed': round(stats['normalized_analyzed'], 4),
                'Normalized Reference': round(stats['normalized_reference'], 4),
                'Normalized Citing': round(stats['normalized_citing'], 4),
                'Risk_Level': risk_level,
                'Risk_Description': risk_description
            }
            data.append(row)

        data.sort(key=lambda x: {'HIGH': 3, 'MEDIUM': 2, 'LOW': 1, 'NORMAL': 0}.get(x['Risk_Level'], 0), reverse=True)

        return data

    def _prepare_affiliation_summary(self) -> List[Dict]:
        data = []

        for affiliation, stats in self.affiliation_stats.items():
            if stats['total_count'] == 0:
                continue

            # Determine main country for affiliation
            main_country = ""
            if stats['countries']:
                country_counter = Counter(stats['countries'])
                main_country = country_counter.most_common(1)[0][0]

            row = {
                'Affiliation': affiliation,
                'Colab ID': stats['colab_id'],
                'Web Site': stats['website'],
                'Main Country': main_country,
                'total count': round(stats['total_count'], 4),
                'Normalized analyzed': round(stats['normalized_analyzed'], 4),
                'Normalized reference': round(stats['normalized_reference'], 4),
                'Normalized citing': round(stats['normalized_citing'], 4)
            }
            data.append(row)

        data.sort(key=lambda x: x['total count'], reverse=True)

        return data

    def _prepare_time_ref_analyzed_connections(self) -> List[Dict]:
        data = []

        for ref_doi, analyzed_dois in self.ref_to_analyzed.items():
            ref_result = self.ref_results.get(ref_doi, {})
            if ref_result.get('status') != 'success':
                continue

            ref_pub_info = ref_result.get('publication_info', {})
            ref_date_str = ref_pub_info.get('publication_date', '')

            ref_date = self._parse_date_string(ref_date_str)

            for analyzed_doi in analyzed_dois:
                analyzed_result = self.analyzed_results.get(analyzed_doi, {})
                if analyzed_result.get('status') != 'success':
                    continue

                analyzed_pub_info = analyzed_result.get('publication_info', {})
                analyzed_date_str = analyzed_pub_info.get('publication_date', '')

                analyzed_date = self._parse_date_string(analyzed_date_str)

                difference_days = self._calculate_date_difference(analyzed_date, ref_date)

                row = {
                    'Ref DOI': ref_doi,
                    'Analyzed DOI': analyzed_doi,
                    'publication date Ref': ref_date_str,
                    'publication date analyzed': analyzed_date_str,
                    'difference (days)': difference_days if difference_days is not None else ''
                }
                data.append(row)

        data_with_diff = [row for row in data if row['difference (days)'] not in ['', None]]
        data_without_diff = [row for row in data if row['difference (days)'] in ['', None]]

        data_with_diff.sort(key=lambda x: x['difference (days)'])

        return data_with_diff + data_without_diff

    def _prepare_time_analyzed_citing_connections(self) -> List[Dict]:
        data = []

        for analyzed_doi, citing_dois in self.analyzed_to_citing.items():
            analyzed_result = self.analyzed_results.get(analyzed_doi, {})
            if analyzed_result.get('status') != 'success':
                continue

            analyzed_pub_info = analyzed_result.get('publication_info', {})
            analyzed_date_str = analyzed_pub_info.get('publication_date', '')

            analyzed_date = self._parse_date_string(analyzed_date_str)

            for citing_doi in citing_dois:
                citing_result = self.citing_results.get(citing_doi, {})
                if citing_result.get('status') != 'success':
                    continue

                citing_pub_info = citing_result.get('publication_info', {})
                citing_date_str = citing_pub_info.get('publication_date', '')

                citing_date = self._parse_date_string(citing_date_str)

                difference_days = self._calculate_date_difference(citing_date, analyzed_date)

                row = {
                    'Analyzed DOI': analyzed_doi,
                    'Citing DOI': citing_doi,
                    'publication date analyzed': analyzed_date_str,
                    'publication date citing': citing_date_str,
                    'difference (days)': difference_days if difference_days is not None else ''
                }
                data.append(row)

        data_with_diff = [row for row in data if row['difference (days)'] not in ['', None]]
        data_without_diff = [row for row in data if row['difference (days)'] in ['', None]]

        data_with_diff.sort(key=lambda x: x['difference (days)'])

        return data_with_diff + data_without_diff

    def _parse_date_string(self, date_str: str) -> Optional[datetime]:
        if not date_str:
            return None

        date_str = date_str.strip()

        try:
            if re.match(r'^\d{4}-\d{1,2}-\d{1,2}$', date_str):
                parts = date_str.split('-')
                year = int(parts[0])
                month = int(parts[1])
                day = int(parts[2])
                return datetime(year, month, day)

            elif re.match(r'^\d{4}-\d{1,2}$', date_str):
                parts = date_str.split('-')
                year = int(parts[0])
                month = int(parts[1])
                return datetime(year, month, 15)

            elif re.match(r'^\d{4}$', date_str):
                year = int(date_str)
                return datetime(year, 7, 1)

            elif re.match(r'^\d{4}/\d{1,2}/\d{1,2}$', date_str):
                parts = date_str.split('/')
                year = int(parts[0])
                month = int(parts[1])
                day = int(parts[2])
                return datetime(year, month, day)

            elif re.match(r'^\d{4}/\d{1,2}$', date_str):
                parts = date_str.split('/')
                year = int(parts[0])
                month = int(parts[1])
                return datetime(year, month, 15)

            elif re.match(r'^\d{1,2}\.\d{1,2}\.\d{4}$', date_str):
                parts = date_str.split('.')
                day = int(parts[0])
                month = int(parts[1])
                year = int(parts[2])
                return datetime(year, month, day)

            elif re.match(r'^\d{1,2}/\d{1,2}/\d{4}$', date_str):
                parts = date_str.split('/')
                month = int(parts[0])
                day = int(parts[1])
                year = int(parts[2])
                return datetime(year, month, day)

            elif re.match(r'^\d{4}\.\d{1,2}\.\d{1,2}$', date_str):
                parts = date_str.split('.')
                year = int(parts[0])
                month = int(parts[1])
                day = int(parts[2])
                return datetime(year, month, day)

        except (ValueError, IndexError):
            pass

        year_match = re.search(r'\b(19\d{2}|20\d{2})\b', date_str)
        if year_match:
            try:
                year = int(year_match.group(1))
                return datetime(year, 7, 1)
            except:
                pass

        return None

    def _calculate_date_difference(self, date1: Optional[datetime], date2: Optional[datetime]) -> Optional[int]:
        if not date1 or not date2:
            return None

        try:
            difference = (date1 - date2).days

            if abs(difference) > 10000:
                if date1.year == date2.year:
                    return (date1 - datetime(date1.year, 1, 1)).days - (date2 - datetime(date2.year, 1, 1)).days

            return difference
        except:
            return None

    def _prepare_journal_frequency(self, results: Dict[str, Dict], source_type: str) -> List[Dict]:
        journal_counter = Counter()

        for doi, result in results.items():
            if result.get('status') != 'success':
                continue

            journal = result['publication_info'].get('journal', '')
            if journal:
                journal_counter[journal] += 1

        sorted_journals = sorted(journal_counter.items(), key=lambda x: x[1], reverse=True)

        return [{'Full Journal Name': journal, 'Count': count}
                for journal, count in sorted_journals]

    def _prepare_affiliation_frequency(self, results: Dict[str, Dict], source_type: str) -> List[Dict]:
        affiliation_counter = Counter()

        for doi, result in results.items():
            if result.get('status') != 'success':
                continue

            unique_affiliations_in_article = set()
            for author in result.get('authors', []):
                for affiliation in author.get('affiliation', []):
                    if affiliation and affiliation.strip():
                        clean_aff = affiliation.strip()
                        unique_affiliations_in_article.add(clean_aff)

            for aff in unique_affiliations_in_article:
                affiliation_counter[aff] += 1

        unique_affiliations = list(set(affiliation_counter.keys()))

        affiliation_data = []

        for aff in unique_affiliations:
            row = {
                'Affiliation': aff,
                'Count': affiliation_counter[aff]
            }
            affiliation_data.append(row)

        affiliation_data.sort(key=lambda x: x['Count'], reverse=True)
        return affiliation_data

    def _prepare_country_frequency(self, results: Dict[str, Dict], source_type: str) -> List[Dict]:
        country_single_counter = Counter()
        country_combined_counter = Counter()

        for doi, result in results.items():
            if result.get('status') != 'success':
                continue

            countries = result.get('countries', [])
            if not countries:
                continue

            for country in countries:
                if country:
                    country_single_counter[country] += 1

            if len(countries) > 1:
                sorted_countries = sorted(countries)
                combination = ';'.join(sorted_countries)
                country_combined_counter[combination] += 1

        data = []

        for country, count in sorted(country_single_counter.items(), key=lambda x: x[1], reverse=True):
            data.append({
                'Country': country,
                'Type': 'single',
                'Count': count
            })

        for combination, count in sorted(country_combined_counter.items(), key=lambda x: x[1], reverse=True):
            data.append({
                'Country': combination,
                'Type': 'combined',
                'Count': count
            })

        return data

    def _prepare_analysis_stats(self, analyzed_results: Dict[str, Dict],
                               ref_results: Dict[str, Dict],
                               citing_results: Dict[str, Dict]) -> List[Dict]:
        stats = []

        analyzed_success = sum(1 for r in analyzed_results.values() if r.get('status') == 'success')
        analyzed_failed = len(analyzed_results) - analyzed_success

        stats.append({
            'Category': 'Analyzed Articles',
            'Total DOI': len(analyzed_results),
            'Successful': analyzed_success,
            'Failed': analyzed_failed,
            'Success Rate': f"{(analyzed_success/len(analyzed_results)*100):.1f}%" if analyzed_results else "0%"
        })

        if ref_results:
            ref_success = sum(1 for r in ref_results.values() if r.get('status') == 'success')
            ref_failed = len(ref_results) - ref_success

            stats.append({
                'Category': 'Reference Articles',
                'Total DOI': len(ref_results),
                'Successful': ref_success,
                'Failed': ref_failed,
                'Success Rate': f"{(ref_success/len(ref_results)*100):.1f}%" if ref_results else "0%"
            })

        if citing_results:
            cite_success = sum(1 for r in citing_results.values() if r.get('status') == 'success')
            cite_failed = len(citing_results) - cite_success

            stats.append({
                'Category': 'Citing Articles',
                'Total DOI': len(citing_results),
                'Successful': cite_success,
                'Failed': cite_failed,
                'Success Rate': f"{(cite_success/len(citing_results)*100):.1f}%" if citing_results else "0%"
            })

        total_dois = len(analyzed_results) + len(ref_results or {}) + len(citing_results or {})
        total_success = analyzed_success + (ref_success if ref_results else 0) + (cite_success if citing_results else 0)

        stats.append({
            'Category': 'TOTAL',
            'Total DOI': total_dois,
            'Successful': total_success,
            'Failed': total_dois - total_success,
            'Success Rate': f"{(total_success/total_dois*100):.1f}%" if total_dois > 0 else "0%"
        })

        return stats

    def update_counters(self, references: List[str], citations: List[str], source_type: str = "analyzed"):
        if source_type == "analyzed":
            counter_ref = self.references_counter
            counter_cite = self.citations_counter
        elif source_type == "ref":
            counter_ref = self.ref_references_counter
            counter_cite = self.ref_citations_counter
        elif source_type == "citing":
            counter_ref = self.cite_references_counter
            counter_cite = self.cite_citations_counter
        else:
            counter_ref = self.references_counter
            counter_cite = self.citations_counter

        for ref in references:
            if ref:
                counter_ref[ref] += 1

        for cite in citations:
            if cite:
                counter_cite[cite] += 1

# ============================================================================
# üöÄ –ì–õ–ê–í–ù–´–ô –ö–õ–ê–°–° –°–ò–°–¢–ï–ú–´ (–ê–î–ê–ü–¢–ò–†–û–í–ê–ù–ù–´–ô –î–õ–Ø STREAMLIT)
# ============================================================================

class ArticleAnalyzerSystem:
    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å–µ—Å—Å–∏–∏
        if 'cache_manager' not in st.session_state:
            st.session_state.cache_manager = SmartCacheManager()
        if 'delay_manager' not in st.session_state:
            st.session_state.delay_manager = AdaptiveDelayManager()
        if 'failed_tracker' not in st.session_state:
            st.session_state.failed_tracker = FailedDOITracker()

        self.cache_manager = st.session_state.cache_manager
        self.delay_manager = st.session_state.delay_manager
        self.failed_tracker = st.session_state.failed_tracker

        self.crossref_client = CrossrefClient(self.cache_manager, self.delay_manager)
        self.openalex_client = OpenAlexClient(self.cache_manager, self.delay_manager)
        self.ror_client = RORClient(self.cache_manager)

        self.data_processor = DataProcessor(self.cache_manager)
        self.doi_processor = OptimizedDOIProcessor(
            self.cache_manager, self.delay_manager,
            self.data_processor, self.failed_tracker
        )
        self.hierarchical_analyzer = HierarchicalDataAnalyzer(
            self.cache_manager, self.data_processor, self.doi_processor
        )
        self.terminology_analyzer = TerminologyAnalyzer(
            self.cache_manager, self.data_processor
        )
        self.excel_exporter = ExcelExporter(self.data_processor, self.ror_client, self.failed_tracker)
        self.excel_exporter.set_hierarchical_analyzer(self.hierarchical_analyzer)
        self.excel_exporter.set_terminology_analyzer(self.terminology_analyzer)
        
        # –°–≤—è–∑—ã–≤–∞–µ–º —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å DOI –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º
        self.doi_processor.set_terminology_analyzer(self.terminology_analyzer)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å–µ—Å—Å–∏–∏
        if 'analyzed_results' not in st.session_state:
            st.session_state.analyzed_results = {}
        if 'ref_results' not in st.session_state:
            st.session_state.ref_results = {}
        if 'citing_results' not in st.session_state:
            st.session_state.citing_results = {}
        if 'processing_complete' not in st.session_state:
            st.session_state.processing_complete = False

        self.system_stats = {
            'total_dois_processed': 0,
            'total_successful': 0,
            'total_failed': 0,
            'total_authors': 0,
            'total_requests': 0,
            'total_ref_dois': 0,
            'total_cite_dois': 0
        }

    def _parse_dois(self, input_text: str) -> List[str]:
        if not input_text:
            return []

        separators = [',', ';', '\n', '\t', '|']

        for sep in separators:
            if sep in input_text:
                parts = input_text.split(sep)
                break
        else:
            parts = input_text.split()

        dois = []
        for part in parts:
            doi = self._clean_doi(part)
            if doi and len(doi) > 5:
                dois.append(doi)

        return list(set(dois))

    def _clean_doi(self, doi: str) -> str:
        if not doi or not isinstance(doi, str):
            return ""

        doi = doi.strip()
        prefixes = ['doi:', 'DOI:', 'https://doi.org/', 'http://doi.org/',
                   'https://dx.doi.org/', 'http://dx.doi.org/']

        for prefix in prefixes:
            if doi.lower().startswith(prefix.lower()):
                doi = doi[len(prefix):]

        return doi.strip()

    def process_dois(self, dois: List[str], num_workers: int = Config.DEFAULT_WORKERS,
                    analysis_types: Dict[str, bool] = None, progress_container=None):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ DOI"""
        
        start_time = time.time()

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö DOI
        if progress_container:
            progress_container.text("üìö –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö DOI...")
            analyzed_progress = progress_container.progress(0)
        else:
            analyzed_progress = None

        st.session_state.analyzed_results = self.doi_processor.process_doi_batch(
            dois, "analyzed", None, True, True, Config.BATCH_SIZE, progress_container
        )

        if analyzed_progress:
            analyzed_progress.progress(1.0)

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–æ–≤
        for doi, result in st.session_state.analyzed_results.items():
            if result.get('status') == 'success':
                self.excel_exporter.update_counters(
                    result.get('references', []),
                    result.get('citations', []),
                    "analyzed"
                )

        # –°–±–æ—Ä –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ reference DOI
        if progress_container:
            progress_container.text("üìé –°–±–æ—Ä reference DOI...")

        all_ref_dois = self.doi_processor.collect_all_references(st.session_state.analyzed_results)
        self.system_stats['total_ref_dois'] = len(all_ref_dois)

        if all_ref_dois:
            if progress_container:
                progress_container.text(f"üìé –ù–∞–π–¥–µ–Ω–æ {len(all_ref_dois)} reference DOI –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                ref_progress = progress_container.progress(0)
            else:
                ref_progress = None

            ref_dois_to_analyze = all_ref_dois[:10000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

            st.session_state.ref_results = self.doi_processor.process_doi_batch(
                ref_dois_to_analyze, "ref", None, True, True, Config.BATCH_SIZE, progress_container
            )

            if ref_progress:
                ref_progress.progress(1.0)

            for doi, result in st.session_state.ref_results.items():
                if result.get('status') == 'success':
                    self.excel_exporter.update_counters(
                        result.get('references', []),
                        result.get('citations', []),
                        "ref"
                    )

        # –°–±–æ—Ä –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ citation DOI
        if progress_container:
            progress_container.text("üîó –°–±–æ—Ä citation DOI...")

        all_cite_dois = self.doi_processor.collect_all_citations(st.session_state.analyzed_results)
        self.system_stats['total_cite_dois'] = len(all_cite_dois)

        if all_cite_dois:
            if progress_container:
                progress_container.text(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(all_cite_dois)} citation DOI –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                cite_progress = progress_container.progress(0)
            else:
                cite_progress = None

            cite_dois_to_analyze = all_cite_dois[:10000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

            st.session_state.citing_results = self.doi_processor.process_doi_batch(
                cite_dois_to_analyze, "citing", None, True, True, Config.BATCH_SIZE, progress_container
            )

            if cite_progress:
                cite_progress.progress(1.0)

            for doi, result in st.session_state.citing_results.items():
                if result.get('status') == 'success':
                    self.excel_exporter.update_counters(
                        result.get('references', []),
                        result.get('citations', []),
                        "citing"
                    )

        # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—É–¥–∞—á–Ω—ã—Ö DOI
        failed_stats = self.failed_tracker.get_stats()
        if failed_stats['total_failed'] > 0:
            if progress_container:
                progress_container.text("üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—É–¥–∞—á–Ω—ã—Ö DOI...")
            retry_results = self.doi_processor.retry_failed_dois(self.failed_tracker)

            for doi, result in retry_results.items():
                if result.get('status') == 'success':
                    source_type = self.failed_tracker.sources.get(doi, 'retry')
                    if source_type == 'analyzed' and doi in self.failed_tracker.failed_dois:
                        st.session_state.analyzed_results[doi] = result
                    elif source_type == 'ref' and doi in self.failed_tracker.failed_dois:
                        st.session_state.ref_results[doi] = result
                    elif source_type == 'citing' and doi in self.failed_tracker.failed_dois:
                        st.session_state.citing_results[doi] = result

        processing_time = time.time() - start_time

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.system_stats['total_dois_processed'] += len(dois)
        successful = sum(1 for r in st.session_state.analyzed_results.values() if r.get('status') == 'success')
        failed = len(dois) - successful

        st.session_state.processing_complete = True
        st.rerun()

        return {
            'processing_time': processing_time,
            'successful': successful,
            'failed': failed,
            'total_refs': self.system_stats['total_ref_dois'],
            'total_cites': self.system_stats['total_cite_dois']
        }

    def create_excel_report(self, analysis_types: Dict[str, bool] = None, progress_container=None):
        """–°–æ–∑–¥–∞–µ—Ç Excel –æ—Ç—á–µ—Ç"""
        if analysis_types is None:
            analysis_types = {
                'quick_checks': True,
                'medium_insights': True,
                'deep_analysis': False,
                'analyzed_citing_relationships': False,
                'emerging_terms': True,
                'convergence_zones': True,
                'frontier_predictions': True
            }

        # –û–±–Ω–æ–≤–ª—è–µ–º —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä –¥–∞–Ω–Ω—ã–º–∏
        self.excel_exporter.analyzed_results = st.session_state.analyzed_results
        self.excel_exporter.ref_results = st.session_state.ref_results
        self.excel_exporter.citing_results = st.session_state.citing_results

        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç –≤ –ø–∞–º—è—Ç–∏
        excel_file = self.excel_exporter.create_comprehensive_report(
            st.session_state.analyzed_results,
            st.session_state.ref_results,
            st.session_state.citing_results,
            analysis_types,
            progress_container=progress_container
        )

        return excel_file

    def clear_data(self):
        """–û—á–∏—â–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ"""
        st.session_state.analyzed_results = {}
        st.session_state.ref_results = {}
        st.session_state.citing_results = {}
        st.session_state.processing_complete = False
        self.failed_tracker.clear()

# ============================================================================
# üéõÔ∏è –ò–ù–¢–ï–†–§–ï–ô–° STREAMLIT
# ============================================================================

def main():
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    st.title("üìö –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ DOI")
    st.markdown("""
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ DOI —Å —É–º–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –∞–Ω–∞–ª–∏–∑–æ–º —Å—Å—ã–ª–æ–∫ –∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π,
    –∞ —Ç–∞–∫–∂–µ –≤—ã—è–≤–ª–µ–Ω–∏–µ–º –Ω–µ—ç—Ç–∏—á–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É—á–Ω—ã—Ö —Ñ—Ä–æ–Ω—Ç–∏—Ä–æ–≤.
    """)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
    if 'system' not in st.session_state:
        st.session_state.system = ArticleAnalyzerSystem()

    system = st.session_state.system

    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫
    with st.sidebar:
        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏
        num_workers = st.slider(
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤",
            min_value=Config.MIN_WORKERS,
            max_value=Config.MAX_WORKERS,
            value=Config.DEFAULT_WORKERS,
            help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ DOI"
        )
        
        st.markdown("---")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—ç—Ç–∏—á–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫
        st.subheader("üîç –ê–Ω–∞–ª–∏–∑ –Ω–µ—ç—Ç–∏—á–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫")
        
        quick_checks = st.checkbox(
            "Quick Checks (5-10 —Å–µ–∫ –Ω–∞ —Å—Ç–∞—Ç—å—é)",
            value=True,
            help="–ë—ã—Å—Ç—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫—Ä–∞—Å–Ω—ã—Ö —Ñ–ª–∞–≥–æ–≤"
        )
        
        medium_insights = st.checkbox(
            "Medium Insights (15-30 —Å–µ–∫ –Ω–∞ —Å—Ç–∞—Ç—å—é)",
            value=True,
            help="–°—Ä–µ–¥–Ω–∏–π –∞–Ω–∞–ª–∏–∑ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ –∏ —Å–µ—Ç–µ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"
        )
        
        deep_analysis = st.checkbox(
            "Deep Analysis (60-120 —Å–µ–∫ –Ω–∞ —Å—Ç–∞—Ç—å—é)",
            value=False,
            help="–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å ML-–æ—Ü–µ–Ω–∫–æ–π —Ä–∏—Å–∫–æ–≤"
        )
        
        citing_relationships = st.checkbox(
            "Analyzed-Citing Relationships (30-60 —Å–µ–∫ –Ω–∞ –ø–∞—Ä—É)",
            value=False,
            help="–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–º–∏ –∏ —Ü–∏—Ç–∏—Ä—É—é—â–∏–º–∏ —Å—Ç–∞—Ç—å—è–º–∏"
        )
        
        st.markdown("---")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ñ—Ä–æ–Ω—Ç–∏—Ä–æ–≤
        st.subheader("üß† –ê–Ω–∞–ª–∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Ñ—Ä–æ–Ω—Ç–∏—Ä–æ–≤")
        
        emerging_terms = st.checkbox(
            "Emerging Terms",
            value=True,
            help="–í—ã—è–≤–ª–µ–Ω–∏–µ –ø–æ—è–≤–ª—è—é—â–∏—Ö—Å—è —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º —Ä–æ—Å—Ç–æ–º"
        )
        
        convergence_zones = st.checkbox(
            "Convergence Zones",
            value=True,
            help="–ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤, —Å–≤—è–∑—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏"
        )
        
        frontier_predictions = st.checkbox(
            "Frontier Predictions",
            value=True,
            help="–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö —Ñ—Ä–æ–Ω—Ç–∏—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏"
        )
        
        st.markdown("---")
        
        # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–µ–º
        st.subheader("üóÇÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–µ–º")
        
        if st.button("–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à", type="secondary"):
            system.cache_manager.clear_all()
            st.success("–ö—ç—à –æ—á–∏—â–µ–Ω!")
        
        # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞
        cache_stats = system.cache_manager.get_stats()
        with st.expander("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"):
            st.write(f"–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {cache_stats['hit_ratio']}%")
            st.write(f"API –≤—ã–∑–æ–≤–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {cache_stats['api_calls_saved']}")
            st.write(f"–†–∞–∑–º–µ—Ä –∫—ç—à–∞: {cache_stats['cache_size_mb']} MB")

    # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å –≤–≤–æ–¥–∞
    st.header("üìù –í–≤–æ–¥ DOI")
    
    doi_input = st.text_area(
        "–í–≤–µ–¥–∏—Ç–µ –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ DOI",
        height=150,
        placeholder="–í–≤–µ–¥–∏—Ç–µ DOI —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π –∏–ª–∏ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏.\n\n–ü—Ä–∏–º–µ—Ä—ã:\n10.1038/nature12373\n10.1126/science.1252914, 10.1016/j.cell.2019.11.017",
        help="–ú–æ–∂–Ω–æ –≤–≤–æ–¥–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ DOI, —Ä–∞–∑–¥–µ–ª—è—è –∏—Ö –∑–∞–ø—è—Ç—ã–º–∏, —Ç–æ—á–∫–∞–º–∏ —Å –∑–∞–ø—è—Ç–æ–π –∏–ª–∏ –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫"
    )
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        process_btn = st.button("üìä –û–±—Ä–∞–±–æ—Ç–∞—Ç—å DOI", type="primary", use_container_width=True)
    
    with col2:
        clear_btn = st.button("üßπ –û—á–∏—Å—Ç–∏—Ç—å –¥–∞–Ω–Ω—ã–µ", type="secondary", use_container_width=True)
    
    with col3:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —É—Å–ª–æ–≤–∏–π –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∫–Ω–æ–ø–∫–∏
        export_disabled = not (
            hasattr(st.session_state, 'processing_complete') and 
            st.session_state.processing_complete and
            hasattr(st.session_state, 'analyzed_results') and 
            st.session_state.analyzed_results
        )
        
        export_btn = st.button("üíæ –≠–∫—Å–ø–æ—Ä—Ç Excel", 
                             type="secondary", 
                             use_container_width=True,
                             disabled=export_disabled)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–∂–∞—Ç–∏–π –∫–Ω–æ–ø–æ–∫
    if process_btn and doi_input:
        dois = system._parse_dois(doi_input)
        
        if not dois:
            st.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö DOI. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç –≤–≤–æ–¥–∞.")
        else:
            st.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(dois)} –≤–∞–ª–∏–¥–Ω—ã—Ö DOI –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            
            # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            progress_container = st.container()
            
            with progress_container:
                st.write("üöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É...")
                
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # –°–æ–±–∏—Ä–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞
                analysis_types = {
                    'quick_checks': quick_checks,
                    'medium_insights': medium_insights,
                    'deep_analysis': deep_analysis,
                    'analyzed_citing_relationships': citing_relationships,
                    'emerging_terms': emerging_terms,
                    'convergence_zones': convergence_zones,
                    'frontier_predictions': frontier_predictions
                }
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
                try:
                    results = system.process_dois(
                        dois, 
                        num_workers, 
                        analysis_types,
                        progress_container
                    )
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                    progress_bar.progress(100)
                    status_text.success("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    st.success(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {results['processing_time']:.1f} —Å–µ–∫—É–Ω–¥")
                    
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("–£—Å–ø–µ—à–Ω–æ", results['successful'])
                    with col2:
                        st.metric("–û—à–∏–±–æ–∫", results['failed'])
                    with col3:
                        st.metric("Reference DOI", results['total_refs'])
                    with col4:
                        st.metric("Citation DOI", results['total_cites'])
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –Ω–µ—É–¥–∞—á–Ω—ã–º DOI
                    failed_stats = system.failed_tracker.get_stats()
                    if failed_stats['total_failed'] > 0:
                        with st.expander(f"‚ùå –ù–µ—É–¥–∞—á–Ω—ã–µ DOI ({failed_stats['total_failed']})"):
                            st.write(f"‚Ä¢ –ò–∑ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã—Ö: {failed_stats['analyzed_failed']}")
                            st.write(f"‚Ä¢ –ò–∑ —Å—Å—ã–ª–æ–∫: {failed_stats['ref_failed']}")
                            st.write(f"‚Ä¢ –ò–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π: {failed_stats['citing_failed']}")
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
                    with st.expander("üìä –ü—Ä–∏–º–µ—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"):
                        successful_count = 0
                        for doi, result in st.session_state.analyzed_results.items():
                            if result.get('status') == 'success' and successful_count < 5:
                                pub_info = result['publication_info']
                                st.write(f"**{pub_info.get('title', '')[:80]}...**")
                                st.write(f"DOI: {doi}")
                                st.write(f"–ñ—É—Ä–Ω–∞–ª: {pub_info.get('journal', '')}")
                                st.write(f"–ì–æ–¥: {pub_info.get('year', '')}")
                                st.write("---")
                                successful_count += 1
                
                except Exception as e:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}")
    
    elif process_btn and not doi_input:
        st.warning("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ DOI –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    if clear_btn:
        system.clear_data()
        st.success("‚úÖ –î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã")
        st.rerun()
    
    if export_btn and st.session_state.processing_complete:
        with st.spinner("üìä –°–æ–∑–¥–∞–Ω–∏–µ Excel –æ—Ç—á–µ—Ç–∞..."):
            try:
                # –°–æ–±–∏—Ä–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞
                analysis_types = {
                    'quick_checks': quick_checks,
                    'medium_insights': medium_insights,
                    'deep_analysis': deep_analysis,
                    'analyzed_citing_relationships': citing_relationships,
                    'emerging_terms': emerging_terms,
                    'convergence_zones': convergence_zones,
                    'frontier_predictions': frontier_predictions
                }
                
                # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
                excel_file = system.create_excel_report(analysis_types)
                
                # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"articles_analysis_{timestamp}.xlsx"
                
                # –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º —Ñ–∞–π–ª –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                st.download_button(
                    label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å Excel —Ñ–∞–π–ª",
                    data=excel_file,
                    file_name=filename,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
                
                st.success("‚úÖ Excel –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é")
                
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {str(e)}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É, –µ—Å–ª–∏ –µ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    if st.session_state.processing_complete:
        st.header("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        successful = sum(1 for r in st.session_state.analyzed_results.values() if r.get('status') == 'success')
        total = len(st.session_state.analyzed_results)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–µ —Å—Ç–∞—Ç—å–∏",
                f"{successful}/{total}",
                f"{successful/total*100:.1f}%" if total > 0 else "0%"
            )
        
        with col2:
            ref_success = sum(1 for r in st.session_state.ref_results.values() if r.get('status') == 'success')
            ref_total = len(st.session_state.ref_results)
            st.metric(
                "Reference —Å—Ç–∞—Ç—å–∏",
                f"{ref_success}/{ref_total}" if ref_total > 0 else "0",
                f"{ref_success/ref_total*100:.1f}%" if ref_total > 0 else "0%"
            )
        
        with col3:
            cite_success = sum(1 for r in st.session_state.citing_results.values() if r.get('status') == 'success')
            cite_total = len(st.session_state.citing_results)
            st.metric(
                "Citing —Å—Ç–∞—Ç—å–∏",
                f"{cite_success}/{cite_total}" if cite_total > 0 else "0",
                f"{cite_success/cite_total*100:.1f}%" if cite_total > 0 else "0%"
            )
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        with st.expander("üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"):
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∞–≤—Ç–æ—Ä–∞–º
            total_authors = 0
            for result in st.session_state.analyzed_results.values():
                if result.get('status') == 'success':
                    total_authors += len(result.get('authors', []))
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Å—ã–ª–∫–∞–º –∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è–º
            total_refs = 0
            total_cites = 0
            for result in st.session_state.analyzed_results.values():
                if result.get('status') == 'success':
                    total_refs += len(result.get('references', []))
                    total_cites += len(result.get('citations', []))
            
            st.write(f"**–í—Å–µ–≥–æ –∞–≤—Ç–æ—Ä–æ–≤:** {total_authors}")
            st.write(f"**–í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ (references):** {total_refs}")
            st.write(f"**–í—Å–µ–≥–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π (citations):** {total_cites}")
            st.write(f"**–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö reference DOI:** {len(system.excel_exporter.references_counter)}")
            st.write(f"**–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö citation DOI:** {len(system.excel_exporter.citations_counter)}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞
            cache_stats = system.cache_manager.get_stats()
            st.write(f"**–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫—ç—à–∞:** {cache_stats['hit_ratio']}%")
            st.write(f"**API –≤—ã–∑–æ–≤–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ:** {cache_stats['api_calls_saved']}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏
            if system.terminology_analyzer:
                term_stats = system.terminology_analyzer.get_term_statistics()
                st.write(f"**–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤:** {term_stats.get('total_terms', 0)}")
                st.write(f"**–°—Ç–∞—Ç–µ–π —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏:** {term_stats.get('total_articles_with_terms', 0)}")
                st.write(f"**–°—Ä–µ–¥–Ω–µ–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ –Ω–∞ —Å—Ç–∞—Ç—å—é:** {term_stats.get('average_terms_per_article', 0):.2f}")

# ============================================================================
# üèÉ‚Äç‚ôÇÔ∏è –ó–ê–ü–£–°–ö –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø
# ============================================================================

if __name__ == "__main__":
    main()



