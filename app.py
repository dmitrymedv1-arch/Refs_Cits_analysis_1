# -*- coding: utf-8 -*-
"""üìö –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ DOI —Å —É–º–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —ç–∫—Å–ø–æ—Ä—Ç–æ–º –≤ Excel - Streamlit –≤–µ—Ä—Å–∏—è"""

# ============================================================================
# üì¶ –ò–ú–ü–û–†–¢–´ –ò –ù–ê–°–¢–†–û–ô–ö–ê
# ============================================================================

import streamlit as st
import requests
import json
import re
import time
import pickle
import hashlib
import os
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Set, Union
from collections import defaultdict, Counter, OrderedDict
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
import math
import networkx as nx
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import base64
import tempfile

# ============================================================================
# ‚öôÔ∏è –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================

class Config:
    CROSSREF_URL = "https://api.crossref.org/works/"
    OPENALEX_URL = "https://api.openalex.org/works/https://doi.org/"
    OPENALEX_WORKS_URL = "https://api.openalex.org/works"
    ORCID_API_URL = "https://pub.orcid.org/v3.0/search/"
    ROR_API_URL = "https://api.ror.org/organizations"
    
    REQUEST_TIMEOUT = 10
    MAX_RETRIES = 2
    MAX_DELAY = 1.0
    MIN_DELAY = 0.1
    INITIAL_DELAY = 0.2
    
    # Streamlit-specific cache directory
    CACHE_DIR = os.path.join(tempfile.gettempdir(), "article_analyzer_cache")
    TTL_HOURS = 24
    MAX_CACHE_SIZE_MB = 50
    
    MIN_WORKERS = 1
    MAX_WORKERS = 10
    DEFAULT_WORKERS = 4
    
    BATCH_SIZE = 50
    
    TOP_PERCENTILE_FOR_DEEP_ANALYSIS = 10
    MIN_CITATIONS_FOR_DEEP_ANALYSIS = 10
    
    # –ü–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—ç—Ç–∏—á–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫
    QUICK_CHECK_THRESHOLDS = {
        'journal_concentration': 0.7,  # >70% –∏–∑ –æ–¥–Ω–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞
        'author_self_citation': 0.3,   # >30% —Å –æ–±—â–∏–º–∏ –∞–≤—Ç–æ—Ä–∞–º–∏
        'affiliation_self_citation': 0.6,  # >60% –∏–∑ —Ç–æ–π –∂–µ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏
        'single_country': 0.8,         # >80% –∏–∑ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω—ã
        'citation_velocity': 20,       # >20 —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ –≥–æ–¥
        'first_year_share': 0.5        # >50% –≤ –ø–µ—Ä–≤—ã–π –≥–æ–¥
    }
    
    MEDIUM_INSIGHT_THRESHOLDS = {
        'first_two_years': 0.7,        # >70% –∑–∞ –ø–µ—Ä–≤—ã–µ 2 –≥–æ–¥–∞
        'top_journal_share': 0.6,      # >60% –∏–∑ —Ç–æ–ø-1 –∂—É—Ä–Ω–∞–ª–∞
        'cluster_coefficient': 0.8,    # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ >0.8
        'geographic_bias': 0.9         # –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π bias >0.9
    }
    
    COUNTRY_CODES = {
        'USA': 'US', 'United States': 'US', 'US': 'US',
        'United Kingdom': 'GB', 'UK': 'GB', 'Great Britain': 'GB',
        'Germany': 'DE', 'Deutschland': 'DE',
        'France': 'FR', 'France': 'FR',
        'China': 'CN', 'People\'s Republic of China': 'CN', 'PR China': 'CN',
        'Japan': 'JP', 'Japan': 'JP',
        'Canada': 'CA', 'Canada': 'CA',
        'Australia': 'AU', 'Australia': 'AU',
        'Italy': 'IT', 'Italia': 'IT',
        'Spain': 'ES', 'Espa√±a': 'ES',
        'Russia': 'RU', 'Russian Federation': 'RU', '–†–æ—Å—Å–∏—è': 'RU', 'Russian': 'RU',
        'India': 'IN', 'India': 'IN',
        'Brazil': 'BR', 'Brasil': 'BR',
        'South Korea': 'KR', 'Korea, Republic of': 'KR', 'Korea': 'KR',
        'Netherlands': 'NL', 'The Netherlands': 'NL',
        'Switzerland': 'CH', 'Switzerland': 'CH',
        'Sweden': 'SE', 'Sweden': 'SE',
        'Norway': 'NO', 'Norway': 'NO',
        'Denmark': 'DK', 'Denmark': 'DK',
        'Finland': 'FI', 'Finland': 'FI',
        'Austria': 'AT', 'Austria': 'AT',
        'Belgium': 'BE', 'Belgium': 'BE',
        'Poland': 'PL', 'Poland': 'PL',
        'Portugal': 'PT', 'Portugal': 'PT',
        'Greece': 'GR', 'Greece': 'GR',
        'Turkey': 'TR', 'T√ºrkiye': 'TR',
        'Israel': 'IL', 'Israel': 'IL',
        'Singapore': 'SG', 'Singapore': 'SG',
        'Taiwan': 'TW', 'Taiwan, Province of China': 'TW',
        'Hong Kong': 'HK', 'Hong Kong SAR': 'HK',
        'Mexico': 'MX', 'Mexico': 'MX',
        'Argentina': 'AR', 'Argentina': 'AR',
        'Chile': 'CL', 'Chile': 'CL',
        'Colombia': 'CO', 'Colombia': 'CO',
        'Ukraine': 'UA', 'Ukraine': 'UA',
        'Czech Republic': 'CZ', 'Czechia': 'CZ',
        'Hungary': 'HU', 'Hungary': 'HU',
        'Romania': 'RO', 'Romania': 'RO',
        'Bulgaria': 'BG', 'Bulgaria': 'BG',
        'Serbia': 'RS', 'Serbia': 'RS',
        'Croatia': 'HR', 'Croatia': 'HR',
        'Slovakia': 'SK', 'Slovakia': 'SK',
        'Slovenia': 'SI', 'Slovenia': 'SI',
        'Lithuania': 'LT', 'Lithuania': 'LT',
        'Latvia': 'LV', 'Latvia': 'LV',
        'Estonia': 'EE', 'Estonia': 'EE',
        'Ireland': 'IE', 'Ireland': 'IE',
        'New Zealand': 'NZ', 'New Zealand': 'NZ',
        'South Africa': 'ZA', 'South Africa': 'ZA',
        'Egypt': 'EG', 'Egypt': 'EG',
        'Saudi Arabia': 'SA', 'Saudi Arabia': 'SA',
        'United Arab Emirates': 'AE', 'UAE': 'AE',
        'Qatar': 'QA', 'Qatar': 'QA',
        'Iran': 'IR', 'Iran, Islamic Republic of': 'IR',
        'Pakistan': 'PK', 'Pakistan': 'PK',
        'Bangladesh': 'BD', 'Bangladesh': 'BD',
        'Vietnam': 'VN', 'Viet Nam': 'VN',
        'Thailand': 'TH', 'Thailand': 'TH',
        'Malaysia': 'MY', 'Malaysia': 'MY',
        'Indonesia': 'ID', 'Indonesia': 'ID',
        'Philippines': 'PH', 'Philippines': 'PH',
        'Kazakhstan': 'KZ', 'Kazakhstan': 'KZ',
        'Belarus': 'BY', 'Belarus': 'BY',
        'Uzbekistan': 'UZ', 'Uzbekistan': 'UZ',
        'Azerbaijan': 'AZ', 'Azerbaijan': 'AZ',
        'Georgia': 'GE', 'Georgia': 'GE',
        'Armenia': 'AM', 'Armenia': 'AM',
        'Moldova': 'MD', 'Moldova': 'MD',
        'Kyrgyzstan': 'KG', 'Kyrgyzstan': 'KG',
        'Tajikistan': 'TJ', 'Tajikistan': 'TJ',
        'Turkmenistan': 'TM', 'Turkmenistan': 'TM',
        'Mongolia': 'MN', 'Mongolia': 'MN',
    }

# ============================================================================
# üóÇÔ∏è –ö–õ–ê–°–° –£–ú–ù–û–ì–û –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø (–£–õ–£–ß–®–ï–ù–ù–´–ô)
# ============================================================================

class SmartCacheManager:
    def __init__(self, cache_dir: str = Config.CACHE_DIR, ttl_hours: int = Config.TTL_HOURS):
        self.cache_dir = cache_dir
        self.ttl_seconds = ttl_hours * 3600
        
        self.stats = {
            'hits': 0,
            'misses': 0,
            'expired': 0,
            'evictions': 0,
            'total_size_mb': 0,
            'memory_hits': 0,
            'file_hits': 0,
            'api_calls_saved': 0
        }
        
        self.memory_cache = OrderedDict()
        self.max_memory_items = 5000
        
        self.failed_cache = {}
        self.failed_cache_ttl = 3600
        
        self.popular_cache = {}
        
        self.ror_cache = {
            'analyzed': {},
            'ref': {},
            'citing': {},
            'summary': {}
        }
        
        self.insights_cache = {
            'geo_bubbles': {},
            'temporal_patterns': {},
            'hyper_citation': {},
            'citation_cascades': {},
            'mutual_citations': {}
        }
        
        # –ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—ç—Ç–∏—á–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫
        self.ethical_analysis_cache = {
            'quick_checks': {},
            'medium_insights': {},
            'deep_analysis': {},
            'citing_relationships': {}
        }
        
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir, exist_ok=True)
            
        self._clean_expired_cache()
        
        self._load_popular_dois()
    
    def _get_cache_key(self, source: str, identifier: str) -> str:
        key_str = f"v3:{source}:{identifier}"
        return hashlib.sha256(key_str.encode()).hexdigest()[:32]
    
    def _get_cache_path(self, key: str) -> str:
        return os.path.join(self.cache_dir, f"{key}.pkl")
    
    def _get_cache_metadata_path(self, key: str) -> str:
        return os.path.join(self.cache_dir, f"{key}_meta.json")
    
    def _calculate_cache_size(self) -> float:
        total_size = 0
        try:
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.pkl'):
                    filepath = os.path.join(self.cache_dir, filename)
                    total_size += os.path.getsize(filepath)
        except:
            pass
        return total_size / (1024 * 1024)
    
    def _clean_expired_cache(self):
        try:
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.pkl'):
                    filepath = os.path.join(self.cache_dir, filename)
                    try:
                        with open(filepath, 'rb') as f:
                            cached_data = pickle.load(f)
                        
                        if time.time() - cached_data.get('timestamp', 0) > self.ttl_seconds:
                            os.remove(filepath)
                            self.stats['expired'] += 1
                            
                            meta_file = filepath.replace('.pkl', '_meta.json')
                            if os.path.exists(meta_file):
                                os.remove(meta_file)
                                
                    except:
                        try:
                            os.remove(filepath)
                        except:
                            pass
            
            cache_size = self._calculate_cache_size()
            if cache_size > Config.MAX_CACHE_SIZE_MB:
                self._evict_old_cache_items()
                
        except Exception as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")
    
    def _evict_old_cache_items(self):
        try:
            cache_files = []
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.pkl'):
                    filepath = os.path.join(self.cache_dir, filename)
                    mtime = os.path.getmtime(filepath)
                    cache_files.append((mtime, filepath))
            
            cache_files.sort()
            
            cache_size = self._calculate_cache_size()
            while cache_files and cache_size > Config.MAX_CACHE_SIZE_MB * 0.8:
                _, old_file = cache_files.pop(0)
                
                try:
                    os.remove(old_file)
                    self.stats['evictions'] += 1
                    
                    meta_file = old_file.replace('.pkl', '_meta.json')
                    if os.path.exists(meta_file):
                        os.remove(meta_file)
                        
                except:
                    pass
                
                cache_size = self._calculate_cache_size()
                
        except Exception as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–∞—Ä—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫—ç—à–∞: {e}")
    
    def get(self, source: str, identifier: str, category: str = "default") -> Optional[Any]:
        failed_key = f"failed:{source}:{identifier}"
        if failed_key in self.failed_cache:
            failed_data = self.failed_cache[failed_key]
            if time.time() - failed_data['timestamp'] < self.failed_cache_ttl:
                return None
        
        key = self._get_cache_key(source, identifier)
        
        memory_key = f"{category}:{key}"
        if memory_key in self.memory_cache:
            data = self.memory_cache[memory_key]
            del self.memory_cache[memory_key]
            self.memory_cache[memory_key] = data
            self.stats['hits'] += 1
            self.stats['memory_hits'] += 1
            return data
        
        cache_path = self._get_cache_path(key)
        meta_path = self._get_cache_metadata_path(key)
        
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    cached_data = pickle.load(f)
                
                if os.path.exists(meta_path):
                    try:
                        with open(meta_path, 'r') as mf:
                            metadata = json.load(mf)
                        category_match = metadata.get('category') == category
                    except:
                        category_match = True
                else:
                    category_match = True
                
                if (time.time() - cached_data.get('timestamp', 0) < self.ttl_seconds and 
                    category_match):
                    
                    if len(self.memory_cache) >= self.max_memory_items:
                        self.memory_cache.popitem(last=False)
                    
                    self.memory_cache[memory_key] = cached_data['data']
                    self.stats['hits'] += 1
                    self.stats['file_hits'] += 1
                    return cached_data['data']
                else:
                    os.remove(cache_path)
                    if os.path.exists(meta_path):
                        os.remove(meta_path)
                    self.stats['expired'] += 1
                    
            except:
                try:
                    os.remove(cache_path)
                    if os.path.exists(meta_path):
                        os.remove(meta_path)
                except:
                    pass
        
        self.stats['misses'] += 1
        return None
    
    def set(self, source: str, identifier: str, data: Any, category: str = "default"):
        key = self._get_cache_key(source, identifier)
        cache_path = self._get_cache_path(key)
        meta_path = self._get_cache_metadata_path(key)
        
        cache_entry = {
            'timestamp': time.time(),
            'source': source,
            'identifier': identifier,
            'data': data,
            'category': category
        }
        
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(cache_entry, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            metadata = {
                'category': category,
                'created': datetime.now().isoformat(),
                'source': source,
                'identifier_hash': hashlib.md5(str(identifier).encode()).hexdigest()
            }
            
            with open(meta_path, 'w') as mf:
                json.dump(metadata, mf, indent=2)
            
            memory_key = f"{category}:{key}"
            if len(self.memory_cache) >= self.max_memory_items:
                self.memory_cache.popitem(last=False)
            
            self.memory_cache[memory_key] = data
            
            self.stats['api_calls_saved'] += 1
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫—ç—à: {e}")
    
    def mark_as_failed(self, source: str, identifier: str, error: str = ""):
        failed_key = f"failed:{source}:{identifier}"
        self.failed_cache[failed_key] = {
            'timestamp': time.time(),
            'error': error,
            'source': source,
            'identifier': identifier
        }
    
    def _load_popular_dois(self):
        popular_file = os.path.join(self.cache_dir, "popular_dois.json")
        if os.path.exists(popular_file):
            try:
                with open(popular_file, 'r') as f:
                    self.popular_cache = json.load(f)
                st.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.popular_cache)} –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö DOI")
            except:
                self.popular_cache = {}
    
    def _save_popular_dois(self):
        popular_file = os.path.join(self.cache_dir, "popular_dois.json")
        try:
            with open(popular_file, 'w') as f:
                json.dump(self.popular_cache, f, indent=2)
        except:
            pass
    
    def update_popularity(self, doi: str):
        if doi in self.popular_cache:
            self.popular_cache[doi] += 1
        else:
            self.popular_cache[doi] = 1
        
        if len(self.popular_cache) % 100 == 0:
            self._save_popular_dois()
    
    def get_stats(self) -> Dict[str, Any]:
        cache_size = self._calculate_cache_size()
        total_requests = self.stats['hits'] + self.stats['misses']
        hit_ratio = (self.stats['hits'] / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'hits': self.stats['hits'],
            'misses': self.stats['misses'],
            'expired': self.stats['expired'],
            'evictions': self.stats['evictions'],
            'memory_hits': self.stats['memory_hits'],
            'file_hits': self.stats['file_hits'],
            'api_calls_saved': self.stats['api_calls_saved'],
            'memory_items': len(self.memory_cache),
            'cache_size_mb': round(cache_size, 2),
            'hit_ratio': round(hit_ratio, 1),
            'failed_cache_size': len(self.failed_cache),
            'popular_dois': len(self.popular_cache)
        }
    
    def clear_all(self):
        try:
            for filename in os.listdir(self.cache_dir):
                filepath = os.path.join(self.cache_dir, filename)
                try:
                    os.remove(filepath)
                except:
                    pass
            
            self.memory_cache.clear()
            self.failed_cache.clear()
            self.popular_cache.clear()
            self.ror_cache = {'analyzed': {}, 'ref': {}, 'citing': {}, 'summary': {}}
            self.insights_cache = {
                'geo_bubbles': {}, 'temporal_patterns': {}, 'hyper_citation': {},
                'citation_cascades': {}, 'mutual_citations': {}
            }
            self.ethical_analysis_cache = {
                'quick_checks': {}, 'medium_insights': {}, 'deep_analysis': {}, 'citing_relationships': {}
            }
            self.stats = {k: 0 for k in self.stats.keys()}
            
            st.success("‚úÖ –ö—ç—à –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω")
            
        except Exception as e:
            st.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")
    
    def get_ror_cache(self, category: str, query: str) -> Optional[Dict]:
        if category in self.ror_cache and query in self.ror_cache[category]:
            return self.ror_cache[category][query]
        return None
    
    def set_ror_cache(self, category: str, query: str, data: Dict):
        if category not in self.ror_cache:
            self.ror_cache[category] = {}
        self.ror_cache[category][query] = data
    
    def clear_ror_cache(self, category: str = None):
        if category:
            if category in self.ror_cache:
                self.ror_cache[category].clear()
        else:
            for cat in self.ror_cache:
                self.ror_cache[cat].clear()
    
    def get_insight_cache(self, insight_type: str, key: str) -> Optional[Dict]:
        if insight_type in self.insights_cache and key in self.insights_cache[insight_type]:
            return self.insights_cache[insight_type][key]
        return None
    
    def set_insight_cache(self, insight_type: str, key: str, data: Dict):
        if insight_type not in self.insights_cache:
            self.insights_cache[insight_type] = {}
        self.insights_cache[insight_type][key] = {
            'data': data,
            'timestamp': time.time()
        }
    
    def clear_insight_cache(self, insight_type: str = None):
        if insight_type:
            if insight_type in self.insights_cache:
                self.insights_cache[insight_type].clear()
        else:
            for insight in self.insights_cache:
                self.insights_cache[insight].clear()
    
    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—ç—Ç–∏—á–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫
    def get_ethical_analysis(self, analysis_type: str, doi: str) -> Optional[Dict]:
        if analysis_type in self.ethical_analysis_cache and doi in self.ethical_analysis_cache[analysis_type]:
            return self.ethical_analysis_cache[analysis_type][doi]
        return None
    
    def set_ethical_analysis(self, analysis_type: str, doi: str, data: Dict):
        if analysis_type not in self.ethical_analysis_cache:
            self.ethical_analysis_cache[analysis_type] = {}
        self.ethical_analysis_cache[analysis_type][doi] = {
            'data': data,
            'timestamp': time.time()
        }
    
    def clear_ethical_analysis(self, analysis_type: str = None):
        if analysis_type:
            if analysis_type in self.ethical_analysis_cache:
                self.ethical_analysis_cache[analysis_type].clear()
        else:
            for analysis in self.ethical_analysis_cache:
                self.ethical_analysis_cache[analysis].clear()

# ============================================================================
# üöÄ –ö–õ–ê–°–° –ê–î–ê–ü–¢–ò–í–ù–´–• –ó–ê–î–ï–†–ñ–ï–ö
# ============================================================================

class AdaptiveDelayManager:
    def __init__(self, initial_delay: float = Config.INITIAL_DELAY):
        self.initial_delay = initial_delay
        self.current_delay = initial_delay
        self.max_delay = Config.MAX_DELAY
        self.min_delay = Config.MIN_DELAY
        self.success_count = 0
        self.failure_count = 0
        self.last_request_time = 0
        self.response_times = []
        
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'avg_response_time': 0,
            'total_wait_time': 0
        }
    
    def wait_if_needed(self):
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        
        if elapsed < self.current_delay:
            wait_time = self.current_delay - elapsed
            time.sleep(wait_time)
            self.stats['total_wait_time'] += wait_time
        
        self.last_request_time = time.time()
        return self.current_delay
    
    def update_delay(self, success: bool, response_time: float = None):
        self.stats['total_requests'] += 1
        
        if response_time:
            self.response_times.append(response_time)
            if len(self.response_times) > 10:
                self.response_times.pop(0)
            self.stats['avg_response_time'] = sum(self.response_times) / len(self.response_times)
        
        if success:
            self.success_count += 1
            self.failure_count = max(0, self.failure_count - 1)
            self.stats['successful_requests'] += 1
            
            if self.success_count >= 2:
                self.current_delay = max(self.min_delay, self.current_delay * 0.7)
                self.success_count = 0
                
        else:
            self.failure_count += 1
            self.success_count = 0
            self.stats['failed_requests'] += 1
            
            self.current_delay = min(self.max_delay, self.current_delay * 1.3)
        
        self.current_delay = min(self.max_delay, self.current_delay)
    
    def get_delay(self) -> float:
        return self.current_delay
    
    def get_stats(self) -> Dict[str, Any]:
        total_requests = self.stats['total_requests']
        success_rate = (self.stats['successful_requests'] / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'current_delay': round(self.current_delay, 3),
            'total_requests': total_requests,
            'success_rate': round(success_rate, 1),
            'avg_response_time': round(self.stats['avg_response_time'], 3) if self.stats['avg_response_time'] > 0 else 0,
            'total_wait_time': round(self.stats['total_wait_time'], 2)
        }

# ============================================================================
# üìä –ö–õ–ê–°–° –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê –ü–†–û–ì–†–ï–°–°–ê (–ù–û–í–´–ô) - –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è Streamlit
# ============================================================================

class ProgressMonitor:
    def __init__(self, total_items: int, stage_name: str = "–û–±—Ä–∞–±–æ—Ç–∫–∞", progress_bar=None, status_text=None):
        self.total_items = total_items
        self.processed_items = 0
        self.start_time = time.time()
        self.stage_name = stage_name
        self.last_progress_time = self.start_time
        self.processing_speeds = []
        
        self.checkpoint_times = []
        self.checkpoint_items = []
        
        self.stats = {
            'success': 0,
            'failed': 0,
            'cached': 0,
            'skipped': 0
        }
        
        # Streamlit progress bar
        self.progress_bar = progress_bar
        self.status_text = status_text  # –¢–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ–º status_text –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞, —Å–æ–∑–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        if self.progress_bar is None:
            st.info(f"üìä {stage_name}: –Ω–∞—á–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ {total_items} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    
    def update(self, count: int = 1, item_type: str = None):
        self.processed_items += count
        
        if item_type:
            if item_type in self.stats:
                self.stats[item_type] += count
            else:
                self.stats[item_type] = count
        
        # Update Streamlit progress bar if available
        if self.progress_bar and self.total_items > 0:
            progress_percent = (self.processed_items / self.total_items) * 100
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
            self.progress_bar.progress(progress_percent / 100.0)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å
            if self.status_text:
                elapsed = time.time() - self.start_time
                if elapsed > 0:
                    speed = self.processed_items / elapsed
                    items_per_min = speed * 60
                    
                    remaining_items = self.total_items - self.processed_items
                    if speed > 0:
                        eta_seconds = remaining_items / speed
                        eta_str = self._format_time(eta_seconds)
                    else:
                        eta_str = "—Ä–∞—Å—á–µ—Ç..."
                    
                    stats_str = ""
                    for stat_type, count in self.stats.items():
                        if count > 0:
                            stats_str += f", {stat_type}: {count}"
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—É—Å–∞
                    self.status_text.text(
                        f"{self.stage_name}: {self.processed_items}/{self.total_items} "
                        f"({progress_percent:.1f}%), "
                        f"—Å–∫–æ—Ä–æ—Å—Ç—å: {items_per_min:.1f} DOI/–º–∏–Ω, "
                        f"–æ—Å—Ç–∞–ª–æ—Å—å: {eta_str}{stats_str}"
                    )
    
    def _format_time(self, seconds: float) -> str:
        if seconds < 60:
            return f"{seconds:.0f} —Å–µ–∫"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.0f} –º–∏–Ω"
        else:
            hours = seconds / 3600
            return f"{hours:.1f} —á"
    
    def get_summary(self) -> Dict[str, Any]:
        elapsed = time.time() - self.start_time
        
        if elapsed > 0:
            total_speed = self.processed_items / elapsed
            items_per_min = total_speed * 60
        else:
            items_per_min = 0
        
        return {
            'total_items': self.total_items,
            'processed_items': self.processed_items,
            'elapsed_time': round(elapsed, 1),
            'speed_per_min': round(items_per_min, 1),
            'success_count': self.stats.get('success', 0),
            'failed_count': self.stats.get('failed', 0),
            'cached_count': self.stats.get('cached', 0),
            'completion_percent': round((self.processed_items / self.total_items * 100), 1) if self.total_items > 0 else 0
        }
    
    def complete(self):
        elapsed = time.time() - self.start_time
        
        if self.total_items > 0:
            progress_percent = (self.processed_items / self.total_items) * 100
        else:
            progress_percent = 100
        
        summary = self.get_summary()
        
        if self.progress_bar:
            self.progress_bar.progress(1.0)
        
        st.success(f"‚úÖ {self.stage_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        st.info(f"–í—Å–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {self.total_items}")
        st.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.processed_items} ({progress_percent:.1f}%)")
        st.info(f"–ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è: {self._format_time(elapsed)}")
        st.info(f"–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: {summary['speed_per_min']:.1f} DOI/–º–∏–Ω")
        
        for stat_type, count in self.stats.items():
            if count > 0:
                st.info(f"{stat_type.capitalize()}: {count}")
        
        return summary

# ============================================================================
# üìù –ö–õ–ê–°–° –¢–†–ï–ö–ò–ù–ì–ê –ù–ï–£–î–ê–ß–ù–´–• DOI (–ù–û–í–´–ô)
# ============================================================================

class FailedDOITracker:
    def __init__(self):
        self.failed_dois = {}
        self.relationships = defaultdict(list)
        self.sources = {}
        
        self.stats = {
            'total_failed': 0,
            'analyzed_failed': 0,
            'ref_failed': 0,
            'citing_failed': 0,
            'retry_failed': 0,
            'by_error_type': defaultdict(int)
        }
    
    def add_failed_doi(self, doi: str, error: str, source_type: str, 
                       related_dois: List[str] = None, original_doi: str = None):
        
        self.failed_dois[doi] = {
            'doi': doi,
            'error': error,
            'source_type': source_type,
            'timestamp': datetime.now().isoformat(),
            'related_dois': related_dois or [],
            'original_doi': original_doi
        }
        
        self.sources[doi] = source_type
        
        if related_dois:
            self.relationships[doi].extend(related_dois)
        
        self.stats['total_failed'] += 1
        
        if source_type in self.stats:
            self.stats[f'{source_type}_failed'] += 1
        else:
            self.stats['by_error_type'][source_type] = self.stats['by_error_type'].get(source_type, 0) + 1
        
        self.stats['by_error_type'][error] += 1
    
    def get_failed_for_excel(self) -> List[Dict]:
        data = []
        
        for doi, info in self.failed_dois.items():
            relationship_info = ""
            if info['original_doi']:
                relationship_info = f"–ò—Å—Ç–æ—á–Ω–∏–∫: {info['original_doi']}"
            elif info['related_dois']:
                relationship_info = f"–°–≤—è–∑–∞–Ω —Å: {', '.join(info['related_dois'][:3])}"
                if len(info['related_dois']) > 3:
                    relationship_info += f"... (–µ—â–µ {len(info['related_dois']) - 3})"
            
            row = {
                'DOI': doi,
                'Source Type': info['source_type'],
                'Error': info['error'],
                'Relationships': relationship_info,
                'Relationship Count': len(info['related_dois']),
                'Error Date': info['timestamp']
            }
            data.append(row)
        
        return data
    
    def get_stats(self) -> Dict[str, Any]:
        return {
            'total_failed': self.stats['total_failed'],
            'analyzed_failed': self.stats['analyzed_failed'],
            'ref_failed': self.stats['ref_failed'],
            'citing_failed': self.stats['citing_failed'],
            'retry_failed': self.stats['retry_failed'],
            'error_types': dict(self.stats['by_error_type']),
            'unique_failed_dois': len(self.failed_dois)
        }
    
    def clear(self):
        self.failed_dois.clear()
        self.relationships.clear()
        self.sources.clear()
        self.stats = {
            'total_failed': 0,
            'analyzed_failed': 0,
            'ref_failed': 0,
            'citing_failed': 0,
            'retry_failed': 0,
            'by_error_type': defaultdict(int)
        }

# ============================================================================
# üåê –ö–õ–ê–°–° –ö–õ–ò–ï–ù–¢–û–í API
# ============================================================================

class APIClient:
    def __init__(self, cache_manager: SmartCacheManager, delay_manager: AdaptiveDelayManager):
        self.cache = cache_manager
        self.delay = delay_manager
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ArticleAnalyzer/3.0 (streamlit-app@example.com)',
            'Accept': 'application/json',
            'Accept-Encoding': 'gzip'
        })
    
    def make_request(self, url: str, cache_key: str, params: Dict = None, 
                    timeout: int = Config.REQUEST_TIMEOUT, category: str = "api") -> Dict:
        
        full_cache_key = f"{url}:{hash(str(params) if params else '')}"
        
        cached_data = self.cache.get(category, full_cache_key)
        if cached_data is not None:
            return cached_data
        
        wait_time = self.delay.wait_if_needed()
        
        try:
            start_time = time.time()
            response = self.session.get(url, params=params, timeout=timeout)
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                
                self.cache.set(category, full_cache_key, data)
                self.delay.update_delay(True, response_time)
                return data
                
            elif response.status_code == 429:
                self.delay.current_delay = min(self.delay.max_delay, self.delay.current_delay * 1.5)
                self.delay.update_delay(False, response_time)
                return {"error": f"Rate limit exceeded, wait {self.delay.current_delay:.1f}s", "status": 429}
                
            else:
                self.delay.update_delay(False, response_time)
                return {"error": f"API error {response.status_code}", "status": response.status_code}
                
        except requests.exceptions.Timeout:
            self.delay.update_delay(False, Config.REQUEST_TIMEOUT)
            return {"error": "Request timeout"}
        except Exception as e:
            self.delay.update_delay(False, 0)
            return {"error": f"Request failed: {str(e)}"}

class CrossrefClient(APIClient):
    def __init__(self, cache_manager: SmartCacheManager, delay_manager: AdaptiveDelayManager):
        super().__init__(cache_manager, delay_manager)
        self.base_url = Config.CROSSREF_URL
    
    def fetch_article(self, doi: str) -> Dict:
        clean_doi = self._clean_doi(doi)
        if not clean_doi:
            return {"error": "Invalid DOI"}
        
        url = f"{self.base_url}{clean_doi}"
        return self.make_request(url, f"crossref:{clean_doi}", category="crossref")
    
    def fetch_references(self, doi: str) -> List[str]:
        clean_doi = self._clean_doi(doi)
        if not clean_doi:
            return []
        
        data = self.fetch_article(clean_doi)
        references = []
        
        if 'message' in data and 'reference' in data['message']:
            for ref in data['message']['reference']:
                if 'DOI' in ref and ref['DOI']:
                    ref_doi = self._clean_doi(ref['DOI'])
                    if ref_doi:
                        references.append(ref_doi)
        
        return references
    
    def fetch_citations(self, doi: str) -> List[str]:
        clean_doi = self._clean_doi(doi)
        if not clean_doi:
            return []
        
        citing_dois = []
        try:
            url = f"{self.base_url}{clean_doi}"
            params = {'filter': 'has-reference:1'}
            data = self.make_request(url, f"crossref_citations:{clean_doi}", params=params)
            
            if 'message' in data and 'is-referenced-by' in data['message']:
                references = data['message']['is-referenced-by']
                for ref in references:
                    if isinstance(ref, dict) and 'DOI' in ref:
                        citing_doi = self._clean_doi(ref['DOI'])
                        if citing_doi:
                            citing_dois.append(citing_doi)
                            
        except Exception as e:
            st.warning(f"Crossref citations error for {doi}: {e}")
        
        return citing_dois
    
    def _clean_doi(self, doi: str) -> str:
        if not doi or not isinstance(doi, str):
            return ""
        
        doi = doi.strip()
        prefixes = ['doi:', 'DOI:', 'https://doi.org/', 'http://doi.org/', 'https://dx.doi.org/', 'http://dx.doi.org/']
        
        for prefix in prefixes:
            if doi.lower().startswith(prefix.lower()):
                doi = doi[len(prefix):]
        
        return doi.strip()

class OpenAlexClient(APIClient):
    def __init__(self, cache_manager: SmartCacheManager, delay_manager: AdaptiveDelayManager):
        super().__init__(cache_manager, delay_manager)
        self.base_url = Config.OPENALEX_URL
        self.works_url = Config.OPENALEX_WORKS_URL
    
    def fetch_article(self, doi: str) -> Dict:
        clean_doi = self._clean_doi(doi)
        if not clean_doi:
            return {"error": "Invalid DOI"}
        
        url = f"{self.base_url}{clean_doi}"
        return self.make_request(url, f"openalex:{clean_doi}", category="openalex")
    
    def fetch_citations(self, doi: str, max_pages: int = 3) -> List[str]:
        clean_doi = self._clean_doi(doi)
        if not clean_doi:
            return []
        
        citing_dois = []
        
        try:
            article_data = self.fetch_article(clean_doi)
            if 'error' in article_data:
                return []
            
            article_id = article_data.get('id', '').split('/')[-1]
            if not article_id:
                return []
            
            params = {
                'filter': f'cites:{article_id}',
                'per-page': 100,
                'select': 'doi,title,publication_year'
            }
            
            page = 1
            has_more = True
            
            while has_more and page <= max_pages:
                self.delay.wait_if_needed()
                
                response = self.session.get(self.works_url, params=params)
                if response.status_code == 200:
                    data = response.json()
                    
                    for work in data.get('results', []):
                        if work.get('doi'):
                            citing_doi = self._clean_doi(work['doi'])
                            if citing_doi:
                                citing_dois.append(citing_doi)
                    
                    if 'meta' in data and data['meta'].get('next_cursor'):
                        params['cursor'] = data['meta']['next_cursor']
                        page += 1
                        time.sleep(0.1)
                    else:
                        has_more = False
                else:
                    has_more = False
        
        except Exception as e:
            st.warning(f"OpenAlex citations error for {doi}: {e}")
        
        return list(set(citing_dois))
    
    def _clean_doi(self, doi: str) -> str:
        if not doi or not isinstance(doi, str):
            return ""
        
        doi = doi.strip()
        prefixes = ['doi:', 'DOI:', 'https://doi.org/', 'http://doi.org/', 'https://dx.doi.org/', 'http://dx.doi.org/']
        
        for prefix in prefixes:
            if doi.lower().startswith(prefix.lower()):
                doi = doi[len(prefix):]
        
        return doi.strip()

class RORClient:
    def __init__(self, cache_manager: SmartCacheManager):
        self.cache = cache_manager
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ArticleAnalyzer-ROR/3.0 (streamlit-app@example.com)',
            'Accept': 'application/json'
        })
        self.last_request_time = 0
        self.min_delay = 0.3

    def _respect_delay(self):
        elapsed = time.time() - self.last_request_time
        if elapsed < self.min_delay:
            time.sleep(self.min_delay - elapsed)
        self.last_request_time = time.time()

    def search_organization(self, query: str, category: str = "summary") -> Dict[str, str]:
        if not query or len(query.strip()) < 2:
            return self._create_empty_result()
        
        cache_key = f"ror_search:{query.strip().lower()}"
        
        if category != "summary":
            cached = self.cache.get_ror_cache(category, cache_key)
            if cached is not None:
                return cached
        
        cached = self.cache.get("ror_search", cache_key)
        if cached is not None:
            if cached.get('ror_id'):
                if category != "summary":
                    self.cache.set_ror_cache(category, cache_key, cached)
                return cached
        
        self._respect_delay()
        
        try:
            response = self.session.get(
                Config.ROR_API_URL,
                params={'query': query.strip()},
                timeout=10
            )
            
            if response.status_code != 200:
                return self._create_empty_result()
            
            data = response.json()
            items = data.get('items', [])
            
            if not items:
                return self._create_empty_result()
            
            best = self._improved_find_best_match(query.strip(), items)
            if not best:
                return self._create_empty_result()
            
            colab_url = ""
            try:
                ror_id = best['id'].split('/')[-1]
                colab_url = f"https://colab.ws/organizations/{ror_id}"
            except:
                pass
            
            website = ""
            try:
                links = best.get('links', []) or []
                for link in links:
                    url = (link.get('value') or link.get('url') if isinstance(link, dict) else str(link)) if link else None
                    if url and isinstance(url, str):
                        url = url.strip()
                        website = url if url.startswith('http') else 'https://' + url
                        break
            except:
                pass
            
            result = {
                'ror_id': colab_url,
                'website': website,
                'score': best.get('score', 0),
                'name': best.get('name', ''),
                'acronyms': best.get('acronyms', [])
            }
            
            if colab_url:
                self.cache.set("ror_search", cache_key, result, category="ror_search")
                if category != "summary":
                    self.cache.set_ror_cache(category, cache_key, result)
            
            return result
            
        except Exception as e:
            st.warning(f"ROR error for query '{query}': {e}")
            return self._create_empty_result()
    
    def _improved_find_best_match(self, query: str, items: List[Dict]) -> Optional[Dict]:
        if not items:
            return None
        
        q = query.strip().lower()
        best_item = None
        best_score = -1
        
        strategies = [
            self._strategy_exact_match,
            self._strategy_partial_match,
            self._strategy_acronym_match,
            self._strategy_fuzzy_match
        ]
        
        for item in items:
            score = 0
            name = item.get('name', '').lower()
            aliases = [a.lower() for a in item.get('aliases', [])]
            acronyms = [a.lower() for a in item.get('acronyms', []) if a]
            
            for strategy in strategies:
                strategy_score = strategy(q, name, aliases, acronyms)
                if strategy_score > score:
                    score = strategy_score
            
            ror_score = item.get('score', 0) * 50
            
            final_score = max(score, ror_score)
            
            if final_score > best_score:
                best_score = final_score
                best_item = item
        
        return best_item
    
    def _strategy_exact_match(self, query: str, name: str, aliases: List[str], acronyms: List[str]) -> int:
        if query == name or query in aliases:
            return 10000
        return 0
    
    def _strategy_partial_match(self, query: str, name: str, aliases: List[str], acronyms: List[str]) -> int:
        all_texts = [name] + aliases + acronyms
        for text in all_texts:
            if query in text or text in query:
                return 9000
        return 0
    
    def _strategy_acronym_match(self, query: str, name: str, aliases: List[str], acronyms: List[str]) -> int:
        if query in acronyms:
            return 9500
        return 0
    
    def _strategy_fuzzy_match(self, query: str, name: str, aliases: List[str], acronyms: List[str]) -> int:
        all_texts = [name] + aliases
        best_fuzzy = 0
        for text in all_texts:
            if text:
                score = fuzz.token_set_ratio(query, text)
                if score > best_fuzzy:
                    best_fuzzy = score
        return best_fuzzy
    
    def _create_empty_result(self) -> Dict[str, str]:
        return {
            'ror_id': '', 
            'website': '', 
            'score': 0,
            'name': '', 
            'acronyms': []
        }

# ============================================================================
# üõ†Ô∏è –ö–õ–ê–°–° –û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–•
# ============================================================================

class DataProcessor:
    def __init__(self, cache_manager: SmartCacheManager):
        self.cache = cache_manager
        self.country_codes = Config.COUNTRY_CODES
    
    def extract_article_info(self, crossref_data: Dict, openalex_data: Dict, 
                           doi: str, references: List[str], citations: List[str]) -> Dict:
        
        pub_info = self._extract_publication_info(crossref_data, openalex_data)
        authors, countries_from_auth = self._extract_authors_info(crossref_data, openalex_data)
        countries = self._extract_countries_info(authors, openalex_data)
        
        country_codes = [self._country_to_code(c) for c in countries]
        country_codes = list(set(filter(None, country_codes)))
        
        orcid_urls = []
        for author in authors:
            if author.get('orcid'):
                orcid_url = self._format_orcid_id(author['orcid'])
                if orcid_url:
                    orcid_urls.append(orcid_url)
        
        pages_field = pub_info['pages']
        if not pages_field and pub_info['article_number']:
            pages_field = f"Article {pub_info['article_number']}"
        
        quick_insights = self._extract_quick_insights(
            authors, countries, references, citations, pub_info
        )
        
        return {
            'doi': doi,
            'publication_info': pub_info,
            'authors': authors,
            'countries': country_codes,
            'orcid_urls': orcid_urls,
            'references': references,
            'citations': citations,
            'pages_formatted': pages_field,
            'status': 'success',
            'quick_insights': quick_insights
        }
    
    def _extract_quick_insights(self, authors: List[Dict], countries: List[str], 
                               references: List[str], citations: List[str], 
                               pub_info: Dict) -> Dict:
        current_year = datetime.now().year
        
        try:
            pub_year = int(pub_info.get('year', current_year))
            article_age = current_year - pub_year
        except:
            article_age = 0
        
        insights = {
            'author_count': len(authors),
            'country_count': len(countries),
            'reference_count': len(references),
            'citation_count': len(citations),
            'publication_year': pub_info.get('year', ''),
            'article_age': article_age,
            'citation_velocity': 0,
            'geographic_diversity': len(countries) / max(1, len(authors)),
            'self_citation_risk': 0,
            'intra_affiliation_citation_ratio': 0
        }
        
        if article_age > 0 and citations:
            insights['citation_velocity'] = len(citations) / article_age
        
        return insights
    
    def _extract_publication_info(self, crossref_data: Dict, openalex_data: Dict) -> Dict:
        pub_info = {
            'title': '',
            'journal': '',
            'publication_date': '',
            'year': '',
            'volume': '',
            'pages': '',
            'article_number': '',
            'citation_count_crossref': 0,
            'citation_count_openalex': 0,
            'doi': ''
        }
        
        if 'message' in crossref_data:
            msg = crossref_data['message']
            
            pub_info['doi'] = msg.get('DOI', '')
            pub_info['title'] = msg.get('title', [''])[0] if msg.get('title') else ''
            pub_info['journal'] = msg.get('container-title', [''])[0] if msg.get('container-title') else ''
            
            # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ Crossref
            pub_date = None
            if 'created' in msg and 'date-parts' in msg['created']:
                created_date = msg.get('created', {})
                if 'date-parts' in created_date and created_date['date-parts']:
                    pub_date = created_date['date-parts'][0]

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ created, —Ç–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º license –∫–∞–∫ fallback
            if not pub_date and 'license' in msg:
                for license_item in msg['license']:
                    if isinstance(license_item, dict) and 'start' in license_item:
                        start_date = license_item.get('start', {})
                        if 'date-parts' in start_date and start_date['date-parts']:
                            pub_date = start_date['date-parts'][0]
                            break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ license, –∏—â–µ–º –≤ created
            if not pub_date and 'created' in msg:
                created_date = msg.get('created', {})
                if 'date-parts' in created_date and created_date['date-parts']:
                    pub_date = created_date['date-parts'][0]
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ created, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É
            if not pub_date and 'published' in msg and 'date-parts' in msg['published']:
                pub_date = msg['published']['date-parts'][0]
            
            if pub_date:
                pub_info['year'] = str(pub_date[0])
                if len(pub_date) >= 2:
                    month = str(pub_date[1]).zfill(2)
                    if len(pub_date) >= 3:
                        day = str(pub_date[2]).zfill(2)
                        pub_info['publication_date'] = f"{pub_info['year']}-{month}-{day}"
                    else:
                        pub_info['publication_date'] = f"{pub_info['year']}-{month}-15"
                else:
                    pub_info['publication_date'] = f"{pub_info['year']}-01-01"
            
            pub_info['volume'] = msg.get('volume', '')
            pub_info['pages'] = msg.get('page', '')
            pub_info['article_number'] = msg.get('article-number', '')
            pub_info['citation_count_crossref'] = msg.get('is-referenced-by-count', 0)
        
        if 'title' in openalex_data:
            pub_info['title'] = pub_info['title'] or openalex_data.get('title', '')
        
        if 'primary_location' in openalex_data:
            source = openalex_data['primary_location'].get('source', {})
            pub_info['journal'] = pub_info['journal'] or source.get('display_name', '')
        
        if 'publication_year' in openalex_data:
            pub_info['year'] = pub_info['year'] or str(openalex_data.get('publication_year', ''))
        
        if 'biblio' in openalex_data:
            biblio = openalex_data['biblio']
            pub_info['volume'] = pub_info['volume'] or biblio.get('volume', '')
            if not pub_info['pages']:
                pub_info['pages'] = biblio.get('first_page', '') + '-' + biblio.get('last_page', '') \
                                    if biblio.get('first_page') else biblio.get('pages', '')
        
        pub_info['citation_count_openalex'] = openalex_data.get('cited_by_count', 0)
        
        return pub_info
    
    def _extract_authors_info(self, crossref_data: Dict, openalex_data: Dict) -> Tuple[List[Dict], List[str]]:
        authors = []
        countries = []
        
        try:
            if openalex_data and 'authorships' in openalex_data:
                for authorship in openalex_data['authorships']:
                    if not authorship:
                        continue
                    
                    author_display = authorship.get('author', {})
                    full_name = authorship.get('raw_author_name') or author_display.get('display_name', '')
                    
                    if not full_name:
                        continue
                    
                    author_info = {
                        'name': full_name,
                        'affiliation': [],
                        'orcid': author_display.get('orcid', '')
                    }
                    
                    institutions = authorship.get('institutions', [])
                    if institutions:
                        for inst in institutions:
                            if inst and isinstance(inst, dict):
                                display_name = inst.get('display_name')
                                if display_name:
                                    clean_aff = self._clean_affiliation(display_name)
                                    if clean_aff:
                                        author_info['affiliation'].append(clean_aff)
                                
                                country_code = inst.get('country_code')
                                if country_code:
                                    countries.append(country_code)
                    
                    authors.append(author_info)
        except Exception as e:
            st.warning(f"‚ö†Ô∏è OpenAlex author extraction error: {e}")
        
        if not authors and crossref_data:
            try:
                message = crossref_data.get('message', {})
                crossref_authors = message.get('author', [])
                
                if crossref_authors:
                    for author_obj in crossref_authors:
                        if not author_obj:
                            continue
                        
                        given = author_obj.get('given', '')
                        family = author_obj.get('family', '')
                        full_name = f"{given} {family}".strip()
                        
                        if not full_name:
                            continue
                        
                        author_info = {
                            'name': full_name,
                            'affiliation': [],
                            'orcid': author_obj.get('ORCID', '')
                        }
                        
                        affiliations = author_obj.get('affiliation', [])
                        if affiliations:
                            for affil in affiliations:
                                if affil and isinstance(affil, dict):
                                    affil_name = affil.get('name')
                                    if affil_name:
                                        clean_aff = self._clean_affiliation(affil_name)
                                        if clean_aff:
                                            author_info['affiliation'].append(clean_aff)
                        
                        authors.append(author_info)
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Crossref author extraction error: {e}")
        
        return authors, list(set(countries))
    
    def _extract_author_from_crossref(self, full_name: Optional[str], crossref_data: Dict, author_obj: Dict = None) -> Optional[Dict]:
        if author_obj is None:
            return None
        
        given = author_obj.get('given', '')
        family = author_obj.get('family', '')
        name = f"{given} {family}".strip() if given or family else full_name
        
        if not name:
            return None
        
        author_info = {
            'name': name,
            'affiliation': [],
            'orcid': author_obj.get('ORCID', '')
        }
        
        if 'affiliation' in author_obj:
            for affil in author_obj['affiliation']:
                if 'name' in affil:
                    clean_aff = self._clean_affiliation(affil['name'])
                    if clean_aff and clean_aff not in author_info['affiliation']:
                        author_info['affiliation'].append(clean_aff)
        
        return author_info
    
    def _clean_affiliation(self, affiliation: str) -> str:
        if not affiliation:
            return ""
        
        patterns_to_remove = [
            r',\s*[A-Z]{2}$',
            r',\s*[A-Z]{2}\s*\d+',
            r',\s*USA$', r',\s*United States$',
            r',\s*UK$', r',\s*United Kingdom$',
            r',\s*China$', r',\s*–†–æ—Å—Å–∏—è$', r',\s*Russia$',
            r'\s*\([^)]*[Cc]ountry[^)]*\)',
            r'\s*\[[^\]]*[Cc]ountry[^\]]*\]',
            r'\b\d{5,6}(-\d{4})?\b',
        ]
        
        clean_aff = affiliation
        for pattern in patterns_to_remove:
            clean_aff = re.sub(pattern, '', clean_aff, flags=re.IGNORECASE)
        
        clean_aff = re.sub(r',\s*,', ',', clean_aff)
        clean_aff = clean_aff.strip(' ,;')
        
        return clean_aff if clean_aff and len(clean_aff) > 2 else affiliation
    
    def _extract_countries_info(self, authors: List[Dict], openalex_data: Dict) -> List[str]:
        countries = []
        
        if 'authorships' in openalex_data:
            for authorship in openalex_data['authorships']:
                for inst in authorship.get('institutions', []):
                    if 'country_code' in inst and inst['country_code']:
                        countries.append(inst['country_code'])
        
        if not countries:
            for author in authors:
                for affil in author['affiliation']:
                    for country_name, country_code in self.country_codes.items():
                        if country_name.lower() in affil.lower():
                            countries.append(country_code)
                            break
        
        return list(set(countries))
    
    def _country_to_code(self, country_name: str) -> str:
        if not country_name:
            return ""
        
        for name, code in self.country_codes.items():
            if country_name.lower() == name.lower():
                return code
        
        for name, code in self.country_codes.items():
            if name.lower() in country_name.lower():
                return code
        
        return country_name[:2].upper() if len(country_name) >= 2 else country_name.upper()
    
    def _format_orcid_id(self, orcid_id: str) -> str:
        if not orcid_id:
            return ""
        
        if orcid_id.startswith('https://orcid.org/'):
            return orcid_id
        
        clean_id = re.sub(r'[^\dXx-]', '', orcid_id.strip())
        
        if '-' in clean_id:
            return f"https://orcid.org/{clean_id}"
        elif len(clean_id) == 16:
            formatted = f"{clean_id[:4]}-{clean_id[4:8]}-{clean_id[8:12]}-{clean_id[12:]}"
            return f"https://orcid.org/{formatted}"
        else:
            return f"https://orcid.org/{clean_id}"
    
    def normalize_author_name(self, full_name: str) -> str:
        if not full_name:
            return ""
        
        name = re.sub(r'\s+', ' ', full_name.strip().replace(',', ' '))
        parts = name.split()
        
        if len(parts) == 0:
            return ""
        if len(parts) == 1:
            return parts[0]
        
        family = parts[-1]
        
        for part in parts[:-1]:
            clean = re.sub(r'[^A-Za-z–ê-—è–Å—ë]', '', part)
            if clean and clean[0].isalpha():
                initial = clean[0].upper()
                return f"{family} {initial}"
        
        return family

# ============================================================================
# üéØ –ö–õ–ê–°–° –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–ò DOI (–ù–û–í–´–ô) - –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è Streamlit
# ============================================================================

class OptimizedDOIProcessor:
    def __init__(self, cache_manager: SmartCacheManager, 
                 delay_manager: AdaptiveDelayManager,
                 data_processor: DataProcessor,
                 failed_tracker: FailedDOITracker):
        
        self.cache = cache_manager
        self.delay = delay_manager
        self.data_processor = data_processor
        self.failed_tracker = failed_tracker
        
        self.crossref_client = CrossrefClient(cache_manager, delay_manager)
        self.openalex_client = OpenAlexClient(cache_manager, delay_manager)
        self.ror_client = RORClient(cache_manager)
        
        self.processed_dois = {}
        self.reference_relationships = defaultdict(list)
        self.citation_relationships = defaultdict(list)
        
        self.author_affiliation_map = defaultdict(set)
        self.doi_author_map = defaultdict(list)
        self.doi_affiliation_map = defaultdict(set)
        
        self.stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'cached_hits': 0,
            'api_calls': 0
        }
    
    def process_doi_batch(self, dois: List[str], source_type: str = "analyzed", 
                         original_doi: str = None, fetch_refs: bool = True, 
                         fetch_cites: bool = True, batch_size: int = Config.BATCH_SIZE,
                         progress_container=None) -> Dict[str, Dict]:
        
        results = {}
        total_batches = (len(dois) + batch_size - 1) // batch_size
        
        if progress_container:
            progress_container.info(f"üîß –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(dois)} DOI (–∏—Å—Ç–æ—á–Ω–∏–∫: {source_type})")
            progress_container.info(f"üì¶ –†–∞–∑–±–∏—Ç–æ –Ω–∞ {total_batches} –ø–∞—á–µ–∫ –ø–æ {batch_size} DOI")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è Streamlit
        progress_bar = None
        status_text = None
        if progress_container:
            progress_bar = progress_container.progress(0)
            status_text = progress_container.empty()
        
        # –ü–µ—Ä–µ–¥–∞–µ–º –∏ progress_bar –∏ status_text –≤ ProgressMonitor
        monitor = ProgressMonitor(len(dois), f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {source_type}", 
                                 progress_bar=progress_bar, status_text=status_text)
        
        for batch_idx in range(0, len(dois), batch_size):
            batch = dois[batch_idx:batch_idx + batch_size]
            batch_results = self._process_single_batch(
                batch, source_type, original_doi, True, True
            )
            
            results.update(batch_results)
            
            monitor.update(len(batch), 'processed')
            
            batch_success = sum(1 for r in batch_results.values() if r.get('status') == 'success')
            if progress_container:
                progress_container.info(f"–ü–∞—á–∫–∞ {batch_idx//batch_size + 1}/{total_batches}: "
                                      f"{batch_success}/{len(batch)} —É—Å–ø–µ—à–Ω–æ, "
                                      f"{self.delay.get_delay():.2f}s –∑–∞–¥–µ—Ä–∂–∫–∞")
        
        monitor.complete()
        
        successful = sum(1 for r in results.values() if r.get('status') == 'success')
        failed = len(dois) - successful
        
        self.stats['total_processed'] += len(dois)
        self.stats['successful'] += successful
        self.stats['failed'] += failed
        
        return results
    
    def _process_single_batch(self, batch: List[str], source_type: str, 
                             original_doi: str, fetch_refs: bool, fetch_cites: bool) -> Dict[str, Dict]:
        results = {}
        
        with ThreadPoolExecutor(max_workers=min(Config.MAX_WORKERS, len(batch))) as executor:
            future_to_doi = {}
            
            for doi in batch:
                future = executor.submit(
                    self._process_single_doi_wrapper,
                    doi, source_type, original_doi, True, True
                )
                future_to_doi[future] = doi
            
            for future in as_completed(future_to_doi):
                doi = future_to_doi[future]
                try:
                    results[doi] = future.result(timeout=60)
                except Exception as e:
                    self._handle_processing_error(doi, str(e), source_type, original_doi)
                    results[doi] = {
                        'doi': doi,
                        'status': 'failed',
                        'error': f"–¢–∞–π–º–∞—É—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
                    }
        
        return results
    
    def _process_single_doi_wrapper(self, doi: str, source_type: str, 
                                   original_doi: str, fetch_refs: bool, fetch_cites: bool) -> Dict:
        try:
            return self._process_single_doi_optimized(
                doi, source_type, original_doi, True, True
            )
        except Exception as e:
            self._handle_processing_error(doi, str(e), source_type, original_doi)
            return {
                'doi': doi,
                'status': 'failed',
                'error': f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
            }
    
    def _process_single_doi_optimized(self, doi: str, source_type: str, 
                                     original_doi: str, fetch_refs: bool, fetch_cites: bool) -> Dict:
        
        cache_key = f"full_result:{doi}"
        cached_result = self.cache.get("full_analysis", cache_key)
        
        if cached_result is not None:
            self.stats['cached_hits'] += 1
            return cached_result
        
        crossref_data = {}
        openalex_data = {}
        
        try:
            crossref_data = self.crossref_client.fetch_article(doi)
            openalex_data = self.openalex_client.fetch_article(doi)
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {str(e)}"
            self._handle_processing_error(doi, error_msg, source_type, original_doi)
            return {
                'doi': doi,
                'status': 'failed',
                'error': error_msg
            }
        
        crossref_error = None
        openalex_error = None
        
        if isinstance(crossref_data, dict):
            crossref_error = crossref_data.get('error')
        if isinstance(openalex_data, dict):
            openalex_error = openalex_data.get('error')
        
        if crossref_error and openalex_error:
            error_msg = f"–û—à–∏–±–∫–∏ API: Crossref - {crossref_error}, OpenAlex - {openalex_error}"
            self._handle_processing_error(doi, error_msg, source_type, original_doi)
            return {
                'doi': doi,
                'status': 'failed',
                'error': error_msg
            }
        
        crossref_data = crossref_data if isinstance(crossref_data, dict) else {}
        openalex_data = openalex_data if isinstance(openalex_data, dict) else {}
        
        references = []
        try:
            refs = self.crossref_client.fetch_references(doi)
            references = refs if isinstance(refs, list) else []
            
            if references:
                self.reference_relationships[doi] = references
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error fetching references for {doi}: {e}")
        
        citations = []
        try:
            cites_openalex = self.openalex_client.fetch_citations(doi)
            cites_crossref = self.crossref_client.fetch_citations(doi)
            
            cites_openalex = cites_openalex if isinstance(cites_openalex, list) else []
            cites_crossref = cites_crossref if isinstance(cites_crossref, list) else []
            
            citations = list(set(cites_openalex + cites_crossref))
            
            if citations:
                self.citation_relationships[doi] = citations
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error fetching citations for {doi}: {e}")
        
        result = self.data_processor.extract_article_info(
            crossref_data, openalex_data, doi, references, citations
        )
        
        if result.get('status') == 'success':
            for author in result.get('authors', []):
                author_name = author.get('name', '')
                if author_name:
                    self.doi_author_map[doi].append(author_name)
                    for affiliation in author.get('affiliation', []):
                        if affiliation:
                            self.author_affiliation_map[author_name].add(affiliation)
                            self.doi_affiliation_map[doi].add(affiliation)
        
        if result.get('status') == 'success':
            self.stats['successful'] += 1
            
            self.cache.set("full_analysis", cache_key, result, category="full_analysis")
            
            self.cache.update_popularity(doi)
        else:
            self.stats['failed'] += 1
        
        self.stats['api_calls'] += 2
        
        return result
    
    def _handle_processing_error(self, doi: str, error: str, source_type: str, original_doi: str):
        
        related_dois = []
        if original_doi:
            related_dois.append(original_doi)
        
        self.failed_tracker.add_failed_doi(
            doi=doi,
            error=error,
            source_type=source_type,
            related_dois=related_dois,
            original_doi=original_doi
        )
        
        self.cache.mark_as_failed("full_analysis", doi, error)
    
    def collect_all_references(self, results: Dict[str, Dict]) -> List[str]:
        all_refs = []
        
        for doi, result in results.items():
            if result.get('status') == 'success':
                refs = result.get('references', [])
                if refs:
                    all_refs.extend(refs)
        
        for doi, result in results.items():
            if result.get('status') == 'success':
                refs = result.get('references', [])
                if refs:
                    for ref_doi in refs:
                        if ref_doi not in self.reference_relationships:
                            self.reference_relationships[ref_doi] = []
                        if doi not in self.reference_relationships[ref_doi]:
                            self.reference_relationships[ref_doi].append(doi)
        
        return all_refs
    
    def collect_all_citations(self, results: Dict[str, Dict]) -> List[str]:
        all_cites = []
        
        for doi, result in results.items():
            if result.get('status') == 'success':
                cites = result.get('citations', [])
                if cites:
                    all_cites.extend(cites)
        
        return all_cites
    
    def get_relationships(self) -> Dict[str, Any]:
        return {
            'reference_relationships': dict(self.reference_relationships),
            'citation_relationships': dict(self.citation_relationships),
            'total_relationships': len(self.reference_relationships) + len(self.citation_relationships)
        }
    
    def get_insights_data(self) -> Dict[str, Any]:
        return {
            'author_affiliation_map': dict(self.author_affiliation_map),
            'doi_author_map': dict(self.doi_author_map),
            'doi_affiliation_map': dict(self.doi_affiliation_map)
        }
    
    def get_stats(self) -> Dict[str, Any]:
        return {
            'total_processed': self.stats['total_processed'],
            'successful': self.stats['successful'],
            'failed': self.stats['failed'],
            'cached_hits': self.stats['cached_hits'],
            'api_calls': self.stats['api_calls'],
            'cache_efficiency': round((self.stats['cached_hits'] / max(1, self.stats['total_processed'])) * 100, 1),
            'success_rate': round((self.stats['successful'] / max(1, self.stats['total_processed'])) * 100, 1)
        }
    
    def retry_failed_dois(self, failed_tracker: FailedDOITracker, max_retries: int = 1,
                         progress_container=None) -> Dict[str, Dict]:
        retry_results = {}
        
        rate_limit_dois = []
        for doi, info in failed_tracker.failed_dois.items():
            if 'Rate limit exceeded' in info.get('error', ''):
                rate_limit_dois.append(doi)
        
        if not rate_limit_dois:
            return retry_results
        
        if progress_container:
            progress_container.info(f"\nüîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(rate_limit_dois)} DOI —Å rate limiting –æ—à–∏–±–∫–∞–º–∏")
        
        original_delay = self.delay.current_delay
        self.delay.current_delay = min(Config.MAX_DELAY, original_delay * 1.5)
        
        if progress_container:
            progress_container.info(f"–£–≤–µ–ª–∏—á–µ–Ω–∞ –∑–∞–¥–µ—Ä–∂–∫–∞: {original_delay:.3f}s -> {self.delay.current_delay:.3f}s")
        
        retry_results = self.process_doi_batch(
            rate_limit_dois, "retry", None, True, True, Config.BATCH_SIZE, progress_container
        )
        
        self.delay.current_delay = original_delay
        
        successful_retries = sum(1 for r in retry_results.values() if r.get('status') == 'success')
        
        if progress_container:
            progress_container.info(f"–£—Å–ø–µ—à–Ω–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful_retries}/{len(rate_limit_dois)}")
        
        return retry_results

# ============================================================================
# üîç –ö–õ–ê–°–° –ò–ï–†–ê–†–•–ò–ß–ï–°–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê –î–ê–ù–ù–´–• (–ù–û–í–´–ô)
# ============================================================================

class HierarchicalDataAnalyzer:
    def __init__(self, cache_manager: SmartCacheManager, data_processor: DataProcessor,
                 doi_processor: OptimizedDOIProcessor):
        self.cache = cache_manager
        self.processor = data_processor
        self.doi_processor = doi_processor
        
        # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —É—Ä–æ–≤–Ω–∏ –¥–∞–Ω–Ω—ã—Ö
        self.data_levels = {
            'level_0': set(),  # DOI –∏ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            'level_1': set(),  # + –∞–≤—Ç–æ—Ä—ã, –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏, –≥–æ–¥—ã
            'level_2': set(),  # + –ø–æ–ª–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ü–∏—Ç–∏—Ä—É—é—â–∏—Ö
            'level_3': set()   # + —Å–µ—Ç–µ–≤–æ–π –∞–Ω–∞–ª–∏–∑ –∏ ML
        }
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.citation_timestamps = defaultdict(list)
        self.journal_citation_counts = defaultdict(Counter)
        self.author_citation_network = defaultdict(set)
        self.affiliation_citation_network = defaultdict(set)
        
        # ML –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
        self.isolation_forest = None
        self.scaler = StandardScaler()
        
    def analyze_quick_checks(self, analyzed_results: Dict[str, Dict], 
                            citing_results: Dict[str, Dict]) -> List[Dict]:
        """–ë—ã—Å—Ç—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ (5-10 —Å–µ–∫—É–Ω–¥ –Ω–∞ —Å—Ç–∞—Ç—å—é)"""
        quick_checks = []
        
        for doi, result in analyzed_results.items():
            if result.get('status') != 'success':
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º —Ü–∏—Ç–∏—Ä—É—é—â–∏–µ —Å—Ç–∞—Ç—å–∏ –¥–ª—è —ç—Ç–æ–π DOI
            citing_dois = result.get('citations', [])
            citing_data = {}
            for cite_doi in citing_dois:
                if cite_doi in citing_results and citing_results[cite_doi].get('status') == 'success':
                    citing_data[cite_doi] = citing_results[cite_doi]
            
            # –ê–Ω–∞–ª–∏–∑
            analysis = self._perform_quick_check_analysis(doi, result, citing_data)
            quick_checks.append(analysis)
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self.cache.set_ethical_analysis('quick_checks', doi, analysis)
        
        return sorted(quick_checks, key=lambda x: x['Quick_Risk_Score'], reverse=True)
    
    def _perform_quick_check_analysis(self, doi: str, result: Dict, 
                                     citing_data: Dict[str, Dict]) -> Dict:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –±—ã—Å—Ç—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
        
        pub_info = result['publication_info']
        authors = result.get('authors', [])
        analyzed_countries = result.get('countries', [])
        
        # 1. Journal Citation Concentration
        journal_concentration = self._calculate_journal_concentration(citing_data)
        
        # 2. Author Self-Citation Rate
        author_self_citation = self._calculate_author_self_citation(authors, citing_data)
        
        # 3. Affiliation Self-Citation
        affiliation_self_citation = self._calculate_affiliation_self_citation(authors, citing_data)
        
        # 4. Single Country Concentration
        single_country = self._calculate_single_country_concentration(citing_data, analyzed_countries)
        
        # 5. Citation Velocity
        citation_velocity = self._calculate_citation_velocity(result, citing_data)
        
        # 6. First Year Share
        first_year_share = self._calculate_first_year_share(result, citing_data)
        
        # 7. Future Citations
        future_citations = self._check_future_citations(result, citing_data)
        
        # –ü–æ–¥—Å—á–µ—Ç –∫—Ä–∞—Å–Ω—ã—Ö —Ñ–ª–∞–≥–æ–≤
        red_flags = 0
        flags = []
        
        if journal_concentration > Config.QUICK_CHECK_THRESHOLDS['journal_concentration']:
            red_flags += 1
            flags.append(f"Journal concentration: {journal_concentration:.1%}")
        
        if author_self_citation > Config.QUICK_CHECK_THRESHOLDS['author_self_citation']:
            red_flags += 1
            flags.append(f"Author self-citation: {author_self_citation:.1%}")
        
        if affiliation_self_citation > Config.QUICK_CHECK_THRESHOLDS['affiliation_self_citation']:
            red_flags += 1
            flags.append(f"Affiliation self-citation: {affiliation_self_citation:.1%}")
        
        if single_country > Config.QUICK_CHECK_THRESHOLDS['single_country']:
            red_flags += 1
            flags.append(f"Single country: {single_country:.1%}")
        
        if citation_velocity > Config.QUICK_CHECK_THRESHOLDS['citation_velocity']:
            red_flags += 1
            flags.append(f"Citation velocity: {citation_velocity:.1f}/year")
        
        if first_year_share > Config.QUICK_CHECK_THRESHOLDS['first_year_share']:
            red_flags += 1
            flags.append(f"First year share: {first_year_share:.1%}")
        
        if future_citations > 0:
            red_flags += 1
            flags.append(f"Future citations: {future_citations}")
        
        # –†–∞—Å—á–µ—Ç —Ä–∏—Å–∫–∞
        quick_risk_score = self._calculate_quick_risk_score(
            journal_concentration, author_self_citation, affiliation_self_citation,
            single_country, citation_velocity, first_year_share, future_citations
        )
        
        # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        recommended_action = self._determine_recommended_action(quick_risk_score, red_flags)
        
        return {
            'DOI': doi,
            'Title': pub_info.get('title', '')[:50] + ('...' if len(pub_info.get('title', '')) > 50 else ''),
            'Quick_Risk_Score': quick_risk_score,
            'Red_Flags': red_flags,
            'Flag_1_Journal_Concentration': journal_concentration > Config.QUICK_CHECK_THRESHOLDS['journal_concentration'],
            'Flag_2_Author_Self_Citation': author_self_citation > Config.QUICK_CHECK_THRESHOLDS['author_self_citation'],
            'Flag_3_Affiliation_Self_Citation': affiliation_self_citation > Config.QUICK_CHECK_THRESHOLDS['affiliation_self_citation'],
            'Flag_4_Single_Country': single_country > Config.QUICK_CHECK_THRESHOLDS['single_country'],
            'Flag_5_Citation_Velocity': citation_velocity > Config.QUICK_CHECK_THRESHOLDS['citation_velocity'],
            'Flag_6_First_Year_Share': first_year_share > Config.QUICK_CHECK_THRESHOLDS['first_year_share'],
            'Flag_7_Future_Citations': future_citations > 0,
            'Future_Citations_Count': future_citations,
            'Journal_Concentration_Rate': round(journal_concentration * 100, 1),
            'Author_Self_Citation_Rate': round(author_self_citation * 100, 1),
            'Affiliation_Self_Citation_Rate': round(affiliation_self_citation * 100, 1),
            'Single_Country_Percent': round(single_country * 100, 1),
            'Citation_Velocity_Annual': round(citation_velocity, 1),
            'First_Year_Share_Percent': round(first_year_share * 100, 1),
            'Recommended_Action': recommended_action,
            'Flags_Details': '; '.join(flags) if flags else 'None'
        }
    
    def _calculate_journal_concentration(self, citing_data: Dict[str, Dict]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –ø–æ –∂—É—Ä–Ω–∞–ª–∞–º"""
        if not citing_data:
            return 0.0
        
        journal_counter = Counter()
        for cite_result in citing_data.values():
            journal = cite_result.get('publication_info', {}).get('journal', '')
            if journal:
                journal_counter[journal] += 1
        
        if not journal_counter:
            return 0.0
        
        total_citations = sum(journal_counter.values())
        top_journal_count = journal_counter.most_common(1)[0][1]
        
        return top_journal_count / total_citations
    
    def _calculate_author_self_citation(self, analyzed_authors: List[Dict], 
                                       citing_data: Dict[str, Dict]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç self-citation –ø–æ –∞–≤—Ç–æ—Ä–∞–º"""
        if not citing_data or not analyzed_authors:
            return 0.0
        
        analyzed_author_names = {author['name'] for author in analyzed_authors}
        total_citations = len(citing_data)
        self_citations = 0
        
        for cite_result in citing_data.values():
            citing_authors = cite_result.get('authors', [])
            citing_author_names = {author['name'] for author in citing_authors}
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—â–∏—Ö –∞–≤—Ç–æ—Ä–æ–≤
            common_authors = analyzed_author_names.intersection(citing_author_names)
            if common_authors:
                self_citations += 1
        
        return self_citations / total_citations if total_citations > 0 else 0.0
    
    def _calculate_affiliation_self_citation(self, analyzed_authors: List[Dict], 
                                           citing_data: Dict[str, Dict]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç self-citation –ø–æ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏—è–º"""
        if not citing_data or not analyzed_authors:
            return 0.0
        
        # –°–æ–±–∏—Ä–∞–µ–º –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–π —Å—Ç–∞—Ç—å–∏
        analyzed_affiliations = set()
        for author in analyzed_authors:
            analyzed_affiliations.update(author.get('affiliation', []))
        
        if not analyzed_affiliations:
            return 0.0
        
        total_citations = len(citing_data)
        self_citations = 0
        
        for cite_result in citing_data.values():
            citing_authors = cite_result.get('authors', [])
            citing_affiliations = set()
            
            for author in citing_authors:
                citing_affiliations.update(author.get('affiliation', []))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—â–∏—Ö –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–π
            common_affiliations = analyzed_affiliations.intersection(citing_affiliations)
            if common_affiliations:
                self_citations += 1
        
        return self_citations / total_citations if total_citations > 0 else 0.0
    
    def _calculate_single_country_concentration(self, citing_data: Dict[str, Dict], 
                                              analyzed_countries: List[str]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –ø–æ —Å—Ç—Ä–∞–Ω–∞–º"""
        if not citing_data:
            return 0.0
        
        country_counter = Counter()
        for cite_result in citing_data.values():
            countries = cite_result.get('countries', [])
            for country in countries:
                if country:
                    country_counter[country] += 1
        
        if not country_counter:
            return 0.0
        
        total_citations = sum(country_counter.values())
        top_country_count = country_counter.most_common(1)[0][1]
        
        return top_country_count / total_citations
    
    def _calculate_citation_velocity(self, result: Dict, citing_data: Dict[str, Dict]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (—Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ –≥–æ–¥)"""
        if not citing_data:
            return 0.0
        
        pub_year_str = result.get('publication_info', {}).get('year', '')
        if not pub_year_str:
            return 0.0
        
        try:
            pub_year = int(pub_year_str)
            current_year = datetime.now().year
            years_passed = max(1, current_year - pub_year)
            
            return len(citing_data) / years_passed
        except:
            return 0.0
    
    def _calculate_first_year_share(self, result: Dict, citing_data: Dict[str, Dict]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –¥–æ–ª—é —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ –ø–µ—Ä–≤—ã–π –≥–æ–¥"""
        if not citing_data:
            return 0.0
        
        pub_year_str = result.get('publication_info', {}).get('year', '')
        if not pub_year_str:
            return 0.0
        
        try:
            pub_year = int(pub_year_str)
            first_year_citations = 0
            total_citations = len(citing_data)
            
            for cite_doi, cite_result in citing_data.items():
                cite_year_str = cite_result.get('publication_info', {}).get('year', '')
                if cite_year_str:
                    try:
                        cite_year = int(cite_year_str)
                        if cite_year == pub_year:
                            first_year_citations += 1
                    except:
                        pass
            
            return first_year_citations / total_citations if total_citations > 0 else 0.0
        except:
            return 0.0
    
    def _check_future_citations(self, result: Dict, citing_data: Dict[str, Dict]) -> int:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ –±—É–¥—É—â–µ–≥–æ"""
        if not citing_data:
            return 0
        
        pub_date_str = result.get('publication_info', {}).get('publication_date', '')
        if not pub_date_str:
            return 0
        
        try:
            pub_date = datetime.strptime(pub_date_str[:10], '%Y-%m-%d')
            future_citations = 0
            
            for cite_result in citing_data.values():
                cite_date_str = cite_result.get('publication_info', {}).get('publication_date', '')
                if cite_date_str:
                    try:
                        cite_date = datetime.strptime(cite_date_str[:10], '%Y-%m-%d')
                        if cite_date < pub_date:
                            future_citations += 1
                    except:
                        pass
            
            return future_citations
        except:
            return 0
    
    def _calculate_quick_risk_score(self, *metrics) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—â–∏–π —Å–∫–æ—Ä–∏–Ω–≥–æ–≤—ã–π —Ä–∏—Å–∫"""
        score = 0
        
        # –í–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        weights = [20, 15, 15, 10, 10, 15, 15]
        
        for metric, weight in zip(metrics, weights):
            if isinstance(metric, float):
                score += int(metric * weight)
            elif isinstance(metric, int):
                score += metric * 5
        
        return min(100, score)
    
    def _determine_recommended_action(self, risk_score: int, red_flags: int) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–∞"""
        if risk_score > 80 or red_flags >= 5:
            return "IMMEDIATE INVESTIGATION"
        elif risk_score > 60 or red_flags >= 3:
            return "DETAILED REVIEW REQUIRED"
        elif risk_score > 40 or red_flags >= 2:
            return "MONITOR AND REVIEW"
        elif risk_score > 20:
            return "MINOR REVIEW SUGGESTED"
        else:
            return "ETHICALLY ACCEPTABLE"
    
    def analyze_medium_insights(self, analyzed_results: Dict[str, Dict], 
                               citing_results: Dict[str, Dict]) -> List[Dict]:
        """–°—Ä–µ–¥–Ω–∏–µ –∏–Ω—Å–∞–π—Ç—ã (15-30 —Å–µ–∫—É–Ω–¥ –Ω–∞ —Å—Ç–∞—Ç—å—é)"""
        medium_insights = []
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∂—É—Ä–Ω–∞–ª–∞–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        journal_stats = self._collect_journal_statistics(analyzed_results, citing_results)
        
        for doi, result in analyzed_results.items():
            if result.get('status') != 'success':
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º —Ü–∏—Ç–∏—Ä—É—é—â–∏–µ —Å—Ç–∞—Ç—å–∏
            citing_dois = result.get('citations', [])
            citing_data = {}
            for cite_doi in citing_dois:
                if cite_doi in citing_results and citing_results[cite_doi].get('status') == 'success':
                    citing_data[cite_doi] = citing_results[cite_doi]
            
            # –ê–Ω–∞–ª–∏–∑
            analysis = self._perform_medium_insight_analysis(doi, result, citing_data, journal_stats)
            medium_insights.append(analysis)
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self.cache.set_ethical_analysis('medium_insights', doi, analysis)
        
        return sorted(medium_insights, key=lambda x: x['Anomaly_Score'], reverse=True)
    
    def _collect_journal_statistics(self, analyzed_results: Dict[str, Dict], 
                                   citing_results: Dict[str, Dict]) -> Dict[str, Dict]:
        """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∂—É—Ä–Ω–∞–ª–∞–º –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
        journal_data = defaultdict(list)
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º —Å—Ç–∞—Ç—å—è–º
        all_results = list(analyzed_results.values()) + list(citing_results.values())
        
        for result in all_results:
            if result.get('status') != 'success':
                continue
            
            pub_info = result.get('publication_info', {})
            journal = pub_info.get('journal', '')
            citation_count = pub_info.get('citation_count_crossref', 0)
            year_str = pub_info.get('year', '')
            
            if journal and year_str:
                try:
                    year = int(year_str)
                    current_year = datetime.now().year
                    age = max(1, current_year - year)
                    annual_citations = citation_count / age
                    
                    journal_data[journal].append({
                        'annual_citations': annual_citations,
                        'citation_count': citation_count,
                        'year': year
                    })
                except:
                    continue
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ–¥–∏–∞–Ω—ã –∏ –∫–≤–∞—Ä—Ç–∏–ª–∏
        journal_stats = {}
        for journal, data_list in journal_data.items():
            if len(data_list) >= 3:  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 3 —Å—Ç–∞—Ç—å–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                annual_citations = [d['annual_citations'] for d in data_list]
                annual_citations.sort()
                
                median_index = len(annual_citations) // 2
                q1_index = len(annual_citations) // 4
                q3_index = 3 * len(annual_citations) // 4

                journal_stats[journal] = {
                    'median_annual_citations': annual_citations[median_index],
                    'q1_annual_citations': annual_citations[q1_index],
                    'q3_annual_citations': annual_citations[q3_index],
                    'count': len(data_list),
                    'min': min(annual_citations),
                    'max': max(annual_citations),
                    # –î–æ–±–∞–≤–ª—è–µ–º 'median' –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º –∫–æ–¥–æ–º
                    'median': annual_citations[median_index]
                }
        
        return journal_stats
    
    def _perform_medium_insight_analysis(self, doi: str, result: Dict, 
                                        citing_data: Dict[str, Dict], 
                                        journal_stats: Dict[str, Dict]) -> Dict:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —Å—Ä–µ–¥–Ω–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
        
        pub_info = result['publication_info']
        authors = result.get('authors', [])
        countries = result.get('countries', [])
        
        # 1. Temporal Citation Pattern
        temporal_pattern = self._analyze_temporal_pattern(result, citing_data)
        
        # 2. Journal Concentration Analysis
        journal_concentration = self._analyze_journal_concentration(citing_data)
        
        # 3. Author Network Analysis
        author_network = self._analyze_author_network(authors, citing_data)
        
        # 4. Geographic Bias Analysis
        geographic_bias = self._analyze_geographic_bias(countries, citing_data)
        
        # 5. Comparison with Journal Norms
        journal_comparison = self._compare_with_journal_norms(pub_info, journal_stats)
        
        # –†–∞—Å—á–µ—Ç –∞–Ω–æ–º–∞–ª—å–Ω–æ–≥–æ —Å–∫–æ—Ä–∞
        anomaly_score = self._calculate_anomaly_score(
            temporal_pattern, journal_concentration, author_network,
            geographic_bias, journal_comparison
        )
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ä–∏—Å–∫–∞
        risk_category, investigation_priority = self._determine_risk_category(anomaly_score)
        
        return {
            'Article_DOI': doi,
            'Publication_Year': pub_info.get('year', ''),
            'Total_Citations': len(citing_data),
            'Annual_Citation_Rate': round(temporal_pattern.get('annual_rate', 0), 2),
            'Citations_Year_1': temporal_pattern.get('year_1', 0),
            'Citations_Year_2': temporal_pattern.get('year_2', 0),
            'First_2_Years_Percent': round(temporal_pattern.get('first_2_years_percent', 0) * 100, 1),
            'Temporal_Anomaly_Index': round(temporal_pattern.get('anomaly_index', 0), 3),
            'Top_Journal_Citing': journal_concentration.get('top_journal', ''),
            'Top_Journal_Percent': round(journal_concentration.get('top_journal_percent', 0) * 100, 1),
            'Journal_Concentration_Index': round(journal_concentration.get('concentration_index', 0), 3),
            'Journal_Diversity_Index': round(journal_concentration.get('diversity_index', 0), 3),
            'Author_Self_Citation_Rate': round(author_network.get('self_citation_rate', 0) * 100, 1),
            'Author_Cluster_Coefficient': round(author_network.get('cluster_coefficient', 0), 3),
            'Author_Network_Density': round(author_network.get('network_density', 0), 3),
            'Top_Country': geographic_bias.get('top_country', ''),
            'Top_Country_Percent': round(geographic_bias.get('top_country_percent', 0) * 100, 1),
            'Country_Diversity_Index': round(geographic_bias.get('diversity_index', 0), 3),
            'Geographic_Bias_Index': round(geographic_bias.get('bias_index', 0), 3),
            'Journal_Median_Annual_Cite': round(journal_comparison.get('journal_median', 0), 2),
            'Citation_Ratio_vs_Journal': round(journal_comparison.get('citation_ratio', 0), 2),
            'Journal_Percentile': round(journal_comparison.get('percentile', 0), 1),
            'Anomaly_Score': round(anomaly_score, 1),
            'Risk_Category': risk_category,
            'Investigation_Priority': investigation_priority,
            'Temporal_Red_Flags': temporal_pattern.get('red_flags', 0),
            'Journal_Red_Flags': journal_concentration.get('red_flags', 0),
            'Author_Red_Flags': author_network.get('red_flags', 0),
            'Geographic_Red_Flags': geographic_bias.get('red_flags', 0)
        }
    
    def _analyze_temporal_pattern(self, result: Dict, citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if not citing_data:
            return {'annual_rate': 0, 'year_1': 0, 'year_2': 0, 
                    'first_2_years_percent': 0, 'anomaly_index': 0, 'red_flags': 0}
        
        pub_year_str = result.get('publication_info', {}).get('year', '')
        if not pub_year_str:
            return {'annual_rate': 0, 'year_1': 0, 'year_2': 0, 
                    'first_2_years_percent': 0, 'anomaly_index': 0, 'red_flags': 0}
        
        try:
            pub_year = int(pub_year_str)
            current_year = datetime.now().year
            years_passed = max(1, current_year - pub_year)
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥–∞–º
            year_distribution = Counter()
            for cite_result in citing_data.values():
                cite_year_str = cite_result.get('publication_info', {}).get('year', '')
                if cite_year_str:
                    try:
                        cite_year = int(cite_year_str)
                        if cite_year >= pub_year:
                            year_distribution[cite_year] += 1
                    except:
                        pass
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            total_citations = len(citing_data)
            annual_rate = total_citations / years_passed
            
            year_1 = year_distribution.get(pub_year, 0)
            year_2 = year_distribution.get(pub_year + 1, 0)
            
            first_2_years = year_1 + year_2
            first_2_years_percent = first_2_years / total_citations if total_citations > 0 else 0
            
            # –ò–Ω–¥–µ–∫—Å –∞–Ω–æ–º–∞–ª–∏–∏ (—á–µ–º –≤—ã—à–µ, —Ç–µ–º –±–æ–ª–µ–µ –∞–Ω–æ–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
            expected_per_year = total_citations / max(1, len(year_distribution))
            anomaly_sum = 0
            for year, count in year_distribution.items():
                if expected_per_year > 0:
                    anomaly_sum += abs(count - expected_per_year) / expected_per_year
            
            anomaly_index = anomaly_sum / len(year_distribution) if year_distribution else 0
            
            # –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏
            red_flags = 0
            if first_2_years_percent > Config.MEDIUM_INSIGHT_THRESHOLDS['first_two_years']:
                red_flags += 1
            if anomaly_index > 0.5:  # –°–∏–ª—å–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                red_flags += 1
            
            return {
                'annual_rate': annual_rate,
                'year_1': year_1,
                'year_2': year_2,
                'first_2_years_percent': first_2_years_percent,
                'anomaly_index': anomaly_index,
                'red_flags': red_flags
            }
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Temporal pattern analysis error: {e}")
            return {'annual_rate': 0, 'year_1': 0, 'year_2': 0, 
                    'first_2_years_percent': 0, 'anomaly_index': 0, 'red_flags': 0}
    
    def _analyze_journal_concentration(self, citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –ø–æ –∂—É—Ä–Ω–∞–ª–∞–º"""
        if not citing_data:
            return {'top_journal': '', 'top_journal_percent': 0, 
                    'concentration_index': 0, 'diversity_index': 0, 'red_flags': 0}
        
        journal_counter = Counter()
        for cite_result in citing_data.values():
            journal = cite_result.get('publication_info', {}).get('journal', '')
            if journal:
                journal_counter[journal] += 1
        
        if not journal_counter:
            return {'top_journal': '', 'top_journal_percent': 0, 
                    'concentration_index': 0, 'diversity_index': 0, 'red_flags': 0}
        
        total_citations = sum(journal_counter.values())
        
        # –¢–æ–ø –∂—É—Ä–Ω–∞–ª –∏ –µ–≥–æ –¥–æ–ª—è
        top_journal, top_count = journal_counter.most_common(1)[0]
        top_journal_percent = top_count / total_citations
        
        # –ò–Ω–¥–µ–∫—Å –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ –•–µ—Ä—Ñ–∏–Ω–¥–∞–ª—è-–•–∏—Ä—à–º–∞–Ω–∞
        hhi = sum((count / total_citations) ** 2 for count in journal_counter.values())
        concentration_index = hhi
        
        # –ò–Ω–¥–µ–∫—Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è (1 - HHI)
        diversity_index = 1 - hhi
        
        # –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏
        red_flags = 0
        if top_journal_percent > Config.MEDIUM_INSIGHT_THRESHOLDS['top_journal_share']:
            red_flags += 1
        if concentration_index > 0.25:  # –í—ã—Å–æ–∫–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è
            red_flags += 1
        
        return {
            'top_journal': top_journal[:50],
            'top_journal_percent': top_journal_percent,
            'concentration_index': concentration_index,
            'diversity_index': diversity_index,
            'red_flags': red_flags
        }
    
    def _analyze_author_network(self, analyzed_authors: List[Dict], 
                               citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Ç—å –∞–≤—Ç–æ—Ä–æ–≤ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π"""
        if not citing_data or not analyzed_authors:
            return {'self_citation_rate': 0, 'cluster_coefficient': 0, 
                    'network_density': 0, 'red_flags': 0}
        
        analyzed_author_names = {author['name'] for author in analyzed_authors}
        
        # –°—Ç—Ä–æ–∏–º —Å–µ—Ç—å –∞–≤—Ç–æ—Ä–æ–≤ —Ü–∏—Ç–∏—Ä—É—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π
        author_network = defaultdict(set)
        all_citing_authors = set()
        
        for cite_result in citing_data.values():
            citing_authors = cite_result.get('authors', [])
            citing_author_names = {author['name'] for author in citing_authors}
            all_citing_authors.update(citing_author_names)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑–∏ –º–µ–∂–¥—É –≤—Å–µ–º–∏ –∞–≤—Ç–æ—Ä–∞–º–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏
            author_list = list(citing_author_names)
            for i in range(len(author_list)):
                for j in range(i + 1, len(author_list)):
                    author_network[author_list[i]].add(author_list[j])
                    author_network[author_list[j]].add(author_list[i])
        
        # Self-citation rate
        total_citations = len(citing_data)
        self_citations = 0
        
        for cite_result in citing_data.values():
            citing_authors = cite_result.get('authors', [])
            citing_author_names = {author['name'] for author in citing_authors}
            
            if analyzed_author_names.intersection(citing_author_names):
                self_citations += 1
        
        self_citation_rate = self_citations / total_citations if total_citations > 0 else 0
        
        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
        if len(author_network) > 0:
            total_possible_connections = len(author_network) * (len(author_network) - 1) / 2
            actual_connections = sum(len(neighbors) for neighbors in author_network.values()) / 2
            
            if total_possible_connections > 0:
                network_density = actual_connections / total_possible_connections
                
                # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
                cluster_coefficient = network_density
            else:
                network_density = 0
                cluster_coefficient = 0
        else:
            network_density = 0
            cluster_coefficient = 0
        
        # –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏
        red_flags = 0
        if self_citation_rate > 0.3:
            red_flags += 1
        if cluster_coefficient > Config.MEDIUM_INSIGHT_THRESHOLDS['cluster_coefficient']:
            red_flags += 1
        
        return {
            'self_citation_rate': self_citation_rate,
            'cluster_coefficient': cluster_coefficient,
            'network_density': network_density,
            'red_flags': red_flags
        }
    
    def _analyze_geographic_bias(self, analyzed_countries: List[str], 
                                citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫—É—é –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å"""
        if not citing_data:
            return {'top_country': '', 'top_country_percent': 0, 
                    'diversity_index': 0, 'bias_index': 0, 'red_flags': 0}
        
        country_counter = Counter()
        for cite_result in citing_data.values():
            countries = cite_result.get('countries', [])
            for country in countries:
                if country:
                    country_counter[country] += 1
        
        if not country_counter:
            return {'top_country': '', 'top_country_percent': 0, 
                    'diversity_index': 0, 'bias_index': 0, 'red_flags': 0}
        
        total_citations = sum(country_counter.values())
        
        # –¢–æ–ø —Å—Ç—Ä–∞–Ω–∞ –∏ –µ–µ –¥–æ–ª—è
        top_country, top_count = country_counter.most_common(1)[0]
        top_country_percent = top_count / total_citations
        
        # –ò–Ω–¥–µ–∫—Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        hhi = sum((count / total_citations) ** 2 for count in country_counter.values())
        diversity_index = 1 - hhi
        
        # –ò–Ω–¥–µ–∫—Å –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏
        # (–¥–æ–ª—è –∏–∑ —Ç–æ–π –∂–µ —Å—Ç—Ä–∞–Ω—ã, —á—Ç–æ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–∞—è —Å—Ç–∞—Ç—å—è)
        same_country_share = 0
        if analyzed_countries:
            for country in analyzed_countries:
                same_country_share += country_counter.get(country, 0) / total_citations
        
        bias_index = same_country_share
        
        # –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏
        red_flags = 0
        if top_country_percent > 0.8:
            red_flags += 1
        if bias_index > Config.MEDIUM_INSIGHT_THRESHOLDS['geographic_bias']:
            red_flags += 1
        
        return {
            'top_country': top_country,
            'top_country_percent': top_country_percent,
            'diversity_index': diversity_index,
            'bias_index': bias_index,
            'red_flags': red_flags
        }
    
    def _compare_with_journal_norms(self, pub_info: Dict, 
                                   journal_stats: Dict[str, Dict]) -> Dict:
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å –Ω–æ—Ä–º–∞–º–∏ –∂—É—Ä–Ω–∞–ª–∞"""
        journal = pub_info.get('journal', '')
        citation_count = pub_info.get('citation_count_crossref', 0)
        year_str = pub_info.get('year', '')
        
        if not journal or not year_str or journal not in journal_stats:
            return {'journal_median': 0, 'citation_ratio': 0, 'percentile': 50}
        
        try:
            year = int(year_str)
            current_year = datetime.now().year
            age = max(1, current_year - year)
            annual_citations = citation_count / age
            
            stats = journal_stats[journal]
            journal_median = stats.get('median_annual_citations', 0.1)
            
            if journal_median > 0:
                citation_ratio = annual_citations / journal_median
            else:
                citation_ratio = 0
            
            # –ü—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∂—É—Ä–Ω–∞–ª—å–Ω—ã—Ö –Ω–æ—Ä–º
            all_citations = [annual_citations]
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∂—É—Ä–Ω–∞–ª—å–Ω—ã—Ö –Ω–æ—Ä–º
            all_citations.append(stats.get('min', annual_citations * 0.5))
            all_citations.append(stats.get('median_annual_citations', annual_citations))
            all_citations.append(stats.get('max', annual_citations * 2))
            all_citations.sort()
            
            position = all_citations.index(annual_citations) + 1
            percentile = (position / len(all_citations)) * 100
            
            return {
                'journal_median': journal_median,
                'citation_ratio': citation_ratio,
                'percentile': percentile
            }
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Journal comparison error: {e}")
            return {'journal_median': 0, 'citation_ratio': 0, 'percentile': 50}
    
    def _calculate_anomaly_score(self, temporal_pattern: Dict, journal_concentration: Dict,
                                author_network: Dict, geographic_bias: Dict, 
                                journal_comparison: Dict) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—â–∏–π –∞–Ω–æ–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä"""
        score = 0
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ (–º–∞–∫—Å 25)
        score += min(25, temporal_pattern.get('anomaly_index', 0) * 50)
        if temporal_pattern.get('first_2_years_percent', 0) > 0.7:
            score += 15
        
        # –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –∂—É—Ä–Ω–∞–ª–æ–≤ (–º–∞–∫—Å 20)
        score += min(20, journal_concentration.get('concentration_index', 0) * 80)
        if journal_concentration.get('top_journal_percent', 0) > 0.6:
            score += 10
        
        # –°–µ—Ç—å –∞–≤—Ç–æ—Ä–æ–≤ (–º–∞–∫—Å 25)
        score += min(25, author_network.get('self_citation_rate', 0) * 83)
        score += min(15, author_network.get('cluster_coefficient', 0) * 20)
        
        # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∞—è –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å (–º–∞–∫—Å 15)
        score += min(15, geographic_bias.get('bias_index', 0) * 30)
        if geographic_bias.get('top_country_percent', 0) > 0.8:
            score += 5
        
        # –û—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –∂—É—Ä–Ω–∞–ª—å–Ω—ã—Ö –Ω–æ—Ä–º (–º–∞–∫—Å 15)
        citation_ratio = journal_comparison.get('citation_ratio', 0)
        if citation_ratio > 3:
            score += 15
        elif citation_ratio > 2:
            score += 10
        elif citation_ratio > 1.5:
            score += 5
        
        return min(100, score)
    
    def _determine_risk_category(self, anomaly_score: float) -> Tuple[str, int]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ä–∏—Å–∫–∞ –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        if anomaly_score > 80:
            return "CRITICAL", 5
        elif anomaly_score > 60:
            return "HIGH", 4
        elif anomaly_score > 40:
            return "MEDIUM", 3
        elif anomaly_score > 20:
            return "LOW", 2
        else:
            return "MINIMAL", 1
    
    def analyze_deep_analysis(self, analyzed_results: Dict[str, Dict], 
                             citing_results: Dict[str, Dict],
                             ref_results: Dict[str, Dict] = None) -> List[Dict]:
        """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ (60-120 —Å–µ–∫—É–Ω–¥ –Ω–∞ —Å—Ç–∞—Ç—å—é)"""
        deep_analysis = []
        
        # –°—Ç—Ä–æ–∏–º –ø–æ–ª–Ω—É—é —Å–µ—Ç—å –¥–ª—è —Å–µ—Ç–µ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        full_network = self._build_citation_network(analyzed_results, citing_results, ref_results)
        
        for doi, result in analyzed_results.items():
            if result.get('status') != 'success':
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            citing_dois = result.get('citations', [])
            citing_data = {}
            for cite_doi in citing_dois:
                if cite_doi in citing_results and citing_results[cite_doi].get('status') == 'success':
                    citing_data[cite_doi] = citing_results[cite_doi]
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑
            analysis = self._perform_deep_analysis(doi, result, citing_data, full_network)
            deep_analysis.append(analysis)
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self.cache.set_ethical_analysis('deep_analysis', doi, analysis)
        
        return sorted(deep_analysis, key=lambda x: x['Machine_Learning_Risk_Score'], reverse=True)
    
    def _build_citation_network(self, analyzed_results: Dict[str, Dict], 
                               citing_results: Dict[str, Dict],
                               ref_results: Dict[str, Dict] = None) -> nx.DiGraph:
        """–°—Ç—Ä–æ–∏—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π"""
        G = nx.DiGraph()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–µ —Å—Ç–∞—Ç—å–∏
        for doi, result in analyzed_results.items():
            if result.get('status') == 'success':
                G.add_node(doi, type='analyzed', 
                          year=result.get('publication_info', {}).get('year', ''))
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ü–∏—Ç–∏—Ä—É—é—â–∏–µ —Å—Ç–∞—Ç—å–∏
        for doi, result in citing_results.items():
            if result.get('status') == 'success':
                G.add_node(doi, type='citing',
                          year=result.get('publication_info', {}).get('year', ''))
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ—Ñ–µ—Ä–µ–Ω—Å—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        if ref_results:
            for doi, result in ref_results.items():
                if result.get('status') == 'success':
                    G.add_node(doi, type='reference',
                              year=result.get('publication_info', {}).get('year', ''))
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–±—Ä–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        for analyzed_doi, result in analyzed_results.items():
            if result.get('status') == 'success':
                citing_dois = result.get('citations', [])
                for cite_doi in citing_dois:
                    if cite_doi in G:
                        G.add_edge(cite_doi, analyzed_doi)  # cite_doi ‚Üí analyzed_doi
        
        return G
    
    def _perform_deep_analysis(self, doi: str, result: Dict, 
                              citing_data: Dict[str, Dict], 
                              citation_network: nx.DiGraph) -> Dict:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
        
        # 1. Network Analysis
        network_metrics = self._analyze_network_metrics(doi, citation_network)
        
        # 2. Temporal Pattern Mining
        temporal_patterns = self._mine_temporal_patterns(result, citing_data)
        
        # 3. Geographic Cluster Analysis
        geographic_clusters = self._analyze_geographic_clusters(result, citing_data)
        
        # 4. Journal Network Analysis
        journal_network = self._analyze_journal_network(result, citing_data)
        
        # 5. Statistical Anomaly Detection
        statistical_anomalies = self._detect_statistical_anomalies(result, citing_data)
        
        # 6. Machine Learning Risk Assessment
        ml_risk_score = self._calculate_ml_risk_score(
            network_metrics, temporal_patterns, geographic_clusters,
            journal_network, statistical_anomalies
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –æ–±–∑–æ—Ä–∞
        expert_review_required = self._determine_expert_review_requirement(
            network_metrics, ml_risk_score, len(citing_data)
        )
        
        return {
            'Article_DOI': doi,
            'Author_Cluster_ID': network_metrics.get('author_cluster_id', 'N/A'),
            'Cluster_Size': network_metrics.get('cluster_size', 0),
            'Internal_Citation_Density': round(network_metrics.get('internal_density', 0), 3),
            'Cross_Cluster_Citations': network_metrics.get('cross_cluster_citations', 0),
            'Betweenness_Centrality': round(network_metrics.get('betweenness_centrality', 0), 4),
            'Clustering_Coefficient': round(network_metrics.get('clustering_coefficient', 0), 3),
            'Eigenvector_Centrality': round(network_metrics.get('eigenvector_centrality', 0), 4),
            'Quarterly_Citation_Peaks': temporal_patterns.get('quarterly_peaks', 0),
            'Seasonal_Pattern_Detected': temporal_patterns.get('seasonal_pattern', False),
            'Citation_Wave_Length': temporal_patterns.get('wave_length', 0),
            'Burst_Detection_Score': round(temporal_patterns.get('burst_score', 0), 3),
            'Geographic_Cluster_Strength': round(geographic_clusters.get('cluster_strength', 0), 3),
            'Cross_Country_Citation_Bias': round(geographic_clusters.get('cross_country_bias', 0), 3),
            'Region_Homophily_Index': round(geographic_clusters.get('homophily_index', 0), 3),
            'Journal_Citation_Circle': journal_network.get('citation_circle', False),
            'Journal_Reciprocity_Index': round(journal_network.get('reciprocity_index', 0), 3),
            'Predatory_Journal_Flags': journal_network.get('predatory_flags', 0),
            'Journal_Network_Modularity': round(journal_network.get('modularity', 0), 3),
            'Citation_Gini_Coefficient': round(statistical_anomalies.get('gini_coefficient', 0), 3),
            'Z_Score_Anomaly': round(statistical_anomalies.get('z_score', 0), 2),
            'Power_Law_Fit': round(statistical_anomalies.get('power_law_fit', 0), 3),
            'Statistical_Outlier_Flag': statistical_anomalies.get('outlier_flag', False),
            'Temporal_Anomaly_Score': round(temporal_patterns.get('temporal_anomaly_score', 0), 1),
            'Network_Centrality_Score': round(network_metrics.get('centrality_score', 0), 1),
            'Pattern_Anomaly_Score': round(temporal_patterns.get('pattern_anomaly_score', 0), 1),
            'Machine_Learning_Risk_Score': round(ml_risk_score, 1),
            'Expert_Review_Required': expert_review_required,
            'Suggested_Audit_Focus': self._suggest_audit_focus(network_metrics, temporal_patterns, 
                                                             geographic_clusters, journal_network),
            'Confidence_Level': self._calculate_confidence_level(len(citing_data), ml_risk_score)
        }
    
    def _analyze_network_metrics(self, doi: str, citation_network: nx.DiGraph) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Ç–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏"""
        if doi not in citation_network:
            return {'author_cluster_id': 'N/A', 'cluster_size': 0, 'internal_density': 0,
                    'cross_cluster_citations': 0, 'betweenness_centrality': 0,
                    'clustering_coefficient': 0, 'eigenvector_centrality': 0,
                    'centrality_score': 0}
        
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏
            betweenness = nx.betweenness_centrality(citation_network, normalized=True).get(doi, 0)
            clustering = nx.clustering(citation_network.to_undirected()).get(doi, 0)
            
            # Eigenvector centrality (—Ç—Ä–µ–±—É–µ—Ç —Å–≤—è–∑–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞)
            try:
                eigenvector = nx.eigenvector_centrality_numpy(citation_network.to_undirected()).get(doi, 0)
            except:
                eigenvector = 0
            
            # –ê–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ—Å—Ç–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º greedy modularity communities
                communities = list(nx.algorithms.community.greedy_modularity_communities(
                    citation_network.to_undirected()))
                
                # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ–±—â–µ—Å—Ç–≤–æ —Ç–µ–∫—É—â–µ–π —Å—Ç–∞—Ç—å–∏
                article_community = None
                for i, community in enumerate(communities):
                    if doi in community:
                        article_community = i
                        break
                
                if article_community is not None:
                    community_nodes = communities[article_community]
                    cluster_size = len(community_nodes)
                    
                    # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞
                    subgraph = citation_network.subgraph(community_nodes)
                    internal_edges = subgraph.number_of_edges()
                    possible_edges = len(community_nodes) * (len(community_nodes) - 1)
                    internal_density = internal_edges / possible_edges if possible_edges > 0 else 0
                    
                    # –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ–∂–¥—É —Å–æ–æ–±—â–µ—Å—Ç–≤–∞–º–∏
                    cross_cluster_citations = 0
                    for node in community_nodes:
                        for neighbor in citation_network.neighbors(node):
                            if neighbor not in community_nodes:
                                cross_cluster_citations += 1
                    
                    author_cluster_id = f"COMM_{article_community:03d}"
                else:
                    cluster_size = 1
                    internal_density = 0
                    cross_cluster_citations = 0
                    author_cluster_id = "ISOLATED"
                    
            except:
                cluster_size = 1
                internal_density = 0
                cross_cluster_citations = 0
                author_cluster_id = "UNKNOWN"
            
            # –û–±—â–∏–π —Å–∫–æ—Ä–∏–Ω–≥ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏
            centrality_score = min(100, (
                betweenness * 40 +
                eigenvector * 30 +
                (1 - clustering) * 30  # –ù–∏–∑–∫–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ = –≤—ã—à–µ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å
            ))
            
            return {
                'author_cluster_id': author_cluster_id,
                'cluster_size': cluster_size,
                'internal_density': internal_density,
                'cross_cluster_citations': cross_cluster_citations,
                'betweenness_centrality': betweenness,
                'clustering_coefficient': clustering,
                'eigenvector_centrality': eigenvector,
                'centrality_score': centrality_score
            }
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Network analysis error for {doi}: {e}")
            return {'author_cluster_id': 'N/A', 'cluster_size': 0, 'internal_density': 0,
                    'cross_cluster_citations': 0, 'betweenness_centrality': 0,
                    'clustering_coefficient': 0, 'eigenvector_centrality': 0,
                    'centrality_score': 0}
    
    def _mine_temporal_patterns(self, result: Dict, citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if not citing_data:
            return {'quarterly_peaks': 0, 'seasonal_pattern': False,
                    'wave_length': 0, 'burst_score': 0,
                    'temporal_anomaly_score': 0, 'pattern_anomaly_score': 0}
        
        pub_date_str = result.get('publication_info', {}).get('publication_date', '')
        if not pub_date_str:
            return {'quarterly_peaks': 0, 'seasonal_pattern': False,
                    'wave_length': 0, 'burst_score': 0,
                    'temporal_anomaly_score': 0, 'pattern_anomaly_score': 0}
        
        try:
            pub_date = datetime.strptime(pub_date_str[:10], '%Y-%m-%d')
            
            # –°–æ–±–∏—Ä–∞–µ–º –¥–∞—Ç—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
            citation_dates = []
            for cite_result in citing_data.values():
                cite_date_str = cite_result.get('publication_info', {}).get('publication_date', '')
                if cite_date_str:
                    try:
                        cite_date = datetime.strptime(cite_date_str[:10], '%Y-%m-%d')
                        if cite_date >= pub_date:  # –¢–æ–ª—å–∫–æ –±—É–¥—É—â–∏–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                            citation_dates.append(cite_date)
                    except:
                        pass
            
            if not citation_dates:
                return {'quarterly_peaks': 0, 'seasonal_pattern': False,
                        'wave_length': 0, 'burst_score': 0,
                        'temporal_anomaly_score': 0, 'pattern_anomaly_score': 0}
            
            # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–≤–∞—Ä—Ç–∞–ª–∞–º
            quarterly_counts = Counter()
            for date in citation_dates:
                quarter = f"{date.year}-Q{(date.month - 1) // 3 + 1}"
                quarterly_counts[quarter] += 1
            
            # –ü–∏–∫–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π (–∫–≤–∞—Ä—Ç–∞–ª—ã —Å >30% –æ—Ç –æ–±—â–µ–≥–æ)
            total_citations = len(citation_dates)
            quarterly_peaks = 0
            for quarter, count in quarterly_counts.items():
                if count / total_citations > 0.3:
                    quarterly_peaks += 1
            
            # –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å (—Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É—é—Ç—Å—è –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –º–µ—Å—è—Ü—ã)
            monthly_counts = Counter()
            for date in citation_dates:
                monthly_counts[date.month] += 1
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å (–±–æ–ª–µ–µ 40% –≤ 2 –º–µ—Å—è—Ü–∞)
            sorted_months = sorted(monthly_counts.items(), key=lambda x: x[1], reverse=True)
            top_2_months_share = sum(count for _, count in sorted_months[:2]) / total_citations
            seasonal_pattern = top_2_months_share > 0.4
            
            # –î–ª–∏–Ω–∞ "–≤–æ–ª–Ω—ã" —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π (–≤ –¥–Ω—è—Ö)
            if len(citation_dates) >= 2:
                citation_dates.sort()
                time_spread = (citation_dates[-1] - citation_dates[0]).days
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ –≤–æ–ª–Ω—ã (0-1)
                if time_spread > 0:
                    wave_length = min(1.0, total_citations / (time_spread / 30.44))  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ –º–µ—Å—è—Ü–∞–º
                else:
                    wave_length = 1.0
            else:
                wave_length = 0
            
            # –û—Ü–µ–Ω–∫–∞ "burst" –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            if len(citation_dates) >= 3:
                # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è–º–∏
                citation_dates.sort()
                time_diffs = []
                for i in range(1, len(citation_dates)):
                    diff = (citation_dates[i] - citation_dates[i-1]).days
                    time_diffs.append(diff)
                
                if time_diffs:
                    avg_diff = sum(time_diffs) / len(time_diffs)
                    # Burst score: —á–µ–º –±–æ–ª—å—à–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ, —Ç–µ–º –≤—ã—à–µ
                    burst_variance = sum((d - avg_diff) ** 2 for d in time_diffs) / len(time_diffs)
                    burst_score = min(1.0, burst_variance / (avg_diff ** 2) if avg_diff > 0 else 0)
                else:
                    burst_score = 0
            else:
                burst_score = 0
            
            # –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–æ–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä
            temporal_anomaly_score = min(100, (
                quarterly_peaks * 20 +
                (1 if seasonal_pattern else 0) * 30 +
                wave_length * 25 +
                burst_score * 25
            ))
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω –∞–Ω–æ–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä
            pattern_anomaly_score = min(100, (
                (quarterly_peaks / max(1, len(quarterly_counts))) * 40 +
                top_2_months_share * 30 +
                burst_score * 30
            ))
            
            return {
                'quarterly_peaks': quarterly_peaks,
                'seasonal_pattern': seasonal_pattern,
                'wave_length': round(wave_length, 3),
                'burst_score': round(burst_score, 3),
                'temporal_anomaly_score': temporal_anomaly_score,
                'pattern_anomaly_score': pattern_anomaly_score
            }
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Temporal pattern mining error: {e}")
            return {'quarterly_peaks': 0, 'seasonal_pattern': False,
                    'wave_length': 0, 'burst_score': 0,
                    'temporal_anomaly_score': 0, 'pattern_anomaly_score': 0}
    
    def _analyze_geographic_clusters(self, result: Dict, citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã"""
        if not citing_data:
            return {'cluster_strength': 0, 'cross_country_bias': 0,
                    'homophily_index': 0}
        
        analyzed_countries = set(result.get('countries', []))
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞–Ω—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        citation_countries = []
        country_counter = Counter()
        
        for cite_result in citing_data.values():
            countries = cite_result.get('countries', [])
            citation_countries.append(set(countries))
            for country in countries:
                if country:
                    country_counter[country] += 1
        
        if not country_counter:
            return {'cluster_strength': 0, 'cross_country_bias': 0,
                    'homophily_index': 0}
        
        total_citations = len(citation_countries)
        
        # –°–∏–ª–∞ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        # (–¥–æ–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–π —Å—Ç—Ä–∞–Ω—ã)
        top_country_share = country_counter.most_common(1)[0][1] / total_citations
        cluster_strength = top_country_share
        
        # –ú–µ–∂—Å—Ç—Ä–∞–Ω–æ–≤–æ–π bias (–¥–æ–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –∏–∑ —Ç–µ—Ö –∂–µ —Å—Ç—Ä–∞–Ω)
        same_country_citations = 0
        for countries in citation_countries:
            if analyzed_countries.intersection(countries):
                same_country_citations += 1
        
        cross_country_bias = same_country_citations / total_citations if total_citations > 0 else 0
        
        # –ò–Ω–¥–µ–∫—Å –≥–æ–º–æ—Ñ–∏–ª–∏–∏ (–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ —Å–≤–æ–µ–π –≥—Ä—É–ø–ø—ã)
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–∞–∫ –¥–æ–ª—è –≤–Ω—É—Ç—Ä–∏–≥—Ä—É–ø–ø–æ–≤—ã—Ö —Å–≤—è–∑–µ–π
        homophily_index = cross_country_bias
        
        return {
            'cluster_strength': round(cluster_strength, 3),
            'cross_country_bias': round(cross_country_bias, 3),
            'homophily_index': round(homophily_index, 3)
        }
    
    def _analyze_journal_network(self, result: Dict, citing_data: Dict[str, Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Ç—å –∂—É—Ä–Ω–∞–ª–æ–≤"""
        if not citing_data:
            return {'citation_circle': False, 'reciprocity_index': 0,
                    'predatory_flags': 0, 'modularity': 0}
        
        analyzed_journal = result.get('publication_info', {}).get('journal', '')
        
        # –°–æ–±–∏—Ä–∞–µ–º –∂—É—Ä–Ω–∞–ª—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        journal_counter = Counter()
        journal_pairs = set()
        
        for cite_result in citing_data.values():
            journal = cite_result.get('publication_info', {}).get('journal', '')
            if journal:
                journal_counter[journal] += 1
                
                # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –ø–∞—Ä—É –∂—É—Ä–Ω–∞–ª–æ–≤
                if analyzed_journal and journal != analyzed_journal:
                    journal_pair = tuple(sorted([analyzed_journal, journal]))
                    journal_pairs.add(journal_pair)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ü–∏—Ç–∞—Ç–Ω—ã–µ –∫—Ä—É–≥–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        # –ï—Å–ª–∏ –µ—Å—Ç—å –≤–∑–∞–∏–º–Ω–æ–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ–∂–¥—É –Ω–µ–±–æ–ª—å—à–∏–º –Ω–∞–±–æ—Ä–æ–º –∂—É—Ä–Ω–∞–ª–æ–≤
        citation_circle = False
        if len(journal_counter) <= 3 and sum(journal_counter.values()) > 5:
            # –ú–∞–ª–æ –∂—É—Ä–Ω–∞–ª–æ–≤, –º–Ω–æ–≥–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
            citation_circle = True
        
        # –ò–Ω–¥–µ–∫—Å –≤–∑–∞–∏–º–Ω–æ—Å—Ç–∏ (—Å–∫–æ–ª—å–∫–æ –∂—É—Ä–Ω–∞–ª–æ–≤ –∏–º–µ—é—Ç –æ–±—Ä–∞—Ç–Ω—ã–µ —Å–≤—è–∑–∏)
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç
        total_journals = len(journal_counter)
        if total_journals > 0:
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –µ—Å–ª–∏ –∂—É—Ä–Ω–∞–ª —Ü–∏—Ç–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—å—é, 
            # —Ç–æ –≤–æ–∑–º–æ–∂–Ω–∞ –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –≤ –¥—Ä—É–≥–∏—Ö —Å—Ç–∞—Ç—å—è—Ö
            reciprocity_index = min(1.0, total_journals / 10)
        else:
            reciprocity_index = 0
        
        # –§–ª–∞–≥–∏ —Ö–∏—â–Ω–∏—á–µ—Å–∫–∏—Ö –∂—É—Ä–Ω–∞–ª–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é)
        predatory_keywords = ['international journal', 'advances in', 'research journal',
                            'journal of', 'annals of', 'archives of', 'european journal']
        
        predatory_flags = 0
        for journal in journal_counter:
            journal_lower = journal.lower()
            for keyword in predatory_keywords:
                if keyword in journal_lower:
                    predatory_flags += 1
                    break
        
        # –ú–æ–¥—É–ª—è—Ä–Ω–æ—Å—Ç—å —Å–µ—Ç–∏ –∂—É—Ä–Ω–∞–ª–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–∞–∫ 1 - (–¥–æ–ª—è –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞)
        if journal_counter:
            top_journal_share = journal_counter.most_common(1)[0][1] / sum(journal_counter.values())
            modularity = 1 - top_journal_share
        else:
            modularity = 0
        
        return {
            'citation_circle': citation_circle,
            'reciprocity_index': round(reciprocity_index, 3),
            'predatory_flags': predatory_flags,
            'modularity': round(modularity, 3)
        }
    
    def _detect_statistical_anomalies(self, result: Dict, citing_data: Dict[str, Dict]) -> Dict:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏"""
        if not citing_data:
            return {'gini_coefficient': 0, 'z_score': 0,
                    'power_law_fit': 0, 'outlier_flag': False}
        
        # –°–æ–±–∏—Ä–∞–µ–º –≥–æ–¥—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        citation_years = []
        for cite_result in citing_data.values():
            year_str = cite_result.get('publication_info', {}).get('year', '')
            if year_str:
                try:
                    year = int(year_str)
                    citation_years.append(year)
                except:
                    pass
        
        if not citation_years:
            return {'gini_coefficient': 0, 'z_score': 0,
                    'power_law_fit': 0, 'outlier_flag': False}
        
        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–∏ –¥–ª—è –Ω–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        citation_years.sort()
        n = len(citation_years)
        
        if n > 1:
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥–∞–º
            year_counts = Counter(citation_years)
            values = list(year_counts.values())
            values.sort()
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–∏
            cum_values = np.cumsum(values).astype(float)
            gini = (n + 1 - 2 * np.sum(cum_values) / cum_values[-1]) / n
        else:
            gini = 0
        
        # Z-score –¥–ª—è –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        pub_year_str = result.get('publication_info', {}).get('year', '')
        if pub_year_str and len(citation_years) >= 3:
            try:
                pub_year = int(pub_year_str)
                current_year = datetime.now().year
                
                # –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ –≥–æ–¥
                year_range = range(pub_year, current_year + 1)
                citations_per_year = []
                
                for year in year_range:
                    count = citation_years.count(year)
                    citations_per_year.append(count)
                
                mean_citations = np.mean(citations_per_year)
                std_citations = np.std(citations_per_year)
                
                if std_citations > 0:
                    # Z-score –¥–ª—è –≥–æ–¥–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
                    max_year_count = max(citations_per_year)
                    z_score = (max_year_count - mean_citations) / std_citations
                else:
                    z_score = 0
            except:
                z_score = 0
        else:
            z_score = 0
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ power-law —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ª–µ—Ç —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        if len(citation_years) >= 5:
            year_counts = Counter(citation_years)
            sorted_counts = sorted(year_counts.values(), reverse=True)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É–±—ã–≤–∞–µ—Ç –ª–∏ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ
            if len(sorted_counts) >= 3:
                ratios = []
                for i in range(len(sorted_counts) - 1):
                    if sorted_counts[i+1] > 0:
                        ratios.append(sorted_counts[i] / sorted_counts[i+1])
                
                if ratios:
                    avg_ratio = np.mean(ratios)
                    # –ß–µ–º –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ, —Ç–µ–º –±–ª–∏–∂–µ –∫ power-law
                    power_law_fit = min(1.0, avg_ratio / 3)
                else:
                    power_law_fit = 0
            else:
                power_law_fit = 0
        else:
            power_law_fit = 0
        
        # –§–ª–∞–≥ –≤—ã–±—Ä–æ—Å–∞
        outlier_flag = (z_score > 3) or (gini > 0.7) or (power_law_fit > 0.8)
        
        return {
            'gini_coefficient': round(gini, 3),
            'z_score': round(z_score, 2),
            'power_law_fit': round(power_law_fit, 3),
            'outlier_flag': outlier_flag
        }
    
    def _calculate_ml_risk_score(self, network_metrics: Dict, temporal_patterns: Dict,
                                geographic_clusters: Dict, journal_network: Dict,
                                statistical_anomalies: Dict) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç ML-based —Ä–∏—Å–∫ —Å–∫–æ—Ä–∏–Ω–≥"""
        score = 0
        
        # –°–µ—Ç–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–º–∞–∫—Å 30)
        score += min(30, network_metrics.get('centrality_score', 0) * 0.3)
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–º–∞–∫—Å 25)
        score += min(25, temporal_patterns.get('temporal_anomaly_score', 0) * 0.25)
        
        # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (–º–∞–∫—Å 20)
        cluster_strength = geographic_clusters.get('cluster_strength', 0)
        cross_country_bias = geographic_clusters.get('cross_country_bias', 0)
        score += min(20, (cluster_strength + cross_country_bias) * 10)
        
        # –°–µ—Ç—å –∂—É—Ä–Ω–∞–ª–æ–≤ (–º–∞–∫—Å 15)
        if journal_network.get('citation_circle', False):
            score += 10
        score += min(5, journal_network.get('predatory_flags', 0) * 2.5)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏ (–º–∞–∫—Å 10)
        if statistical_anomalies.get('outlier_flag', False):
            score += 10
        
        return min(100, score)
    
    def _determine_expert_review_requirement(self, network_metrics: Dict, 
                                           ml_risk_score: float, 
                                           citation_count: int) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ª–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ"""
        if ml_risk_score > 70:
            return True
        
        if citation_count > 50 and network_metrics.get('centrality_score', 0) > 60:
            return True
        
        if network_metrics.get('cluster_size', 0) > 20:
            return True
        
        return False
    
    def _suggest_audit_focus(self, network_metrics: Dict, temporal_patterns: Dict,
                           geographic_clusters: Dict, journal_network: Dict) -> str:
        """–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ñ–æ–∫—É—Å –¥–ª—è –∞—É–¥–∏—Ç–∞"""
        factors = []
        
        if network_metrics.get('centrality_score', 0) > 60:
            factors.append(('Network', network_metrics.get('centrality_score', 0)))
        
        if temporal_patterns.get('temporal_anomaly_score', 0) > 60:
            factors.append(('Temporal', temporal_patterns.get('temporal_anomaly_score', 0)))
        
        if geographic_clusters.get('cluster_strength', 0) > 0.7:
            factors.append(('Geographic', geographic_clusters.get('cluster_strength', 0) * 100))
        
        if journal_network.get('citation_circle', False):
            factors.append(('Journal', 100))
        
        if factors:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é –∏ –±–µ—Ä–µ–º —Ç–æ–ø
            factors.sort(key=lambda x: x[1], reverse=True)
            return factors[0][0]
        else:
            return 'Normal'
    
    def _calculate_confidence_level(self, citation_count: int, 
                                  ml_risk_score: float) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –æ—Ü–µ–Ω–∫–µ"""
        if citation_count == 0:
            return 50
        
        base_confidence = min(90, citation_count * 2)
        
        if ml_risk_score > 80:
            # –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ = –≤—ã—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏
            confidence = min(95, base_confidence + 10)
        elif ml_risk_score > 60:
            confidence = min(90, base_confidence + 5)
        elif ml_risk_score > 40:
            confidence = base_confidence
        elif ml_risk_score > 20:
            confidence = max(60, base_confidence - 10)
        else:
            confidence = max(50, base_confidence - 20)
        
        return confidence
    
    def analyze_citing_relationships(self, analyzed_results: Dict[str, Dict], 
                                   citing_results: Dict[str, Dict]) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–≤—è–∑–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–µ ‚Üî —Ü–∏—Ç–∏—Ä—É—é—â–∏–µ (30-60 —Å–µ–∫)"""
        relationships = []
        
        # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ –¥–ª—è —Å–µ—Ç–µ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        citation_graph = self._build_citation_network(analyzed_results, citing_results)
        
        for analyzed_doi, analyzed_result in analyzed_results.items():
            if analyzed_result.get('status') != 'success':
                continue
            
            citing_dois = analyzed_result.get('citations', [])
            
            for citing_doi in citing_dois:
                if citing_doi in citing_results and citing_results[citing_doi].get('status') == 'success':
                    citing_result = citing_results[citing_doi]
                    
                    # –ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–∏
                    analysis = self._perform_relationship_analysis(
                        analyzed_doi, analyzed_result, 
                        citing_doi, citing_result, 
                        citation_graph
                    )
                    
                    relationships.append(analysis)
        
        return sorted(relationships, key=lambda x: x['Gift_Citation_Probability'], reverse=True)
    
    def _perform_relationship_analysis(self, analyzed_doi: str, analyzed_result: Dict,
                                     citing_doi: str, citing_result: Dict,
                                     citation_graph: nx.DiGraph) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç–∞—Ç—å—è–º–∏"""
        
        # 1. –í—Ä–µ–º–µ–Ω–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞
        time_diff = self._calculate_time_difference(analyzed_result, citing_result)
        
        # 2. –û–±—â–∏–µ –∞–≤—Ç–æ—Ä—ã
        common_authors = self._find_common_authors(analyzed_result, citing_result)
        
        # 3. –û–±—â–∏–µ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏
        common_affiliations = self._find_common_affiliations(analyzed_result, citing_result)
        
        # 4. –°–µ—Ç–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        network_metrics = self._calculate_relationship_network_metrics(
            analyzed_doi, citing_doi, citation_graph
        )
        
        # 5. –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å "–ø–æ–¥–∞—Ä–æ—á–Ω–æ–≥–æ" —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        gift_probability = self._calculate_gift_citation_probability(
            time_diff, common_authors, common_affiliations, network_metrics
        )
        
        # 6. –†–æ–ª—å –≤ —Å–µ—Ç–∏
        network_role = self._determine_network_role(analyzed_doi, citing_doi, citation_graph)
        
        # 7. –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
        time_sync = self._calculate_time_synchronization(analyzed_result, citing_result, citation_graph)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Ä–∏—Å–∫–∞
        relationship_risk, action_required = self._determine_relationship_risk(gift_probability)
        
        return {
            'Analyzed_DOI': analyzed_doi,
            'Citing_DOI': citing_doi,
            'Time_Difference_Days': time_diff,
            'Same_Authors': len(common_authors),
            'Same_Affiliations': len(common_affiliations),
            'Common_Authors_List': '; '.join(common_authors),
            'Common_Affiliations_List': '; '.join(common_affiliations),
            'Connection_Strength': network_metrics.get('connection_strength', 0),
            'Reciprocity_Flag': network_metrics.get('reciprocity', False),
            'Temporal_Anomaly': self._determine_temporal_anomaly(time_diff),
            'Group_Citation_Cluster': network_metrics.get('cluster_id', 'N/A'),
            'Cluster_Size': network_metrics.get('cluster_size', 1),
            'Intra_Cluster_Density': round(network_metrics.get('intra_cluster_density', 0), 3),
            'Citation_Wave_Position': network_metrics.get('wave_position', 'Normal'),
            'Time_Sync_Score': round(time_sync, 3),
            'Batch_Citation_Flag': network_metrics.get('batch_citation', False),
            'Bridge_Role': network_role,
            'Betweenness_Centrality': round(network_metrics.get('betweenness', 0), 4),
            'Clustering_Coefficient': round(network_metrics.get('clustering', 0), 3),
            'Gift_Citation_Probability': round(gift_probability, 1),
            'Citation_Circle_Member': network_metrics.get('citation_circle', False),
            'Artificial_Boost_Flag': gift_probability > 70,
            'Journal_Pair_Frequency': network_metrics.get('journal_pair_freq', 1),
            'Country_Pair': self._create_country_pair(analyzed_result, citing_result),
            'Aff_Pair_Strength': len(common_affiliations),
            'Relationship_Risk': relationship_risk,
            'Action_Required': action_required,
            'Notes': self._generate_relationship_notes(
                common_authors, common_affiliations, time_diff, gift_probability
            )
        }
    
    def _calculate_time_difference(self, analyzed_result: Dict, citing_result: Dict) -> Optional[int]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Ä–∞–∑–Ω–∏—Ü—É –≤ –¥–Ω—è—Ö"""
        analyzed_date_str = analyzed_result.get('publication_info', {}).get('publication_date', '')
        citing_date_str = citing_result.get('publication_info', {}).get('publication_date', '')
        
        if not analyzed_date_str or not citing_date_str:
            return None
        
        try:
            analyzed_date = datetime.strptime(analyzed_date_str[:10], '%Y-%m-%d')
            citing_date = datetime.strptime(citing_date_str[:10], '%Y-%m-%d')
            
            return (citing_date - analyzed_date).days
        except:
            return None
    
    def _find_common_authors(self, analyzed_result: Dict, citing_result: Dict) -> Set[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–±—â–∏—Ö –∞–≤—Ç–æ—Ä–æ–≤"""
        analyzed_authors = {author['name'] for author in analyzed_result.get('authors', [])}
        citing_authors = {author['name'] for author in citing_result.get('authors', [])}
        
        return analyzed_authors.intersection(citing_authors)
    
    def _find_common_affiliations(self, analyzed_result: Dict, citing_result: Dict) -> Set[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–±—â–∏–µ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏"""
        analyzed_affiliations = set()
        for author in analyzed_result.get('authors', []):
            analyzed_affiliations.update(author.get('affiliation', []))
        
        citing_affiliations = set()
        for author in citing_result.get('authors', []):
            citing_affiliations.update(author.get('affiliation', []))
        
        return analyzed_affiliations.intersection(citing_affiliations)
    
    def _calculate_relationship_network_metrics(self, analyzed_doi: str, citing_doi: str,
                                              citation_graph: nx.DiGraph) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ—Ç–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å–≤—è–∑–∏"""
        metrics = {
            'connection_strength': 1,
            'reciprocity': False,
            'cluster_id': 'N/A',
            'cluster_size': 1,
            'intra_cluster_density': 0,
            'wave_position': 'Normal',
            'batch_citation': False,
            'betweenness': 0,
            'clustering': 0,
            'citation_circle': False,
            'journal_pair_freq': 1
        }
        
        if analyzed_doi not in citation_graph or citing_doi not in citation_graph:
            return metrics
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∑–∞–∏–º–Ω–æ—Å—Ç–∏
            if citation_graph.has_edge(analyzed_doi, citing_doi):
                metrics['reciprocity'] = True
            
            # –°–∏–ª–∞ —Å–≤—è–∑–∏ (–Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–µ–π)
            try:
                betweenness = nx.betweenness_centrality(citation_graph, normalized=True)
                metrics['betweenness'] = betweenness.get(analyzed_doi, 0) + betweenness.get(citing_doi, 0)
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å–∏–ª–∞ —Å–≤—è–∑–∏
                metrics['connection_strength'] = min(10, int(metrics['betweenness'] * 20 + 1))
            except:
                pass
            
            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            try:
                undirected_graph = citation_graph.to_undirected()
                clustering = nx.clustering(undirected_graph)
                metrics['clustering'] = (clustering.get(analyzed_doi, 0) + clustering.get(citing_doi, 0)) / 2
            except:
                pass
            
            # –ê–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ—Å—Ç–≤
            try:
                communities = list(nx.algorithms.community.greedy_modularity_communities(
                    citation_graph.to_undirected()))
                
                # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ–±—â–µ—Å—Ç–≤–æ
                for i, community in enumerate(communities):
                    if analyzed_doi in community and citing_doi in community:
                        metrics['cluster_id'] = f"CLUSTER_{i:03d}"
                        metrics['cluster_size'] = len(community)
                        
                        # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞
                        subgraph = citation_graph.subgraph(community)
                        possible_edges = len(community) * (len(community) - 1)
                        if possible_edges > 0:
                            metrics['intra_cluster_density'] = subgraph.number_of_edges() / possible_edges
                        
                        break
            except:
                pass
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ batch citation
            # (–º–Ω–æ–≥–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ –∫–æ—Ä–æ—Ç–∫–∏–π –ø–µ—Ä–∏–æ–¥)
            analyzed_neighbors = list(citation_graph.predecessors(analyzed_doi))
            if len(analyzed_neighbors) > 10:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≥—Ä—É–ø–ø—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π —Å –±–ª–∏–∑–∫–∏–º–∏ –¥–∞—Ç–∞–º–∏
                metrics['batch_citation'] = True
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ü–∏—Ç–∞—Ç–Ω—ã–µ –∫—Ä—É–≥–∏
            try:
                # –ò—â–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ü–∏–∫–ª—ã
                for path in nx.all_simple_paths(citation_graph, citing_doi, analyzed_doi, cutoff=3):
                    if len(path) <= 3:
                        metrics['citation_circle'] = True
                        break
            except:
                pass
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Network metrics error for {analyzed_doi}-{citing_doi}: {e}")
        
        return metrics
    
    def _calculate_gift_citation_probability(self, time_diff: Optional[int],
                                           common_authors: Set[str],
                                           common_affiliations: Set[str],
                                           network_metrics: Dict) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å "–ø–æ–¥–∞—Ä–æ—á–Ω–æ–≥–æ" —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        probability = 0
        
        # –û–±—â–∏–µ –∞–≤—Ç–æ—Ä—ã (—Å–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª)
        if common_authors:
            probability += min(50, len(common_authors) * 20)
        
        # –û–±—â–∏–µ –∞—Ñ—Ñ–∏–ª–∏–∞—Ü–∏–∏ (—Å—Ä–µ–¥–Ω–∏–π —Å–∏–≥–Ω–∞–ª)
        if common_affiliations:
            probability += min(40, len(common_affiliations) * 15)
        
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å (—Å–ª–∞–±—ã–π —Å–∏–≥–Ω–∞–ª)
        if time_diff is not None:
            if abs(time_diff) < 30:  # –ú–µ–Ω—å—à–µ –º–µ—Å—è—Ü–∞
                probability += 20
            elif abs(time_diff) < 90:  # –ú–µ–Ω—å—à–µ 3 –º–µ—Å—è—Ü–µ–≤
                probability += 10
        
        # –°–µ—Ç–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if network_metrics.get('reciprocity', False):
            probability += 15
        
        if network_metrics.get('citation_circle', False):
            probability += 20
        
        if network_metrics.get('batch_citation', False):
            probability += 10
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        return min(100, probability)
    
    def _determine_network_role(self, analyzed_doi: str, citing_doi: str,
                               citation_graph: nx.DiGraph) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–æ–ª—å –≤ —Å–µ—Ç–∏"""
        if analyzed_doi not in citation_graph or citing_doi not in citation_graph:
            return "Normal"
        
        try:
            # –°—Ç–µ–ø–µ–Ω–∏ –≤–µ—Ä—à–∏–Ω
            analyzed_in_degree = citation_graph.in_degree(analyzed_doi)
            analyzed_out_degree = citation_graph.out_degree(analyzed_doi)
            citing_in_degree = citation_graph.in_degree(citing_doi)
            citing_out_degree = citation_graph.out_degree(citing_doi)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–æ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–µ–ø–µ–Ω–µ–π
            if analyzed_in_degree > 10 or citing_in_degree > 10:
                return "Hub"
            elif analyzed_out_degree > 5 or citing_out_degree > 5:
                return "Connector"
            else:
                return "Normal"
                
        except:
            return "Normal"
    
    def _calculate_time_synchronization(self, analyzed_result: Dict, citing_result: Dict,
                                      citation_graph: nx.DiGraph) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç
        time_diff = self._calculate_time_difference(analyzed_result, citing_result)
        
        if time_diff is None:
            return 0.5
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if abs(time_diff) < 30:
            return 0.8  # –í—ã—Å–æ–∫–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
        elif abs(time_diff) < 90:
            return 0.6  # –°—Ä–µ–¥–Ω—è—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
        elif abs(time_diff) < 365:
            return 0.4  # –ù–∏–∑–∫–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
        else:
            return 0.2  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
    
    def _determine_temporal_anomaly(self, time_diff: Optional[int]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é –∞–Ω–æ–º–∞–ª–∏—é"""
        if time_diff is None:
            return "Unknown"
        
        if time_diff < 0:
            return "Future citation"
        elif time_diff < 30:
            return "Rapid citation"
        elif time_diff < 90:
            return "Prompt citation"
        else:
            return "Normal"
    
    def _create_country_pair(self, analyzed_result: Dict, citing_result: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –ø–∞—Ä—ã —Å—Ç—Ä–∞–Ω"""
        analyzed_countries = analyzed_result.get('countries', [''])[:1]
        citing_countries = citing_result.get('countries', [''])[:1]
        
        analyzed_country = analyzed_countries[0] if analyzed_countries else 'Unknown'
        citing_country = citing_countries[0] if citing_countries else 'Unknown'
        
        return f"{analyzed_country}‚Üí{citing_country}"
    
    def _determine_relationship_risk(self, gift_probability: float) -> Tuple[str, str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞ —Å–≤—è–∑–∏"""
        if gift_probability > 80:
            return "CRITICAL", "IMMEDIATE VALIDATION REQUIRED"
        elif gift_probability > 60:
            return "HIGH", "DETAILED REVIEW REQUIRED"
        elif gift_probability > 40:
            return "MEDIUM", "MONITOR AND REVIEW"
        elif gift_probability > 20:
            return "LOW", "MINOR REVIEW SUGGESTED"
        else:
            return "MINIMAL", "ETHICALLY ACCEPTABLE"
    
    def _generate_relationship_notes(self, common_authors: Set[str],
                                   common_affiliations: Set[str],
                                   time_diff: Optional[int],
                                   gift_probability: float) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–º–µ—Ç–∫–∏ –æ —Å–≤—è–∑–∏"""
        notes = []
        
        if common_authors:
            notes.append(f"Common authors: {len(common_authors)}")
        
        if common_affiliations:
            notes.append(f"Common affiliations: {len(common_affiliations)}")
        
        if time_diff is not None:
            if time_diff < 0:
                notes.append(f"Future citation: {abs(time_diff)} days before")
            else:
                notes.append(f"Time gap: {time_diff} days")
        
        notes.append(f"Gift citation probability: {gift_probability:.1f}%")
        
        return "; ".join(notes)

# ============================================================================
# üìä –ö–õ–ê–°–° –≠–ö–°–ü–û–†–¢–ê –í EXCEL (–£–õ–£–ß–®–ï–ù–ù–´–ô –° –ù–û–í–´–ú–ò –§–£–ù–ö–¶–ò–Ø–ú–ò)
# ============================================================================

class ExcelExporter:
    def __init__(self, data_processor: DataProcessor, ror_client: RORClient,
                 failed_tracker: FailedDOITracker):
        self.processor = data_processor
        self.ror_client = ror_client
        self.failed_tracker = failed_tracker
        
        self.references_counter = Counter()
        self.citations_counter = Counter()
        self.ref_references_counter = Counter()
        self.ref_citations_counter = Counter()
        self.cite_references_counter = Counter()
        self.cite_citations_counter = Counter()
        
        self.analyzed_results = {}
        self.ref_results = {}
        self.citing_results = {}
        
        self.doi_to_source_counts = defaultdict(lambda: defaultdict(int))
        self.source_dois = {
            'analyzed': set(),
            'ref': set(),
            'citing': set()
        }
        
        self.ref_to_analyzed = defaultdict(list)
        self.analyzed_to_citing = defaultdict(list)
        
        self.author_stats = defaultdict(lambda: {
            'normalized_name': '',
            'orcid': '',
            'affiliation': '',
            'country': '',
            'total_count': 0,
            'normalized_analyzed': 0,
            'normalized_reference': 0,
            'normalized_citing': 0
        })
        
        self.affiliation_stats = defaultdict(lambda: {
            'colab_id': '',
            'website': '',
            'countries': [],
            'total_count': 0,
            'normalized_analyzed': 0,
            'normalized_reference': 0,
            'normalized_citing': 0
        })
        
        self.affiliation_country_stats = defaultdict(lambda: defaultdict(int))
        self.current_year = datetime.now().year
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        self.hierarchical_analyzer = None
    
    def set_hierarchical_analyzer(self, hierarchical_analyzer: HierarchicalDataAnalyzer):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        self.hierarchical_analyzer = hierarchical_analyzer
    
    def _correct_country_for_author(self, author_key: str, affiliation_stats: Dict[str, Any]) -> str:
        """Correct country for author based on affiliation statistics"""
        author_info = self.author_stats[author_key]
        if not author_info['affiliation']:
            return author_info['country']
        
        affiliation = author_info['affiliation']
        if affiliation in affiliation_stats and affiliation_stats[affiliation]['countries']:
            countries = affiliation_stats[affiliation]['countries']
            if countries:
                country_counter = Counter(countries)
                most_common_country = country_counter.most_common(1)[0][0]
                
                if author_info['country'] != most_common_country:
                    website = affiliation_stats[affiliation].get('website', '')
                    if website:
                        domain_match = re.search(r'\.([a-z]{2,3})$', website.lower())
                        if domain_match:
                            domain_zone = domain_match.group(1).upper()
                            domain_to_country = {
                                'RU': 'RU', 'SU': 'RU',
                                'US': 'US', 'COM': 'US', 'ORG': 'US', 'NET': 'US',
                                'UK': 'GB', 'GB': 'GB', 'CO.UK': 'GB',
                                'DE': 'DE', 'FR': 'FR', 'IT': 'IT', 'ES': 'ES',
                                'CN': 'CN', 'JP': 'JP', 'KR': 'KR', 'IN': 'IN',
                                'AU': 'AU', 'CA': 'CA', 'BR': 'BR', 'MX': 'MX'
                            }
                            
                            if domain_zone in domain_to_country:
                                website_country = domain_to_country[domain_zone]
                                if website_country == most_common_country:
                                    return most_common_country
                    
                    if len(countries) >= 3:
                        country_freq = country_counter[most_common_country] / len(countries)
                        if country_freq >= 0.7:
                            return most_common_country
        
        return author_info['country']
    
    def _calculate_annual_citation_rate(self, citation_count: int, publication_year_str: str) -> float:
        """Calculate average annual citations"""
        if not citation_count or not publication_year_str:
            return 0.0
        
        try:
            pub_year = int(publication_year_str)
            age = self.current_year - pub_year + 1
            if age <= 0:
                age = 1
            
            return citation_count / age
        except:
            return 0.0
    
    def _analyze_ethical_insights(self, analysis_types: Dict[str, bool]) -> Dict[str, Any]:
        """Analyze ethical insights from collected data"""
        insights = {
            'quick_checks': [],
            'medium_insights': [],
            'deep_analysis': [],
            'analyzed_citing_relationships': []
        }
        
        if not self.hierarchical_analyzer:
            st.warning("‚ö†Ô∏è Hierarchical analyzer not set. Skipping ethical insights.")
            return insights
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ç–∏–ø—ã –∞–Ω–∞–ª–∏–∑–∞
        if analysis_types.get('quick_checks', False):
            st.info("üîç Performing Quick Checks analysis...")
            insights['quick_checks'] = self.hierarchical_analyzer.analyze_quick_checks(
                self.analyzed_results, self.citing_results
            )
        
        if analysis_types.get('medium_insights', False):
            st.info("üîç Performing Medium Insights analysis...")
            insights['medium_insights'] = self.hierarchical_analyzer.analyze_medium_insights(
                self.analyzed_results, self.citing_results
            )
        
        if analysis_types.get('deep_analysis', False):
            st.info("üîç Performing Deep Analysis...")
            insights['deep_analysis'] = self.hierarchical_analyzer.analyze_deep_analysis(
                self.analyzed_results, self.citing_results, self.ref_results
            )
        
        if analysis_types.get('analyzed_citing_relationships', False):
            st.info("üîç Performing Analyzed-Citing Relationships analysis...")
            insights['analyzed_citing_relationships'] = self.hierarchical_analyzer.analyze_citing_relationships(
                self.analyzed_results, self.citing_results
            )
        
        return insights
    
    def create_comprehensive_report(self, analyzed_results: Dict[str, Dict], 
                                   ref_results: Dict[str, Dict] = None,
                                   citing_results: Dict[str, Dict] = None,
                                   analysis_types: Dict[str, bool] = None,
                                   filename: str = None) -> BytesIO:
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"articles_analysis_comprehensive_{timestamp}.xlsx"
        
        st.info(f"üìä Creating comprehensive report: {filename}")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∏–ø—ã –∞–Ω–∞–ª–∏–∑–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã
        if analysis_types is None:
            analysis_types = {
                'quick_checks': True,
                'medium_insights': True,
                'deep_analysis': False,
                'analyzed_citing_relationships': False
            }
        
        self.analyzed_results = analyzed_results
        self.ref_results = ref_results or {}
        self.citing_results = citing_results or {}
        
        self._prepare_summary_data()
        
        # Generate ethical insights
        st.info("üîç Generating ethical insights...")
        ethical_insights = self._analyze_ethical_insights(analysis_types)
        
        # Create Excel file in memory
        output = BytesIO()
        
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            st.info("üìë Generating sheets...")
            
            st.info("  1. Article_Analyzed...")
            analyzed_data = self._prepare_analyzed_articles(analyzed_results)
            if analyzed_data:
                df = pd.DataFrame(analyzed_data)
                df.to_excel(writer, sheet_name='Article_Analyzed', index=False)
            
            st.info("  2. Author freq_analyzed...")
            author_data = self._prepare_author_frequency(analyzed_results, "analyzed")
            if author_data:
                df = pd.DataFrame(author_data)
                df.to_excel(writer, sheet_name='Author freq_analyzed', index=False)
            
            st.info("  3. Journal freq_analyzed...")
            journal_data = self._prepare_journal_frequency(analyzed_results, "analyzed")
            if journal_data:
                df = pd.DataFrame(journal_data)
                df.to_excel(writer, sheet_name='Journal freq_analyzed', index=False)
            
            st.info("  4. Affiliation freq_analyzed...")
            affiliation_data = self._prepare_affiliation_frequency(analyzed_results, "analyzed")
            if affiliation_data:
                df = pd.DataFrame(affiliation_data)
                df.to_excel(writer, sheet_name='Affiliation freq_analyzed', index=False)
            
            st.info("  5. Country freq_analyzed...")
            country_data = self._prepare_country_frequency(analyzed_results, "analyzed")
            if country_data:
                df = pd.DataFrame(country_data)
                df.to_excel(writer, sheet_name='Country freq_analyzed', index=False)
            
            st.info("  6. Article_ref...")
            ref_data = self._prepare_article_ref()
            if ref_data:
                df = pd.DataFrame(ref_data)
                df.to_excel(writer, sheet_name='Article_ref', index=False)
            
            if ref_results:
                st.info("  7. Author freq_ref...")
                author_ref_data = self._prepare_author_frequency(ref_results, "ref")
                if author_ref_data:
                    df = pd.DataFrame(author_ref_data)
                    df.to_excel(writer, sheet_name='Author freq_ref', index=False)
                
                st.info("  8. Journal freq_ref...")
                journal_ref_data = self._prepare_journal_frequency(ref_results, "ref")
                if journal_ref_data:
                    df = pd.DataFrame(journal_ref_data)
                    df.to_excel(writer, sheet_name='Journal freq_ref', index=False)
                
                st.info("  9. Affiliation freq_ref...")
                affiliation_ref_data = self._prepare_affiliation_frequency(ref_results, "ref")
                if affiliation_ref_data:
                    df = pd.DataFrame(affiliation_ref_data)
                    df.to_excel(writer, sheet_name='Affiliation freq_ref', index=False)
                
                st.info("  10. Country freq_ref...")
                country_ref_data = self._prepare_country_frequency(ref_results, "ref")
                if country_ref_data:
                    df = pd.DataFrame(country_ref_data)
                    df.to_excel(writer, sheet_name='Country freq_ref', index=False)
            
            st.info("  11. Article_citing...")
            citing_data = self._prepare_article_citing()
            if citing_data:
                df = pd.DataFrame(citing_data)
                df.to_excel(writer, sheet_name='Article_citing', index=False)
            
            if citing_results:
                st.info("  12. Author freq_citing...")
                author_citing_data = self._prepare_author_frequency(citing_results, "citing")
                if author_citing_data:
                    df = pd.DataFrame(author_citing_data)
                    df.to_excel(writer, sheet_name='Author freq_citing', index=False)
                
                st.info("  13. Journal freq_citing...")
                journal_citing_data = self._prepare_journal_frequency(citing_results, "citing")
                if journal_citing_data:
                    df = pd.DataFrame(journal_citing_data)
                    df.to_excel(writer, sheet_name='Journal freq_citing', index=False)
                
                st.info("  14. Affiliation freq_citing...")
                affiliation_citing_data = self._prepare_affiliation_frequency(citing_results, "citing")
                if affiliation_citing_data:
                    df = pd.DataFrame(affiliation_citing_data)
                    df.to_excel(writer, sheet_name='Affiliation freq_citing', index=False)
                
                st.info("  15. Country freq_citing...")
                country_citing_data = self._prepare_country_frequency(citing_results, "citing")
                if country_citing_data:
                    df = pd.DataFrame(country_citing_data)
                    df.to_excel(writer, sheet_name='Country freq_citing', index=False)
            
            st.info("  16. Author_summary...")
            author_summary_data = self._prepare_author_summary()
            if author_summary_data:
                df = pd.DataFrame(author_summary_data)
                df.to_excel(writer, sheet_name='Author_summary', index=False)
            
            st.info("  17. Affiliation_summary...")
            affiliation_summary_data = self._prepare_affiliation_summary()
            if affiliation_summary_data:
                df = pd.DataFrame(affiliation_summary_data)
                df.to_excel(writer, sheet_name='Affiliation_summary', index=False)
            
            st.info("  18. Time (Ref,analyzed)_connections...")
            time_ref_analyzed_data = self._prepare_time_ref_analyzed_connections()
            if time_ref_analyzed_data:
                df = pd.DataFrame(time_ref_analyzed_data)
                df.to_excel(writer, sheet_name='Time (Ref,analyzed)_connections', index=False)
            
            st.info("  19. Time (analyzed,citing)_connections...")
            time_analyzed_citing_data = self._prepare_time_analyzed_citing_connections()
            if time_analyzed_citing_data:
                df = pd.DataFrame(time_analyzed_citing_data)
                df.to_excel(writer, sheet_name='Time (analyzed,citing)_connections', index=False)
            
            st.info("  20. Failed_DOI...")
            failed_data = self.failed_tracker.get_failed_for_excel()
            if failed_data:
                df = pd.DataFrame(failed_data)
                df.to_excel(writer, sheet_name='Failed_DOI', index=False)
            
            st.info("  21. Analysis_Stats...")
            stats_data = self._prepare_analysis_stats(analyzed_results, ref_results, citing_results)
            if stats_data:
                df = pd.DataFrame(stats_data)
                df.to_excel(writer, sheet_name='Analysis_Stats', index=False)
            
            # Ethical Insights Sheets (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∞–Ω–∞–ª–∏–∑)
            if analysis_types.get('quick_checks', False) and ethical_insights['quick_checks']:
                st.info("  22. Quick_Checks...")
                df = pd.DataFrame(ethical_insights['quick_checks'])
                df.to_excel(writer, sheet_name='Quick_Checks', index=False)
            
            if analysis_types.get('medium_insights', False) and ethical_insights['medium_insights']:
                st.info("  23. Medium_Insights...")
                df = pd.DataFrame(ethical_insights['medium_insights'])
                df.to_excel(writer, sheet_name='Medium_Insights', index=False)
            
            if analysis_types.get('deep_analysis', False) and ethical_insights['deep_analysis']:
                st.info("  24. Deep_Analysis...")
                df = pd.DataFrame(ethical_insights['deep_analysis'])
                df.to_excel(writer, sheet_name='Deep_Analysis', index=False)
            
            if analysis_types.get('analyzed_citing_relationships', False) and ethical_insights['analyzed_citing_relationships']:
                st.info("  25. Analyzed_Citing_Relationships...")
                df = pd.DataFrame(ethical_insights['analyzed_citing_relationships'])
                df.to_excel(writer, sheet_name='Analyzed_Citing_Relationships', index=False)
        
        output.seek(0)
        
        st.success(f"‚úÖ Report created!")
        
        self._print_summary(analyzed_results, ref_results, citing_results, analysis_types)
        
        return output
    
    def _prepare_summary_data(self):
        st.info("   üîç Preparing data for summary tables...")
        
        total_analyzed_articles = len([r for r in self.analyzed_results.values() if r.get('status') == 'success'])
        total_ref_articles = len([r for r in self.ref_results.values() if r.get('status') == 'success'])
        total_citing_articles = len([r for r in self.citing_results.values() if r.get('status') == 'success'])
        
        for doi, result in self.analyzed_results.items():
            if result.get('status') != 'success':
                continue
            
            self.source_dois['analyzed'].add(doi)
            
            for ref_doi in result.get('references', []):
                self.ref_to_analyzed[ref_doi].append(doi)
                self.doi_to_source_counts[ref_doi]['ref'] += 1
                self.source_dois['ref'].add(ref_doi)
            
            for cite_doi in result.get('citations', []):
                self.analyzed_to_citing[doi].append(cite_doi)
                self.doi_to_source_counts[cite_doi]['citing'] += 1
                self.source_dois['citing'].add(cite_doi)
            
            # Update author stats with normalized values
            for author in result.get('authors', []):
                full_name = author.get('name', '')
                if not full_name:
                    continue
                
                normalized_name = self.processor.normalize_author_name(full_name)
                key = normalized_name
                
                if author.get('orcid'):
                    key = f"{normalized_name}_{author['orcid']}"
                
                # Calculate normalized value for analyzed articles
                normalized_value = 1 / total_analyzed_articles if total_analyzed_articles > 0 else 0
                self.author_stats[key]['normalized_analyzed'] += normalized_value
                self.author_stats[key]['total_count'] += normalized_value
                
                if not self.author_stats[key]['orcid'] and author.get('orcid'):
                    self.author_stats[key]['orcid'] = self.processor._format_orcid_id(author.get('orcid', ''))
                
                if not self.author_stats[key]['affiliation'] and author.get('affiliation'):
                    self.author_stats[key]['affiliation'] = author.get('affiliation')[0] if author.get('affiliation') else ''
                
                if result.get('countries'):
                    country = result.get('countries')[0] if result.get('countries') else ''
                    if country and not self.author_stats[key]['country']:
                        self.author_stats[key]['country'] = country
                    
                    if self.author_stats[key]['affiliation']:
                        self.affiliation_country_stats[self.author_stats[key]['affiliation']][country] += 1
                
                self.author_stats[key]['normalized_name'] = normalized_name
            
            # Update affiliation stats with normalized values
            unique_affiliations_in_article = set()
            for author in result.get('authors', []):
                for affiliation in author.get('affiliation', []):
                    if affiliation:
                        unique_affiliations_in_article.add(affiliation)
            
            normalized_aff_value = 1 / total_analyzed_articles if total_analyzed_articles > 0 else 0
            for affiliation in unique_affiliations_in_article:
                self.affiliation_stats[affiliation]['normalized_analyzed'] += normalized_aff_value
                self.affiliation_stats[affiliation]['total_count'] += normalized_aff_value
                
                if result.get('countries'):
                    for country in result.get('countries'):
                        if country:
                            self.affiliation_stats[affiliation]['countries'].append(country)
        
        # Process ref results
        for doi, result in self.ref_results.items():
            if result.get('status') != 'success':
                continue
            
            # Update author stats for ref articles
            for author in result.get('authors', []):
                full_name = author.get('name', '')
                if not full_name:
                    continue
                
                normalized_name = self.processor.normalize_author_name(full_name)
                key = normalized_name
                
                if author.get('orcid'):
                    key = f"{normalized_name}_{author['orcid']}"
                
                # Calculate normalized value for ref articles
                normalized_value = 1 / total_ref_articles if total_ref_articles > 0 else 0
                self.author_stats[key]['normalized_reference'] += normalized_value
                self.author_stats[key]['total_count'] += normalized_value
                
                if not self.author_stats[key]['orcid'] and author.get('orcid'):
                    self.author_stats[key]['orcid'] = self.processor._format_orcid_id(author.get('orcid', ''))
                
                if not self.author_stats[key]['affiliation'] and author.get('affiliation'):
                    self.author_stats[key]['affiliation'] = author.get('affiliation')[0] if author.get('affiliation') else ''
                
                if not self.author_stats[key]['country'] and result.get('countries'):
                    self.author_stats[key]['country'] = result.get('countries')[0] if result.get('countries') else ''
                
                self.author_stats[key]['normalized_name'] = normalized_name
            
            # Update affiliation stats for ref articles
            unique_affiliations_in_article = set()
            for author in result.get('authors', []):
                for affiliation in author.get('affiliation', []):
                    if affiliation:
                        unique_affiliations_in_article.add(affiliation)
            
            normalized_aff_value = 1 / total_ref_articles if total_ref_articles > 0 else 0
            for affiliation in unique_affiliations_in_article:
                self.affiliation_stats[affiliation]['normalized_reference'] += normalized_aff_value
                self.affiliation_stats[affiliation]['total_count'] += normalized_aff_value
        
        # Process citing results
        for doi, result in self.citing_results.items():
            if result.get('status') != 'success':
                continue
            
            # Update author stats for citing articles
            for author in result.get('authors', []):
                full_name = author.get('name', '')
                if not full_name:
                    continue
                
                normalized_name = self.processor.normalize_author_name(full_name)
                key = normalized_name
                
                if author.get('orcid'):
                    key = f"{normalized_name}_{author['orcid']}"
                
                # Calculate normalized value for citing articles
                normalized_value = 1 / total_citing_articles if total_citing_articles > 0 else 0
                self.author_stats[key]['normalized_citing'] += normalized_value
                self.author_stats[key]['total_count'] += normalized_value
                
                if not self.author_stats[key]['orcid'] and author.get('orcid'):
                    self.author_stats[key]['orcid'] = self.processor._format_orcid_id(author.get('orcid', ''))
                
                if not self.author_stats[key]['affiliation'] and author.get('affiliation'):
                    self.author_stats[key]['affiliation'] = author.get('affiliation')[0] if author.get('affiliation') else ''
                
                if not self.author_stats[key]['country'] and result.get('countries'):
                    self.author_stats[key]['country'] = result.get('countries')[0] if result.get('countries') else ''
                
                self.author_stats[key]['normalized_name'] = normalized_name
            
            # Update affiliation stats for citing articles
            unique_affiliations_in_article = set()
            for author in result.get('authors', []):
                for affiliation in author.get('affiliation', []):
                    if affiliation:
                        unique_affiliations_in_article.add(affiliation)
            
            normalized_aff_value = 1 / total_citing_articles if total_citing_articles > 0 else 0
            for affiliation in unique_affiliations_in_article:
                self.affiliation_stats[affiliation]['normalized_citing'] += normalized_aff_value
                self.affiliation_stats[affiliation]['total_count'] += normalized_aff_value
        
        st.info("   üîç Getting ROR data for affiliation summary...")
        affiliations_list = list(self.affiliation_stats.keys())
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, aff in enumerate(affiliations_list):
            ror_info = self.ror_client.search_organization(aff, category="summary")
            if ror_info.get('ror_id'):
                self.affiliation_stats[aff]['colab_id'] = ror_info.get('ror_id', '')
                self.affiliation_stats[aff]['website'] = ror_info.get('website', '')
            
            progress_bar.progress((i + 1) / len(affiliations_list))
            status_text.text(f"Processing {i+1}/{len(affiliations_list)} affiliations...")
        
        progress_bar.empty()
        status_text.empty()
    
    def _prepare_analyzed_articles(self, results: Dict[str, Dict]) -> List[Dict]:
        return self._prepare_article_sheet(results, "analyzed")
    
    def _prepare_article_ref(self) -> List[Dict]:
        data = []
        
        processed_refs = {}
        for ref_doi, ref_result in self.ref_results.items():
            if ref_result.get('status') == 'success':
                processed_refs[ref_doi] = ref_result
        
        for ref_doi, ref_result in processed_refs.items():
            count = len(self.ref_to_analyzed.get(ref_doi, []))
            
            pub_info = ref_result.get('publication_info', {})
            authors = ref_result.get('authors', [])
            
            orcid_urls = ref_result.get('orcid_urls', [])
            affiliations = list(set([aff for author in authors for aff in author.get('affiliation', []) if aff]))
            countries = ref_result.get('countries', [])
            
            annual_cr = self._calculate_annual_citation_rate(
                pub_info.get('citation_count_crossref', 0), 
                pub_info.get('year', '')
            )
            annual_oa = self._calculate_annual_citation_rate(
                pub_info.get('citation_count_openalex', 0), 
                pub_info.get('year', '')
            )
            
            row = {
                'doi': ref_doi,
                'publication_date': pub_info.get('publication_date', ''),
                'authors': '; '.join([a['name'] for a in authors]),
                'ORCID ID 1; ORCID ID 2... ORCID ID last': '; '.join(orcid_urls),
                'author count': len(authors),
                'affiliations {aff 1; aff 2... aff last}': '; '.join(affiliations),
                'countries {country 1; ... country last}': '; '.join(countries),
                'Full journal Name': pub_info.get('journal', ''),
                'year': pub_info.get('year', ''),
                'Volume': pub_info.get('volume', ''),
                'Pages (or article number)': ref_result.get('pages_formatted', ''),
                'Citation counts (CR)': pub_info.get('citation_count_crossref', 0),
                'Citation counts (OA)': pub_info.get('citation_count_openalex', 0),
                'Annual cit counts (CR)': round(annual_cr, 2),
                'Annual cit counts (OA)': round(annual_oa, 2),
                'references_count': len(ref_result.get('references', [])),
                'count': count
            }
            
            data.append(row)
        
        for ref_doi in self.ref_to_analyzed:
            if ref_doi not in processed_refs:
                count = len(self.ref_to_analyzed.get(ref_doi, []))
                row = {
                    'doi': ref_doi,
                    'publication_date': '',
                    'authors': '',
                    'ORCID ID 1; ORCID ID 2... ORCID ID last': '',
                    'author count': 0,
                    'affiliations {aff 1; aff 2... aff last}': '',
                    'countries {country 1; ... country last}': '',
                    'Full journal Name': '',
                    'year': '',
                    'Volume': '',
                    'Pages (or article number)': '',
                    'Citation counts (CR)': 0,
                    'Citation counts (OA)': 0,
                    'Annual cit counts (CR)': 0.0,
                    'Annual cit counts (OA)': 0.0,
                    'references_count': 0,
                    'count': count
                }
                data.append(row)
        
        data = self._sort_article_data_by_count_and_date(data)
        
        return data
    
    def _prepare_article_citing(self) -> List[Dict]:
        data = []
        
        processed_cites = {}
        for cite_doi, cite_result in self.citing_results.items():
            if cite_result.get('status') == 'success':
                processed_cites[cite_doi] = cite_result
        
        for cite_doi, cite_result in processed_cites.items():
            count = sum(1 for analyzed_list in self.analyzed_to_citing.values() if cite_doi in analyzed_list)
            
            pub_info = cite_result.get('publication_info', {})
            authors = cite_result.get('authors', [])
            
            orcid_urls = cite_result.get('orcid_urls', [])
            affiliations = list(set([aff for author in authors for aff in author.get('affiliation', []) if aff]))
            countries = cite_result.get('countries', [])
            
            annual_cr = self._calculate_annual_citation_rate(
                pub_info.get('citation_count_crossref', 0), 
                pub_info.get('year', '')
            )
            annual_oa = self._calculate_annual_citation_rate(
                pub_info.get('citation_count_openalex', 0), 
                pub_info.get('year', '')
            )
            
            row = {
                'doi': cite_doi,
                'publication_date': pub_info.get('publication_date', ''),
                'authors': '; '.join([a['name'] for a in authors]),
                'ORCID ID 1; ORCID ID 2... ORCID ID last': '; '.join(orcid_urls),
                'author count': len(authors),
                'affiliations {aff 1; aff 2... aff last}': '; '.join(affiliations),
                'countries {country 1; ... country last}': '; '.join(countries),
                'Full journal Name': pub_info.get('journal', ''),
                'year': pub_info.get('year', ''),
                'Volume': pub_info.get('volume', ''),
                'Pages (or article number)': cite_result.get('pages_formatted', ''),
                'Citation counts (CR)': pub_info.get('citation_count_crossref', 0),
                'Citation counts (OA)': pub_info.get('citation_count_openalex', 0),
                'Annual cit counts (CR)': round(annual_cr, 2),
                'Annual cit counts (OA)': round(annual_oa, 2),
                'references_count': len(cite_result.get('references', [])),
                'count': count
            }
            
            data.append(row)
        
        all_citing_dois = set()
        for analyzed_list in self.analyzed_to_citing.values():
            all_citing_dois.update(analyzed_list)
        
        for cite_doi in all_citing_dois:
            if cite_doi not in processed_cites:
                count = sum(1 for analyzed_list in self.analyzed_to_citing.values() if cite_doi in analyzed_list)
                row = {
                    'doi': cite_doi,
                    'publication_date': '',
                    'authors': '',
                    'ORCID ID 1; ORCID ID 2... ORCID ID last': '',
                    'author count': 0,
                    'affiliations {aff 1; aff 2... aff last}': '',
                    'countries {country 1; ... country last}': '',
                    'Full journal Name': '',
                    'year': '',
                    'Volume': '',
                    'Pages (or article number)': '',
                    'Citation counts (CR)': 0,
                    'Citation counts (OA)': 0,
                    'Annual cit counts (CR)': 0.0,
                    'Annual cit counts (OA)': 0.0,
                    'references_count': 0,
                    'count': count
                }
                data.append(row)
        
        data = self._sort_article_data_by_count_and_date(data)
        
        return data
    
    def _sort_article_data_by_count_and_date(self, data: List[Dict]) -> List[Dict]:
        def sort_key(row):
            count = row.get('count', 0)
            date_str = row.get('publication_date', '')
            
            date_value = None
            if date_str:
                try:
                    for fmt in ['%Y-%m-%d', '%Y-%m', '%Y']:
                        try:
                            date_value = datetime.strptime(date_str[:len(fmt)], fmt)
                            break
                        except:
                            continue
                except:
                    date_value = None
            
            count_sort = -count
            
            if date_value:
                date_sort = -date_value.timestamp()
            else:
                date_sort = 0
                
            return (count_sort, date_sort)
        
        return sorted(data, key=sort_key)
    
    def _prepare_article_sheet(self, results: Dict[str, Dict], source_type: str) -> List[Dict]:
        data = []
        
        for doi, result in results.items():
            if result.get('status') != 'success':
                continue
            
            pub_info = result['publication_info']
            authors = result['authors']
            
            orcid_urls = result.get('orcid_urls', [])
            affiliations = list(set([aff for author in authors for aff in author.get('affiliation', []) if aff]))
            countries = result.get('countries', [])
            
            annual_cr = self._calculate_annual_citation_rate(
                pub_info.get('citation_count_crossref', 0), 
                pub_info.get('year', '')
            )
            annual_oa = self._calculate_annual_citation_rate(
                pub_info.get('citation_count_openalex', 0), 
                pub_info.get('year', '')
            )
            
            row = {
                'doi': doi,
                'publication_date': pub_info.get('publication_date', ''),
                'authors': '; '.join([a['name'] for a in authors]),
                'ORCID ID 1; ORCID ID 2... ORCID ID last': '; '.join(orcid_urls),
                'author count': len(authors),
                'affiliations {aff 1; aff 2... aff last}': '; '.join(affiliations),
                'countries {country 1; ... country last}': '; '.join(countries),
                'Full journal Name': pub_info.get('journal', ''),
                'year': pub_info.get('year', ''),
                'Volume': pub_info.get('volume', ''),
                'Pages (or article number)': result.get('pages_formatted', ''),
                'Citation counts (CR)': pub_info.get('citation_count_crossref', 0),
                'Citation counts (OA)': pub_info.get('citation_count_openalex', 0),
                'Annual cit counts (CR)': round(annual_cr, 2),
                'Annual cit counts (OA)': round(annual_oa, 2),
                'references_count': len(result.get('references', []))
            }
            
            data.append(row)
        
        return data
    
    def _prepare_author_frequency(self, results: Dict[str, Dict], source_type: str) -> List[Dict]:
        author_counter = Counter()
        author_details = {}
        
        for doi, result in results.items():
            if result.get('status') != 'success':
                continue
            
            for author in result['authors']:
                full_name = author['name']
                normalized_name = self.processor.normalize_author_name(full_name)
                
                key = normalized_name
                if author.get('orcid'):
                    key = f"{normalized_name}_{author['orcid']}"
                
                author_counter[key] += 1
                
                if key not in author_details:
                    affiliation = author['affiliation'][0] if author.get('affiliation') else ""
                    orcid = author.get('orcid', '')
                    
                    author_details[key] = {
                        'orcid': self.processor._format_orcid_id(orcid) if orcid else '',
                        'affiliation': affiliation,
                        'country': result.get('countries', [''])[0] if result.get('countries') else '',
                        'normalized_name': normalized_name
                    }
        
        sorted_authors = sorted(author_counter.items(), key=lambda x: x[1], reverse=True)
        
        data = []
        for key, count in sorted_authors:
            details = author_details.get(key, {})
            
            if source_type == "analyzed":
                frequency_column = 'Frequency count {in the analyzed articles}'
            elif source_type == "ref":
                frequency_column = 'Frequency count {in the reference articles}'
            elif source_type == "citing":
                frequency_column = 'Frequency count {in the citing articles}'
            else:
                frequency_column = f'Frequency count {{{source_type}}}'
            
            row = {
                'Surname + Initial_normalized': details.get('normalized_name', ''),
                frequency_column: count,
                'ORCID ID': details.get('orcid', ''),
                'Affiliation': details.get('affiliation', ''),
                'Country': details.get('country', '')
            }
            data.append(row)
        
        return data
    
    def _prepare_author_summary(self) -> List[Dict]:
        data = []
        
        for key, stats in self.author_stats.items():
            if stats['total_count'] == 0:
                continue
            
            # Calculate total count as sum of normalized values (as requested)
            total_count = stats['total_count']
            
            # Correct country
            corrected_country = self._correct_country_for_author(key, self.affiliation_stats)
            
            # Determine risk level based on normalized values
            risk_level = "NORMAL"
            risk_description = "Minimal author overlap between article types. Ethically acceptable."
            
            if stats['normalized_reference'] > 0.21:
                risk_level = "HIGH"
                risk_description = "Potential high self-citing for reference works"
            elif stats['normalized_citing'] > 0.5:
                risk_level = "HIGH"
                risk_description = "Potential high self-citing for citing works"
            elif total_count > 0.3:
                risk_level = "HIGH"
                risk_description = "HIGH RISK OF SELF-CITATION/CROWDING: author is present in analyzed articles and actively cites them or is cited in them. Thorough review recommended."
            elif total_count > 0.15:
                risk_level = "MEDIUM"
                risk_description = "MEDIUM LEVEL: moderate author presence in different article types. Possible normal academic interaction."
            elif total_count > 0.05:
                risk_level = "LOW"
                risk_description = "LOW LEVEL: small author presence in various article types. Likely normal academic practice."
            
            row = {
                'Surname + Initial_normalized': stats['normalized_name'],
                'ORCID ID': stats['orcid'],
                'Affiliation': stats['affiliation'],
                'Country': corrected_country,
                'Total Count': round(total_count, 4),
                'Normalized Analyzed': round(stats['normalized_analyzed'], 4),
                'Normalized Reference': round(stats['normalized_reference'], 4),
                'Normalized Citing': round(stats['normalized_citing'], 4),
                'Risk_Level': risk_level,
                'Risk_Description': risk_description
            }
            data.append(row)
        
        data.sort(key=lambda x: {'HIGH': 3, 'MEDIUM': 2, 'LOW': 1, 'NORMAL': 0}.get(x['Risk_Level'], 0), reverse=True)
        
        return data
    
    def _prepare_affiliation_summary(self) -> List[Dict]:
        data = []
        
        for affiliation, stats in self.affiliation_stats.items():
            if stats['total_count'] == 0:
                continue
            
            # Determine main country for affiliation
            main_country = ""
            if stats['countries']:
                country_counter = Counter(stats['countries'])
                main_country = country_counter.most_common(1)[0][0]
            
            row = {
                'Affiliation': affiliation,
                'Colab ID': stats['colab_id'],
                'Web Site': stats['website'],
                'Main Country': main_country,
                'total count': round(stats['total_count'], 4),
                'Normalized analyzed': round(stats['normalized_analyzed'], 4),
                'Normalized reference': round(stats['normalized_reference'], 4),
                'Normalized citing': round(stats['normalized_citing'], 4)
            }
            data.append(row)
        
        data.sort(key=lambda x: x['total count'], reverse=True)
        
        return data
    
    def _prepare_time_ref_analyzed_connections(self) -> List[Dict]:
        data = []
        
        for ref_doi, analyzed_dois in self.ref_to_analyzed.items():
            ref_result = self.ref_results.get(ref_doi, {})
            if ref_result.get('status') != 'success':
                continue
            
            ref_pub_info = ref_result.get('publication_info', {})
            ref_date_str = ref_pub_info.get('publication_date', '')
            
            ref_date = self._parse_date_string(ref_date_str)
            
            for analyzed_doi in analyzed_dois:
                analyzed_result = self.analyzed_results.get(analyzed_doi, {})
                if analyzed_result.get('status') != 'success':
                    continue
                
                analyzed_pub_info = analyzed_result.get('publication_info', {})
                analyzed_date_str = analyzed_pub_info.get('publication_date', '')
                
                analyzed_date = self._parse_date_string(analyzed_date_str)
                
                difference_days = self._calculate_date_difference(analyzed_date, ref_date)
                
                row = {
                    'Ref DOI': ref_doi,
                    'Analyzed DOI': analyzed_doi,
                    'publication date Ref': ref_date_str,
                    'publication date analyzed': analyzed_date_str,
                    'difference (days)': difference_days if difference_days is not None else ''
                }
                data.append(row)
        
        data_with_diff = [row for row in data if row['difference (days)'] not in ['', None]]
        data_without_diff = [row for row in data if row['difference (days)'] in ['', None]]
        
        data_with_diff.sort(key=lambda x: x['difference (days)'])
        
        return data_with_diff + data_without_diff
    
    def _prepare_time_analyzed_citing_connections(self) -> List[Dict]:
        data = []
        
        for analyzed_doi, citing_dois in self.analyzed_to_citing.items():
            analyzed_result = self.analyzed_results.get(analyzed_doi, {})
            if analyzed_result.get('status') != 'success':
                continue
            
            analyzed_pub_info = analyzed_result.get('publication_info', {})
            analyzed_date_str = analyzed_pub_info.get('publication_date', '')
            
            analyzed_date = self._parse_date_string(analyzed_date_str)
            
            for citing_doi in citing_dois:
                citing_result = self.citing_results.get(citing_doi, {})
                if citing_result.get('status') != 'success':
                    continue
                
                citing_pub_info = citing_result.get('publication_info', {})
                citing_date_str = citing_pub_info.get('publication_date', '')
                
                citing_date = self._parse_date_string(citing_date_str)
                
                difference_days = self._calculate_date_difference(citing_date, analyzed_date)
                
                row = {
                    'Analyzed DOI': analyzed_doi,
                    'Citing DOI': citing_doi,
                    'publication date analyzed': analyzed_date_str,
                    'publication date citing': citing_date_str,
                    'difference (days)': difference_days if difference_days is not None else ''
                }
                data.append(row)
        
        data_with_diff = [row for row in data if row['difference (days)'] not in ['', None]]
        data_without_diff = [row for row in data if row['difference (days)'] in ['', None]]
        
        data_with_diff.sort(key=lambda x: x['difference (days)'])
        
        return data_with_diff + data_without_diff
    
    def _parse_date_string(self, date_str: str) -> Optional[datetime]:
        if not date_str:
            return None
        
        date_str = date_str.strip()
        
        try:
            if re.match(r'^\d{4}-\d{1,2}-\d{1,2}$', date_str):
                parts = date_str.split('-')
                year = int(parts[0])
                month = int(parts[1])
                day = int(parts[2])
                return datetime(year, month, day)
            
            elif re.match(r'^\d{4}-\d{1,2}$', date_str):
                parts = date_str.split('-')
                year = int(parts[0])
                month = int(parts[1])
                return datetime(year, month, 15)
            
            elif re.match(r'^\d{4}$', date_str):
                year = int(date_str)
                return datetime(year, 7, 1)
            
            elif re.match(r'^\d{4}/\d{1,2}/\d{1,2}$', date_str):
                parts = date_str.split('/')
                year = int(parts[0])
                month = int(parts[1])
                day = int(parts[2])
                return datetime(year, month, day)
            
            elif re.match(r'^\d{4}/\d{1,2}$', date_str):
                parts = date_str.split('/')
                year = int(parts[0])
                month = int(parts[1])
                return datetime(year, month, 15)
            
            elif re.match(r'^\d{1,2}\.\d{1,2}\.\d{4}$', date_str):
                parts = date_str.split('.')
                day = int(parts[0])
                month = int(parts[1])
                year = int(parts[2])
                return datetime(year, month, day)
            
            elif re.match(r'^\d{1,2}/\d{1,2}/\d{4}$', date_str):
                parts = date_str.split('/')
                month = int(parts[0])
                day = int(parts[1])
                year = int(parts[2])
                return datetime(year, month, day)
            
            elif re.match(r'^\d{4}\.\d{1,2}\.\d{1,2}$', date_str):
                parts = date_str.split('.')
                year = int(parts[0])
                month = int(parts[1])
                day = int(parts[2])
                return datetime(year, month, day)
                
        except (ValueError, IndexError):
            pass
        
        year_match = re.search(r'\b(19\d{2}|20\d{2})\b', date_str)
        if year_match:
            try:
                year = int(year_match.group(1))
                return datetime(year, 7, 1)
            except:
                pass
        
        return None
    
    def _calculate_date_difference(self, date1: Optional[datetime], date2: Optional[datetime]) -> Optional[int]:
        if not date1 or not date2:
            return None
        
        try:
            difference = (date1 - date2).days
            
            if abs(difference) > 10000:
                if date1.year == date2.year:
                    return (date1 - datetime(date1.year, 1, 1)).days - (date2 - datetime(date2.year, 1, 1)).days
            
            return difference
        except:
            return None
    
    def _prepare_journal_frequency(self, results: Dict[str, Dict], source_type: str) -> List[Dict]:
        journal_counter = Counter()
        
        for doi, result in results.items():
            if result.get('status') != 'success':
                continue
            
            journal = result['publication_info'].get('journal', '')
            if journal:
                journal_counter[journal] += 1
        
        sorted_journals = sorted(journal_counter.items(), key=lambda x: x[1], reverse=True)
        
        return [{'Full Journal Name': journal, 'Count': count} 
                for journal, count in sorted_journals]
    
    def _prepare_affiliation_frequency(self, results: Dict[str, Dict], source_type: str) -> List[Dict]:
        affiliation_counter = Counter()
        
        for doi, result in results.items():
            if result.get('status') != 'success':
                continue
            
            unique_affiliations_in_article = set()
            for author in result.get('authors', []):
                for affiliation in author.get('affiliation', []):
                    if affiliation and affiliation.strip():
                        clean_aff = affiliation.strip()
                        unique_affiliations_in_article.add(clean_aff)
            
            for aff in unique_affiliations_in_article:
                affiliation_counter[aff] += 1
        
        unique_affiliations = list(set(affiliation_counter.keys()))
        
        affiliation_data = []
        
        st.info(f"   üîç Collecting data for {len(unique_affiliations)} affiliations ({source_type})...")
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, aff in enumerate(unique_affiliations):
            row = {
                'Affiliation': aff,
                'Count': affiliation_counter[aff]
            }
            affiliation_data.append(row)
            
            progress_bar.progress((i + 1) / len(unique_affiliations))
            status_text.text(f"Processing {i+1}/{len(unique_affiliations)} affiliations...")
        
        progress_bar.empty()
        status_text.empty()
        
        affiliation_data.sort(key=lambda x: x['Count'], reverse=True)
        return affiliation_data
    
    def _prepare_country_frequency(self, results: Dict[str, Dict], source_type: str) -> List[Dict]:
        country_single_counter = Counter()
        country_combined_counter = Counter()
        
        for doi, result in results.items():
            if result.get('status') != 'success':
                continue
            
            countries = result.get('countries', [])
            if not countries:
                continue
            
            for country in countries:
                if country:
                    country_single_counter[country] += 1
            
            if len(countries) > 1:
                sorted_countries = sorted(countries)
                combination = ';'.join(sorted_countries)
                country_combined_counter[combination] += 1
        
        data = []
        
        for country, count in sorted(country_single_counter.items(), key=lambda x: x[1], reverse=True):
            data.append({
                'Country': country,
                'Type': 'single',
                'Count': count
            })
        
        for combination, count in sorted(country_combined_counter.items(), key=lambda x: x[1], reverse=True):
            data.append({
                'Country': combination,
                'Type': 'combined',
                'Count': count
            })
        
        return data
    
    def _prepare_analysis_stats(self, analyzed_results: Dict[str, Dict], 
                               ref_results: Dict[str, Dict], 
                               citing_results: Dict[str, Dict]) -> List[Dict]:
        stats = []
        
        analyzed_success = sum(1 for r in analyzed_results.values() if r.get('status') == 'success')
        analyzed_failed = len(analyzed_results) - analyzed_success
        
        stats.append({
            'Category': 'Analyzed Articles',
            'Total DOI': len(analyzed_results),
            'Successful': analyzed_success,
            'Failed': analyzed_failed,
            'Success Rate': f"{(analyzed_success/len(analyzed_results)*100):.1f}%" if analyzed_results else "0%"
        })
        
        if ref_results:
            ref_success = sum(1 for r in ref_results.values() if r.get('status') == 'success')
            ref_failed = len(ref_results) - ref_success
            
            stats.append({
                'Category': 'Reference Articles',
                'Total DOI': len(ref_results),
                'Successful': ref_success,
                'Failed': ref_failed,
                'Success Rate': f"{(ref_success/len(ref_results)*100):.1f}%" if ref_results else "0%"
            })
        
        if citing_results:
            cite_success = sum(1 for r in citing_results.values() if r.get('status') == 'success')
            cite_failed = len(citing_results) - cite_success
            
            stats.append({
                'Category': 'Citing Articles',
                'Total DOI': len(citing_results),
                'Successful': cite_success,
                'Failed': cite_failed,
                'Success Rate': f"{(cite_success/len(citing_results)*100):.1f}%" if citing_results else "0%"
            })
        
        total_dois = len(analyzed_results) + len(ref_results or {}) + len(citing_results or {})
        total_success = analyzed_success + (ref_success if ref_results else 0) + (cite_success if citing_results else 0)
        
        stats.append({
            'Category': 'TOTAL',
            'Total DOI': total_dois,
            'Successful': total_success,
            'Failed': total_dois - total_success,
            'Success Rate': f"{(total_success/total_dois*100):.1f}%" if total_dois > 0 else "0%"
        })
        
        return stats
    
    def _print_summary(self, analyzed_results: Dict[str, Dict], 
                      ref_results: Dict[str, Dict], 
                      citing_results: Dict[str, Dict],
                      analysis_types: Dict[str, bool] = None):
        st.subheader("üìä ANALYSIS SUMMARY:")
        
        analyzed_success = sum(1 for r in analyzed_results.values() if r.get('status') == 'success')
        st.info(f"üìö Analyzed articles: {analyzed_success}/{len(analyzed_results)} successful")
        
        if analyzed_success > 0:
            total_authors = sum(len(r.get('authors', [])) for r in analyzed_results.values() if r.get('status') == 'success')
            total_refs = sum(len(r.get('references', [])) for r in analyzed_results.values() if r.get('status') == 'success')
            total_cites = sum(len(r.get('citations', [])) for r in analyzed_results.values() if r.get('status') == 'success')
            
            st.info(f"üë• Authors: {total_authors}")
            st.info(f"üìé References: {total_refs}")
            st.info(f"üîó Citations: {total_cites}")
        
        if ref_results:
            ref_success = sum(1 for r in ref_results.values() if r.get('status') == 'success')
            st.info(f"üìé Reference articles: {ref_success}/{len(ref_results)} successful")
        
        if citing_results:
            cite_success = sum(1 for r in citing_results.values() if r.get('status') == 'success')
            st.info(f"üîó Citation articles: {cite_success}/{len(citing_results)} successful")
        
        if analysis_types:
            st.subheader("üîç Ethical Analysis Types:")
            for analysis_type, enabled in analysis_types.items():
                status = "‚úÖ ENABLED" if enabled else "‚ùå DISABLED"
                st.info(f"‚Ä¢ {analysis_type.replace('_', ' ').title()}: {status}")
        
        failed_stats = self.failed_tracker.get_stats()
        st.warning(f"‚ùå Failed DOI: {failed_stats['total_failed']}")
        st.warning(f"‚Ä¢ From analyzed: {failed_stats['analyzed_failed']}")
        st.warning(f"‚Ä¢ From references: {failed_stats['ref_failed']}")
        st.warning(f"‚Ä¢ From citations: {failed_stats['citing_failed']}")
    
    def update_counters(self, references: List[str], citations: List[str], source_type: str = "analyzed"):
        if source_type == "analyzed":
            counter_ref = self.references_counter
            counter_cite = self.citations_counter
        elif source_type == "ref":
            counter_ref = self.ref_references_counter
            counter_cite = self.ref_citations_counter
        elif source_type == "citing":
            counter_ref = self.cite_references_counter
            counter_cite = self.cite_citations_counter
        else:
            counter_ref = self.references_counter
            counter_cite = self.citations_counter
        
        for ref in references:
            if ref:
                counter_ref[ref] += 1
        
        for cite in citations:
            if cite:
                counter_cite[cite] += 1

# ============================================================================
# üéõÔ∏è –ö–õ–ê–°–° –£–ü–†–ê–í–õ–ï–ù–ò–Ø –ò–ù–¢–ï–†–§–ï–ô–°–û–ú STREAMLIT
# ============================================================================

class StreamlitInterfaceManager:
    def __init__(self, system):
        self.system = system
        self.initialize_session_state()
    
    def initialize_session_state(self):
        """Initialize Streamlit session state variables"""
        if 'system_initialized' not in st.session_state:
            st.session_state.system_initialized = False
        if 'processed_data' not in st.session_state:
            st.session_state.processed_data = None
        if 'excel_file' not in st.session_state:
            st.session_state.excel_file = None
        if 'analysis_types' not in st.session_state:
            st.session_state.analysis_types = {
                'quick_checks': True,
                'medium_insights': True,
                'deep_analysis': False,
                'analyzed_citing_relationships': False
            }
    
    def render_sidebar(self):
        """Render the sidebar controls"""
        with st.sidebar:
            st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
            
            # Workers setting
            st.subheader("–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å")
            workers = st.slider(
                "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤",
                min_value=Config.MIN_WORKERS,
                max_value=Config.MAX_WORKERS,
                value=Config.DEFAULT_WORKERS,
                help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ DOI",
                key="workers_slider"  # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á
            )
            
            # Analysis types
            st.subheader("üîç –¢–∏–ø—ã –∞–Ω–∞–ª–∏–∑–∞")
            st.session_state.analysis_types['quick_checks'] = st.checkbox(
                "Quick Checks (5-10 —Å–µ–∫)", 
                value=st.session_state.analysis_types['quick_checks'],
                help="–ë—ã—Å—Ç—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –Ω–µ—ç—Ç–∏—á–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏"
            )
            st.session_state.analysis_types['medium_insights'] = st.checkbox(
                "Medium Insights (15-30 —Å–µ–∫)", 
                value=st.session_state.analysis_types['medium_insights'],
                help="–°—Ä–µ–¥–Ω–∏–µ –∏–Ω—Å–∞–π—Ç—ã —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"
            )
            st.session_state.analysis_types['deep_analysis'] = st.checkbox(
                "Deep Analysis (60-120 —Å–µ–∫)", 
                value=st.session_state.analysis_types['deep_analysis'],
                help="–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å ML –∏ —Å–µ—Ç–µ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"
            )
            st.session_state.analysis_types['analyzed_citing_relationships'] = st.checkbox(
                "Analyzed-Citing Relationships (30-60 —Å–µ–∫)", 
                value=st.session_state.analysis_types['analyzed_citing_relationships'],
                help="–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–º–∏ –∏ —Ü–∏—Ç–∏—Ä—É—é—â–∏–º–∏ —Å—Ç–∞—Ç—å—è–º–∏"
            )
            
            # Cache controls
            st.subheader("üóÇÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–µ–º")
            if st.button("üßπ –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à", type="secondary"):
                self.system.cache_manager.clear_all()
                st.success("–ö—ç—à –æ—á–∏—â–µ–Ω!")
            
            # Display cache stats
            cache_stats = self.system.cache_manager.get_stats()
            with st.expander("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"):
                st.metric("–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", f"{cache_stats['hit_ratio']}%")
                st.metric("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ API –≤—ã–∑–æ–≤–æ–≤", cache_stats['api_calls_saved'])
                st.metric("–†–∞–∑–º–µ—Ä –∫—ç—à–∞", f"{cache_stats['cache_size_mb']} MB")
            
            return workers

            # Analysis types —Å –∫–ª—é—á–∞–º–∏
            st.subheader("üîç –¢–∏–ø—ã –∞–Ω–∞–ª–∏–∑–∞")
            st.session_state.analysis_types['quick_checks'] = st.checkbox(
                "Quick Checks (5-10 —Å–µ–∫)", 
                value=st.session_state.analysis_types['quick_checks'],
                help="–ë—ã—Å—Ç—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –Ω–µ—ç—Ç–∏—á–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏",
                key="quick_checks_checkbox"  # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á
            )
            st.session_state.analysis_types['medium_insights'] = st.checkbox(
                "Medium Insights (15-30 —Å–µ–∫)", 
                value=st.session_state.analysis_types['medium_insights'],
                help="–°—Ä–µ–¥–Ω–∏–µ –∏–Ω—Å–∞–π—Ç—ã —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º",
                key="medium_insights_checkbox"  # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á
            )
            st.session_state.analysis_types['deep_analysis'] = st.checkbox(
                "Deep Analysis (60-120 —Å–µ–∫)", 
                value=st.session_state.analysis_types['deep_analysis'],
                help="–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å ML –∏ —Å–µ—Ç–µ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏",
                key="deep_analysis_checkbox"  # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á
            )
            st.session_state.analysis_types['analyzed_citing_relationships'] = st.checkbox(
                "Analyzed-Citing Relationships (30-60 —Å–µ–∫)", 
                value=st.session_state.analysis_types['analyzed_citing_relationships'],
                help="–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–º–∏ –∏ —Ü–∏—Ç–∏—Ä—É—é—â–∏–º–∏ —Å—Ç–∞—Ç—å—è–º–∏",
                key="relationships_checkbox"  # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á
            )
    
    def render_main_interface(self):
        """Render the main interface"""
        st.title("üìö –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ DOI")
        
        st.markdown("""
        ### –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º –Ω–µ—ç—Ç–∏—á–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫
        
        **–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:**
        - üìä –£–º–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π (–ø–∞–º—è—Ç—å, —Ñ–∞–π–ª, –∑–∞–ø—Ä–æ—Å—ã)
        - üîç –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (4 —É—Ä–æ–≤–Ω—è –≥–ª—É–±–∏–Ω—ã)
        - ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–µ—ç—Ç–∏—á–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        - üìà 25+ –≤–∫–ª–∞–¥–æ–∫ –≤ Excel –æ—Ç—á–µ—Ç–µ
        - üåê –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ 10 –ø–æ—Ç–æ–∫–æ–≤
        """)
        
        # DOI input
        st.subheader("üìù –í–≤–µ–¥–∏—Ç–µ DOI –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        doi_input = st.text_area(
            "–í–≤–µ–¥–∏—Ç–µ –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ DOI (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π –∏–ª–∏ –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫–∏)",
            height=150,
            placeholder="–ü—Ä–∏–º–µ—Ä—ã:\n10.1038/nature12373\n10.1126/science.1252914\n10.1016/j.cell.2019.11.017"
        )
        
        # Process button
        col1, col2, col3 = st.columns(3)
        with col1:
            process_btn = st.button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å DOI", type="primary", use_container_width=True)
        with col2:
            clear_btn = st.button("üßπ –û—á–∏—Å—Ç–∏—Ç—å", type="secondary", use_container_width=True)
        with col3:
            export_btn = st.button("üíæ –≠–∫—Å–ø–æ—Ä—Ç Excel", type="secondary", use_container_width=True, 
                                 disabled=st.session_state.processed_data is None)
        
        if clear_btn:
            st.session_state.processed_data = None
            st.session_state.excel_file = None
            st.rerun()
        
        if process_btn and doi_input:
            self.process_dois(doi_input)
        
        if export_btn and st.session_state.processed_data:
            self.export_to_excel()
        
        # Display results if available
        if st.session_state.processed_data:
            self.display_results()
    
    def process_dois(self, doi_input: str):
        """Process the entered DOIs"""
        with st.spinner("üîç –ü–∞—Ä—Å–∏–Ω–≥ DOI..."):
            dois = self.system._parse_dois(doi_input)
        
        if not dois:
            st.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö DOI")
            return
        
        st.info(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(dois)} DOI –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        # Create progress containers
        progress_container = st.container()
        results_container = st.container()

        with progress_container:
            st.subheader("üìà –ü—Ä–æ–≥—Ä–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            
            # Main progress —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º st.empty() –¥–ª—è —Ç–µ–∫—Å—Ç–∞
            main_status = st.empty()
            main_progress = st.progress(0, key="main_progress")
            main_status.text("–û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å")
            
            # Individual progress bars —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏
            analyzed_status = st.empty()
            analyzed_progress = st.progress(0, key="analyzed_progress")
            analyzed_status.text("–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–µ–π")
            
            refs_status = st.empty()
            refs_progress = st.progress(0, key="refs_progress")
            refs_status.text("–ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫")
            
            cites_status = st.empty()
            cites_progress = st.progress(0, key="cites_progress")
            cites_status.text("–ê–Ω–∞–ª–∏–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π")
            
            insights_status = st.empty()
            insights_progress = st.progress(0, key="insights_progress")
            insights_status.text("–ê–Ω–∞–ª–∏–∑ –∏–Ω—Å–∞–π—Ç–æ–≤")
            
            excel_status = st.empty()
            excel_progress = st.progress(0, key="excel_progress")
            excel_status.text("–°–æ–∑–¥–∞–Ω–∏–µ Excel")
        
        # –í–º–µ—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ render_sidebar(), –ø–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ session_state
        # –ï—Å–ª–∏ workers_slider –µ—â–µ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        workers = st.session_state.get('workers_slider', Config.DEFAULT_WORKERS)
        
        # Update system settings
        self.system.widgets.workers_slider.value = workers
        
        # Process DOIs
        try:
            start_time = time.time()
            
            # Update main progress
            main_progress.progress(10, text="–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
            main_status.info("üîÑ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É DOI...")
            
            # Process analyzed DOIs
            analyzed_progress.progress(0, text="–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö DOI...")
            self.system.analyzed_results = self.system.doi_processor.process_doi_batch(
                dois, "analyzed", None, True, True, Config.BATCH_SIZE, progress_container
            )
            analyzed_progress.progress(100, text="–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ DOI –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
            main_progress.progress(40, text="–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ DOI –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
            
            # Collect and process references
            all_ref_dois = self.system.doi_processor.collect_all_references(self.system.analyzed_results)
            self.system.system_stats['total_ref_dois'] = len(all_ref_dois)
            
            if all_ref_dois:
                main_status.info(f"üìé –ù–∞–π–¥–µ–Ω–æ {len(all_ref_dois)} reference DOI")
                refs_progress.progress(0, text="–û–±—Ä–∞–±–æ—Ç–∫–∞ reference DOI...")
                
                ref_dois_to_analyze = all_ref_dois[:5000]
                self.system.ref_results = self.system.doi_processor.process_doi_batch(
                    ref_dois_to_analyze, "ref", None, True, True, Config.BATCH_SIZE, progress_container
                )
                refs_progress.progress(100, text="Reference DOI –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
                main_progress.progress(60, text="Reference DOI –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
            
            # Collect and process citations
            all_cite_dois = self.system.doi_processor.collect_all_citations(self.system.analyzed_results)
            self.system.system_stats['total_cite_dois'] = len(all_cite_dois)
            
            if all_cite_dois:
                main_status.info(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(all_cite_dois)} citation DOI")
                cites_progress.progress(0, text="–û–±—Ä–∞–±–æ—Ç–∫–∞ citation DOI...")
                
                cite_dois_to_analyze = all_cite_dois[:5000]
                self.system.citing_results = self.system.doi_processor.process_doi_batch(
                    cite_dois_to_analyze, "citing", None, True, True, Config.BATCH_SIZE, progress_container
                )
                cites_progress.progress(100, text="Citation DOI –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
                main_progress.progress(80, text="Citation DOI –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
            
            # Retry failed DOIs
            failed_stats = self.system.failed_tracker.get_stats()
            if failed_stats['total_failed'] > 0:
                main_status.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {failed_stats['total_failed']} –Ω–µ—É–¥–∞—á–Ω—ã—Ö DOI")
                self.system.doi_processor.retry_failed_dois(self.system.failed_tracker, progress_container=progress_container)
            
            # Generate insights
            insights_progress.progress(0, text="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤...")
            # Insights are generated during export
            insights_progress.progress(100, text="–ò–Ω—Å–∞–π—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")
            main_progress.progress(90, text="–ò–Ω—Å–∞–π—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")
            
            # Store processed data
            st.session_state.processed_data = {
                'analyzed': self.system.analyzed_results,
                'ref': self.system.ref_results,
                'citing': self.system.citing_results,
                'stats': self.system.system_stats
            }
            
            processing_time = time.time() - start_time
            
            # Update counters
            for doi, result in self.system.analyzed_results.items():
                if result.get('status') == 'success':
                    self.system.excel_exporter.update_counters(
                        result.get('references', []), 
                        result.get('citations', []),
                        "analyzed"
                    )
            
            main_progress.progress(100, text="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            main_status.success(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.1f} —Å–µ–∫—É–Ω–¥!")
            
            # Clear progress bars
            time.sleep(1)
            main_progress.empty()
            analyzed_progress.empty()
            refs_progress.empty()
            cites_progress.empty()
            insights_progress.empty()
            excel_progress.empty()
            main_status.empty()
            
            st.rerun()
            
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")
            import traceback
            st.code(traceback.format_exc())
    
    def export_to_excel(self):
        """Export results to Excel"""
        if not st.session_state.processed_data:
            st.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return
        
        with st.spinner("üìä –°–æ–∑–¥–∞–Ω–∏–µ Excel –æ—Ç—á–µ—Ç–∞..."):
            excel_data = self.system.excel_exporter.create_comprehensive_report(
                st.session_state.processed_data['analyzed'],
                st.session_state.processed_data['ref'],
                st.session_state.processed_data['citing'],
                st.session_state.analysis_types
            )
            
            st.session_state.excel_file = excel_data
            
            # Create download button
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"article_analysis_{timestamp}.xlsx"
            
            st.download_button(
                label="üì• –°–∫–∞—á–∞—Ç—å Excel —Ñ–∞–π–ª",
                data=excel_data,
                file_name=filename,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            
            st.success("‚úÖ Excel –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω!")
    
    def display_results(self):
        """Display processing results"""
        if not st.session_state.processed_data:
            return
        
        st.subheader("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        data = st.session_state.processed_data
        analyzed_results = data['analyzed']
        
        # Display summary statistics
        successful_count = sum(1 for r in analyzed_results.values() if r.get('status') == 'success')
        failed_count = len(analyzed_results) - successful_count
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("‚úÖ –£—Å–ø–µ—à–Ω–æ", successful_count)
        with col2:
            st.metric("‚ùå –û—à–∏–±–∫–∏", failed_count)
        with col3:
            success_rate = (successful_count / len(analyzed_results) * 100) if analyzed_results else 0
            st.metric("üìà –£—Å–ø–µ—à–Ω–æ—Å—Ç—å", f"{success_rate:.1f}%")
        
        # Display detailed results
        with st.expander("üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", expanded=False):
            if successful_count > 0:
                total_authors = sum(len(r.get('authors', [])) for r in analyzed_results.values() if r.get('status') == 'success')
                total_refs = sum(len(r.get('references', [])) for r in analyzed_results.values() if r.get('status') == 'success')
                total_cites = sum(len(r.get('citations', [])) for r in analyzed_results.values() if r.get('status') == 'success')
                
                st.info(f"üë• –í—Å–µ–≥–æ –∞–≤—Ç–æ—Ä–æ–≤: {total_authors}")
                st.info(f"üìé –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫: {total_refs}")
                st.info(f"üîó –í—Å–µ–≥–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π: {total_cites}")
            
            if data['ref']:
                ref_success = sum(1 for r in data['ref'].values() if r.get('status') == 'success')
                st.info(f"üìé Reference DOI: {ref_success}/{len(data['ref'])} —É—Å–ø–µ—à–Ω–æ")
            
            if data['citing']:
                cite_success = sum(1 for r in data['citing'].values() if r.get('status') == 'success')
                st.info(f"üîó Citation DOI: {cite_success}/{len(data['citing'])} —É—Å–ø–µ—à–Ω–æ")
        
        # Display individual results
        with st.expander("üìÑ –î–µ—Ç–∞–ª–∏ –ø–æ —Å—Ç–∞—Ç—å—è–º", expanded=False):
            tabs = st.tabs(["‚úÖ –£—Å–ø–µ—à–Ω—ã–µ", "‚ùå –û—à–∏–±–∫–∏"])
            
            with tabs[0]:
                successful_articles = [(doi, r) for doi, r in analyzed_results.items() if r.get('status') == 'success']
                for doi, result in successful_articles[:10]:  # Show first 10
                    self.display_article_card(result, "analyzed")
                
                if len(successful_articles) > 10:
                    st.info(f"... –∏ –µ—â–µ {len(successful_articles) - 10} —É—Å–ø–µ—à–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
            
            with tabs[1]:
                failed_articles = [(doi, r) for doi, r in analyzed_results.items() if r.get('status') != 'success']
                for doi, result in failed_articles[:5]:  # Show first 5 errors
                    self.display_error_card(doi, result, "analyzed")
                
                if len(failed_articles) > 5:
                    st.info(f"... –∏ –µ—â–µ {len(failed_articles) - 5} –æ—à–∏–±–æ–∫")
    
    def display_article_card(self, result: Dict, source_type: str):
        """Display an article card"""
        pub_info = result['publication_info']
        source_badge = {
            "analyzed": ("üìö", "#4CAF50"),
            "ref": ("üìé", "#2196F3"),
            "citing": ("üîó", "#9C27B0")
        }.get(source_type, ("üìÑ", "#666"))
        
        quick_insights = result.get('quick_insights', {})
        insight_badge = ""
        if quick_insights:
            citation_velocity = quick_insights.get('citation_velocity', 0)
            if citation_velocity > 20:
                insight_badge = " ‚ö†Ô∏è"
            elif citation_velocity > 10:
                insight_badge = " ‚ÑπÔ∏è"
        
        st.markdown(f"""
        <div style="background: #e8f5e9; padding: 12px; border-radius: 6px; margin: 8px 0; border-left: 4px solid {source_badge[1]};">
            <div style="display: flex; justify-content: space-between; align-items: flex-start;">
                <div style="flex: 1;">
                    <h4 style="margin: 0 0 8px 0; color: #2e7d32; font-size: 14px;">
                        {source_badge[0]} {pub_info.get('title', '')[:70]}{'...' if len(pub_info.get('title', '')) > 70 else ''}{insight_badge}
                    </h4>
                    <div style="font-size: 12px; color: #555; line-height: 1.4;">
                        <strong>DOI:</strong> {result['doi']}<br>
                        <strong>–ñ—É—Ä–Ω–∞–ª:</strong> {pub_info.get('journal', '')[:50]}{'...' if len(pub_info.get('journal', '')) > 50 else ''}<br>
                        <strong>–ì–æ–¥:</strong> {pub_info.get('year', '')} | 
                        <strong>–ê–≤—Ç–æ—Ä–æ–≤:</strong> {len(result.get('authors', []))}<br>
                        <strong>–¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:</strong> 
                        <span style="color: {'green' if pub_info.get('citation_count_crossref', 0) > 0 else '#999'}">Crossref: {pub_info.get('citation_count_crossref', 0)}</span> | 
                        <span style="color: {'green' if pub_info.get('citation_count_openalex', 0) > 0 else '#999'}">OpenAlex: {pub_info.get('citation_count_openalex', 0)}</span>
                    </div>
                </div>
                <span style="background: {source_badge[1]}; color: white; padding: 2px 8px; border-radius: 12px; font-size: 11px; font-weight: bold;">
                    {source_type.upper()}
                </span>
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    def display_error_card(self, doi: str, result: Dict, source_type: str):
        """Display an error card"""
        source_badge = {
            "analyzed": ("üìö", "#f44336"),
            "ref": ("üìé", "#f44336"),
            "citing": ("üîó", "#f44336")
        }.get(source_type, ("üìÑ", "#f44336"))
        
        st.markdown(f"""
        <div style="background: #ffebee; padding: 12px; border-radius: 6px; margin: 8px 0; border-left: 4px solid {source_badge[1]};">
            <div style="display: flex; justify-content: space-between; align-items: center;">
                <div>
                    <h4 style="margin: 0 0 4px 0; color: #c62828; font-size: 14px;">{source_badge[0]} ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏</h4>
                    <div style="font-size: 12px; color: #555;">
                        <strong>DOI:</strong> {doi}
                    </div>
                </div>
                <span style="background: {source_badge[1]}; color: white; padding: 2px 8px; border-radius: 12px; font-size: 11px; font-weight: bold;">
                    {source_type.upper()}
                </span>
            </div>
            
            <div style="margin-top: 8px; padding: 8px; background: #ffcdd2; border-radius: 4px; font-size: 11px;">
                <strong>–ü—Ä–∏—á–∏–Ω–∞:</strong> {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}
            </div>
        </div>
        """, unsafe_allow_html=True)

# ============================================================================
# üöÄ –ì–õ–ê–í–ù–´–ô –ö–õ–ê–°–° –°–ò–°–¢–ï–ú–´ (–ê–î–ê–ü–¢–ò–†–û–í–ê–ù–ù–´–ô –î–õ–Ø STREAMLIT)
# ============================================================================

class ArticleAnalyzerSystem:
    def __init__(self):
        # Initialize components
        self.cache_manager = SmartCacheManager()
        self.delay_manager = AdaptiveDelayManager()
        self.failed_tracker = FailedDOITracker()
        
        # Initialize API clients
        self.crossref_client = CrossrefClient(self.cache_manager, self.delay_manager)
        self.openalex_client = OpenAlexClient(self.cache_manager, self.delay_manager)
        self.ror_client = RORClient(self.cache_manager)
        
        # Initialize processors
        self.data_processor = DataProcessor(self.cache_manager)
        self.doi_processor = OptimizedDOIProcessor(
            self.cache_manager, self.delay_manager, 
            self.data_processor, self.failed_tracker
        )
        self.hierarchical_analyzer = HierarchicalDataAnalyzer(
            self.cache_manager, self.data_processor, self.doi_processor
        )
        self.excel_exporter = ExcelExporter(self.data_processor, self.ror_client, self.failed_tracker)
        self.excel_exporter.set_hierarchical_analyzer(self.hierarchical_analyzer)
        
        # Streamlit interface
        self.interface_manager = StreamlitInterfaceManager(self)
        
        # System stats
        self.system_stats = {
            'total_dois_processed': 0,
            'total_successful': 0,
            'total_failed': 0,
            'total_authors': 0,
            'total_requests': 0,
            'total_ref_dois': 0,
            'total_cite_dois': 0
        }
        
        # Results storage
        self.analyzed_results = {}
        self.ref_results = {}
        self.citing_results = {}
        
        # Widgets compatibility (minimal)
        self.widgets = type('obj', (object,), {
            'workers_slider': type('obj', (object,), {'value': Config.DEFAULT_WORKERS})()
        })()
    
    def _parse_dois(self, input_text: str) -> List[str]:
        """Parse DOIs from input text"""
        if not input_text:
            return []
        
        separators = [',', ';', '\n', '\t', '|']
        
        for sep in separators:
            if sep in input_text:
                parts = input_text.split(sep)
                break
        else:
            parts = input_text.split()
        
        dois = []
        for part in parts:
            doi = self._clean_doi(part)
            if doi and len(doi) > 5:
                dois.append(doi)
        
        return list(set(dois))
    
    def _clean_doi(self, doi: str) -> str:
        """Clean DOI string"""
        if not doi or not isinstance(doi, str):
            return ""
        
        doi = doi.strip()
        prefixes = ['doi:', 'DOI:', 'https://doi.org/', 'http://doi.org/', 
                   'https://dx.doi.org/', 'http://dx.doi.org/']
        
        for prefix in prefixes:
            if doi.lower().startswith(prefix.lower()):
                doi = doi[len(prefix):]
        
        return doi.strip()

    def run(self):
        """Run the Streamlit application"""
        # Set page config
        st.set_page_config(
            page_title="–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π",
            page_icon="üìö",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        # Initialize system
        if not st.session_state.get('system_initialized', False):
            with st.spinner("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã..."):
                self.cache_manager._clean_expired_cache()
                st.session_state.system_initialized = True
        
        # Render interface - render_sidebar() —Å–æ—Ö—Ä–∞–Ω–∏—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ session_state
        self.interface_manager.render_sidebar()  # –ù–µ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.interface_manager.render_main_interface()

# ============================================================================
# üèÉ‚Äç‚ôÇÔ∏è –ó–ê–ü–£–°–ö –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø STREAMLIT
# ============================================================================

if __name__ == "__main__":
    # Create and run the system
    system = ArticleAnalyzerSystem()

    system.run()

